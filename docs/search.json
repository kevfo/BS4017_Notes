[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BS4017: High Throughput Bioinformatics",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "chapters/week1.html",
    "href": "chapters/week1.html",
    "title": "1  Introduction to Linux and the Command Line (untested)",
    "section": "",
    "text": "This week’s lecture aims to cover the following topics:\nLike the title of chapter implies, the information presented in this chapter will not be tested."
  },
  {
    "objectID": "chapters/week1.html#a-brief-introduction-to-computers",
    "href": "chapters/week1.html#a-brief-introduction-to-computers",
    "title": "1  Introduction to Linux and the Command Line (untested)",
    "section": "1.1 A Brief Introduction to Computers",
    "text": "1.1 A Brief Introduction to Computers\n\n1.1.1 Turing’s Machine\n\n\n\n\n\nIllustration of a Turing Machine\n\n\n\n\nA universal machine is a machine that can solve any sequence of problems that can be solved using a computer. An machine is called Turing complete if it can act like a Turing machine: a machine that is capable of following certain rules to solve problems in a stepwise fashion. Many of today’s programming languages are Turing-complete.\n\n\n1.1.2 Machine Language\nMachine language is the only thing that computers understand.\nThe central processing unit (i.e., CPU) is the so-called “brain” of the computer and uses bits to work. When one talks about a “64-bit: CPU, it means the groups of bits are 64 in length.\nComputers understand instructions in a language made of bits, but it’s really tough for people to read and figure out what’s going on. So, to make it easier, an operating system (i.e., OS) is necessary. This is like a middleman that helps us talk to the computer in a way we can understand better."
  },
  {
    "objectID": "chapters/week1.html#unix-os",
    "href": "chapters/week1.html#unix-os",
    "title": "1  Introduction to Linux and the Command Line (untested)",
    "section": "1.2 Unix OS",
    "text": "1.2 Unix OS\nThe Uniplexed nformation and Computer Service (i.e., Unix for short) OS - often pronounced as “eunuchs” was created at Bell Laboratories in the early 1970s to help make software.\n\n\n\n\n\nPhotograph of Ken Thompson and Dennis Ritchie\n\n\n\n\nIn 1969, it started as a bunch of instructions written in a language computers understand, and it was made by Ken Thompson. Then, between 1972 and 1974, Ken Thompson and Dennis Ritchie made a new version using a language called “C”. Back then, anyone could look at and use the code for free.\nBut in the early 1980s, the company AT&T decided to keep the code secret and started selling licenses for using Unix. It even led to different versions of Unix being made by different companies.\nInterestingly, Mac OS X, the operating system used in Mac computers, is a type of Unix too!\n\n1.2.1 Unix Philosophy\nThe design idea behind Unix programs can be summarized into three bullet points:\n\nWrite programs that are really good at doing one thing.\nWrite programs that can team up and help one another out.\nWrite programs that are good at dealing with text, because text is one way that everyone can talk to one another.\n\nThe command line is also the key to using a Linux system.\nThere are also nine “paramount precepts” as Mike Gancarz summarizes:\n\nRemember, small things are nice.\nEach program should be awesome at doing just one thing.\nBuild a simple version first to test.\nIt’s better if your stuff works on lots of different computers.\nKeep your information in plain text files.\nUse existing software to help you do more.\nUse special scripts to make things even better and work on different computers.\nDon’t trap people in your program; let them do what they want.\nMake each program a tool to help with tasks."
  },
  {
    "objectID": "chapters/week1.html#linux-os",
    "href": "chapters/week1.html#linux-os",
    "title": "1  Introduction to Linux and the Command Line (untested)",
    "section": "1.3 Linux OS",
    "text": "1.3 Linux OS\n\n“I’m doing a (free) operating system (just a hobby, won’t be big and professional like gnu) for 386(486) AT clones.”\n– Linus Torvalds, August 25th, 1991.\n\nLinux is like a free operating system that works with Unix (i.e., older systems). When Unix started costing money in the 1980s, people wanted a free option. Linux is like the core part of a computer system (i.e., the kernel), connecting the software to the hardware and different programs to each other. To make it a complete system, it’s joined with other software, and this whole package is called a distribution.\n\n1.3.1 Why Use Linux?\nIt’s trustworthy and stays steady with fewer problems. There’s hardly any viruses to worry about. It’s quick because it’s built really well. Plus, Linux is free - the software is open, which means it works well with other things and gets better quickly.\nIf you want to make your own programs, it’s easy with Linux because there are free tools and helpful information. One can also change the Linux OS code if they want to.\n\n1.3.1.1 Why Use Linux for Bioinformatics Data Analysis?\nLinux has lots of little tools made by many people that can each do a small part of the work. One can mix these tools together to create pipelines that do big tasks.\nIt’s also a fantastic platform for open-source software, which means one can use many programming languages and libraries without paying.\nBut, keep in mind that using Linux might not be as easy as some other options. one’ll have to type commands on a special line, and it only does exactly what one tells it to, not what you might want it to do automatically."
  },
  {
    "objectID": "chapters/week1.html#bash-and-the-command-line",
    "href": "chapters/week1.html#bash-and-the-command-line",
    "title": "1  Introduction to Linux and the Command Line (untested)",
    "section": "1.4 Bash and the Command Line",
    "text": "1.4 Bash and the Command Line\n\n1.4.1 What is Bash?\nWhen one uses a computer with Unix, they can talk to it through something called a terminal emulator. This is a “window” to type things.\nThe terminal helps one use a special interface called a shell, which is like a way to talk to the computer by typing commands. There are different types of shells, but “bash” is the most common. It’s been around since 1989 and is used in systems like Linux and Apple’s OS X.\n\n\n1.4.2 The Command Line\nThe command line is a place where one types in what they want the computer to do. It’s not as easy as clicking on icons like in a Graphical User Interface (i.e., GUI), but it has its benefits.\n\n\n\n\n\nCommand Line Appearance After Logging into Singapore’s National Supercomputing Center\n\n\n\n\nIt’s a bit harder for beginners because they need to learn the right words (commands), but it’s faster and lets them do more. One can write down everything they do in a text file, which can be helpful. It’s also great for working with text and making the computer do things over and over automatically.\n\n1.4.2.1 Example: Connecting to a Remote Server Using an Encrypted SSH Protocol\nSecure shell (i.e., SSH) and secure copy (i.e., SCP) are like special ways to talk to a computer securely. They were made by someone named Tatu Ylönen in 1995.\n\n\n\n\n\nDiagrammatic Explanation of How SSH Works\n\n\n\n\nImagine one is sending secret messages over an unsecure network. SSH makes a safe pathway using a special key that has a lot of numbers. This key comes in two parts: the public key, which can be shared, and the private key, which keeps things secret.\n\n\n\n1.4.3 Basic Bash Operations\n\n1.4.3.1 Fundamental Operations\nSome basic commands include:\n\nls - this lists all files and folders in the current working directory.\ncd - this changes the working directory.\ncp - this makes a copy of a file.\n\nCommands also have something called flags that modify the output. For instance, ls -la lists all files (including the hidden ones) in the current working directory in long form.\nCommands also take in arguments to complete the command - for instance, cp file1 ../folder1.\nThe man command displays helpful information about a command - for instance, man ls will list information about the ls command.\n\n\n1.4.3.2 Bash Variables\nIn bash, one can use variables like containers for information.\nFor example, one can say A=1 to put the number 1 in a variable called A. When one wants to use that number, they can add a “$” before the variable’s name, like “$A”.\n\n\n\n\n\nExamples of Assigning and Displaying the Values of Bash Variables\n\n\n\n\nTo show the value of a variable on the screen, they can use echo and write echo $A. Some variables are made by the computer, like PATH, which tells the computer where to look for programs. One can also add their own places to look by changing PATH.\n\n\n1.4.3.3 Redirects\nWhen you one runs a regular Unix command, it shows words on the screen. However, one can make those words go somewhere else too.\nFor example, if they want to list files and save the list in a file, they can write ls > listfile.txt. If they want to add more files to that list, they can write ls >> listfile.txt.\nThey can also make the words go to another command by using a pipe, like ls | grep listfile.\nYou can do this as many times as you need.\n\n\n\n1.4.4 Linux File System\nIn Unix, there aren’t “hard drives” like we usually think of them. Instead, there are directories. Think of these as special folders. ’\n\n\n\n\n\nExample of a Linux File System\n\n\n\n\nWhen one connects to a hard drive, it’s divided into pieces, and each piece is attached to a directory. So, the directory shows what’s in that piece of the hard drive.\nWhen one uses the “mount” command, it links a special part of the hard drive or even something from another computer to a directory. This way, the directory shows the stuff from that hard drive or computer.\n\n\n1.4.5 Permission Management in Linux\nIn Unix, there are three groups for who can do what with files. The first is the user who owns the file - they can keep things private.\nThen there’s a bigger group of users who can share files. Finally, there’s others, which means everyone else. There are three types of things you can do with files:\n\nread (r)\nwrite (w)\nexecute (x)\n\nThese permissions are like rules for each group saying what they can and can’t do with the files.\n\n\n1.4.6 Bash as a Turing-Complete Language\nThe bash shell isn’t just about doing things; it’s like a language for telling the computer what to do. The programs you make using the Bash language are called shell scripts. These scripts are like lists of instructions that the computer understands.\nThey’re translated and done by the computer right away, which makes Bash an “interpreted” language. Shell scripts are really good for quickly working with text and doing powerful things. For example, at NSCC, they have a system where you put your special list (script) in a line and the computer runs it when it’s ready.\n\n\n1.4.7 Useful Pointers When Using Linux\nProf. Jarkko also lists some tips when using Linux to work on tasks:\n\nYou can’t easily get back deleted files.\nSmall and capital letters matter in commands and file names.\nSome characters like #;& ” / ’ : < > | * ? $ ( ) { } [ ] and space do special things.\nIt’s safest to use only letters, numbers, _ (underscore), and . (dot) in file names.\nIf you use those special characters, put quotes around the name.\nFile names can be anything, like a mytext.txt file might not be text, but it’s good to follow conventions!"
  },
  {
    "objectID": "chapters/week1.html#how-does-programming-work",
    "href": "chapters/week1.html#how-does-programming-work",
    "title": "1  Introduction to Linux and the Command Line (untested)",
    "section": "1.5 How Does Programming Work?",
    "text": "1.5 How Does Programming Work?\n\n\n\n\n\nRelationships Between Levels of Programming Languages\n\n\n\n\nProgramming languages, like C/C++ and Visual Basic, or Python, R, and Matlab, use special tools to change the commands one writes into instructions the computer understands. These tools are like translators. Some languages use a compiler, which does the translation all at once, while others use an interpreter, which does it step by step.\n\n1.5.1 Compilers vs. Interpreters\nThere are two main ways to change one’s programming commands into computer language:\n\nCompiler:\nIt takes your list of commands (code) and changes it all into computer language at once. This gives one a file that they can run as a program. One can use it from the command line or through a nice interface.\nInterpreter:\nIt changes your commands into computer language one by one as they use them. They can give commands directly or run a list of them from a file called a script. This is like a set of instructions in a text file.\n\n\n\n1.5.2 Low versus High Level Languages\nLow-level programming languages are like really simple tools for computers. They only know how to do basic things, and they’re kind of like talking to the computer in its own language. They can work directly with memory and other computer parts. This makes them run really fast, but when one wants to do complex things, they have to write a lot of instructions, and it can be hard to find mistakes in their code.\nHigh-level programming languages are like using simpler words for computers. They’re farther away from the computer’s language, so they’re easier for people to understand. They work kind of like how humans talk, and they’re good for organizing things like objects. When one uses them, they can write less code because they can do complicated stuff in a simpler way. That’s why many people like using them for most things they create on computers."
  },
  {
    "objectID": "chapters/week2.html",
    "href": "chapters/week2.html",
    "title": "2  Genomic Sequencing and Databases",
    "section": "",
    "text": "Modern day genomic sequencing began with Sanger sequencing.\nSanger sequencing, created in 1977, is a method used to read the genetic code of living things. It’s like reading a very long sentence written in a language that cells use. This method can read up to 800 to 1000 ‘letters’ of this genetic sentence in one go. Think of it as reading a paragraph with 800 to 1000 words! And now, modern versions of this method can read 96 paragraphs all at once, making it faster and more efficient.\nStarting from 2004, progress in this field has been really fast. The first machinery that were used before that have become old-fashioned and aren’t used as much anymore."
  },
  {
    "objectID": "chapters/week2.html#sequencing-fundamentals",
    "href": "chapters/week2.html#sequencing-fundamentals",
    "title": "2  Genomic Sequencing and Databases",
    "section": "2.1 Sequencing Fundamentals",
    "text": "2.1 Sequencing Fundamentals\nThere are a few special words we need to know\n\nDepth\nThis is like how many times we read the same sentence in the book to be really sure we got it right.\nCoverage\nThis is like the average number of times we read different sentences in a part of the book.\nRead length\nThis is just how much of the book we can read at once. It’s like reading a long or short paragraph.\n\nWhen we’re reading this book, we want to make sure we cover every part equally, like giving the same attention to every page. This is called uniform coverage.\nThere are two main ways to read the book:\n\nOne way reads a lot of sentences quickly, but not all of them in detail.\nThe other way reads fewer sentences, but really understands them well.\n\nSo, you can choose to read a lot of the book or read less but really understand it deeply."
  },
  {
    "objectID": "chapters/week2.html#illumina-hiseq-and-novaseq",
    "href": "chapters/week2.html#illumina-hiseq-and-novaseq",
    "title": "2  Genomic Sequencing and Databases",
    "section": "2.2 Illumina HiSeq and NovaSeq",
    "text": "2.2 Illumina HiSeq and NovaSeq\n\n\n\n\n\nIllumina RNA Sequencing Machine Models\n\n\n\n\nCreating Illumina libraries is like preparing a special recipe for reading DNA.\nFirst, a long DNA strand and break it into smaller pieces. Then, the ends of these pieces are fixed to make them neat and ready. A special tail is then added to one end, like putting a ribbon on a gift.\nAfter that, tiny adapters, like bookends, are attached to both ends of the DNA pieces. This helps scientists to read them later. Only the right-sized pieces are desired, so any pieces that are too big or too small are removed.\nScientists make many copies of these DNA pieces so we have enough to read. Finally, they use a special machine to read the DNA letters one by one, just like flipping through the pages of a book really fast. This helps scientists learn about the DNA’s secrets and how it works!\n\n2.2.1 Typical Illumina Read Structure\n\n\n\n\n\nIllumina RNA Read Structures\n\n\n\n\nA usual Illumina read has the following structure:\n\nIllumina Forward\nImagine starting an exciting adventure book. The “Illumina Forward” is like the first page of the book, where the story begins. It’s the beginning point of our DNA reading journey.\nForward Target Primer\nThink of this like a special bookmark in the adventure book. It’s a tiny piece that helps find the exact spot to start reading about in the DNA. It’s like telling the story, “Hey, start reading right here!”\nTarget\nPicture this as a hidden treasure in the adventure story. The “target” is the part of the DNA that scientists really want to learn about. It’s like the exciting part of the story that holds a secret they’re curious about.\nReverse Target Primer\nJust like the “Forward Target Primer” helps scientists find the beginning, this is like another bookmark that helps scientists find the end of the part we’re interested in. It’s like saying, “Okay, stop reading here!”\nIllumina Reverse\nThis is like reaching the last page of a book. The Illumina Reverse marks the end of our reading journey for this section of DNA, letting scientists know that we’ve completed the sequence.\n\n\n\n2.2.2 Single End, Paired End, and Mate-Pairs\n\n\n\n\n\nExample of Mate Pair Construction to Construct a Full Genome\n\n\n\n\nIllumina is a method for reading DNA that’s like solving different puzzles. Imagine having pieces of a jigsaw puzzle.\nFirst, the scientist can look at just one side of each piece to see what’s there (i.e., single end). Or they can look at both sides of the pieces, but not the part in the middle (i.e., paired end). There’s also a special kind where they link bigger puzzle pieces together (i.e., mate-pair) to understand them better.\nIn the end, no matter which puzzle they choose, they get to read both sides and find out the average distance between them. These days, the most popular puzzle is the paired end, while the mate-pair puzzle is not used much.\nAlso, the smaller the puzzle piece, the harder it is to figure out where it fits in the bigger picture of the DNA. But having two sets of puzzle pieces helps scientists figure out where they go in the DNA picture more accurately.\n\n\n2.2.3 Barcoding\n\n\n\n\n\nExample of How a Barcode Can be Used\n\n\n\n\nIllumina sequencing can use something called **barcodes*&, which are like special tags added to each sequence. Think of them like labels on different books.\nImagine a library of books where and each book has a unique label. In the same way, the sequences get their own labels. If the labels are longer, it’s like having more specific tags for each book. This means you can put even more sequences together for reading, like having a huge shelf of books with similar tags.\nFor instance, a technology called 10x Genomics uses these longer labels to group sequences together, especially when looking at individual cells.\n\n\n2.2.4 FASTQ Formats\nIllumina returns data in a special text format called FASTQ. Imagine it’s like a recipe card for reading DNA. This card has four parts for each piece of DNA it reads:\n\nThe first part is like a name tag. It tells us which piece of DNA we’re looking at. Imagine it’s like a label on a box that says what’s inside.\nThe second part is the actual DNA sequence, like a secret code of letters (A, T, C, G, N). It’s like a coded message that we need to decode.\nThe third part is a repeat of the name tag. It’s like someone saying, “Hey, this is still the same box we talked about earlier.”\nThe fourth part is a set of quality codes. This helps us know how sure we are about each letter in the DNA sequence. It’s like having a confidence score for each letter.\n\nAnd these quality codes are kind of like a secret code too, but they’re similar to something called Phred. It’s like a scale that tells us how reliable each letter is in the DNA code, just like how we might trust different people’s opinions more or less.\n\n2.2.4.1 Phred Quality Scores\n\n\n\n\n\nPhred Score Meanings\n\n\n\n\nThe quality \\(Q\\) is calculated using the following formula:\n\\[\\begin{equation}\n  Q = -10\\log_{10}P\n\\end{equation}\\]\nThough, the scores are reported in something called ASCII to save space."
  },
  {
    "objectID": "chapters/week2.html#pacbio-sequencing",
    "href": "chapters/week2.html#pacbio-sequencing",
    "title": "2  Genomic Sequencing and Databases",
    "section": "2.3 PacBio Sequencing",
    "text": "2.3 PacBio Sequencing\nPacBio sequencing is a way to read DNA that’s like watching a movie frame by frame.\n\n\n\n\n\nHow PacBio Sequencing Works\n\n\n\n\nImagine a movie reel, but instead of film frames, there are DNA pieces. First, a special machine copies the DNA, making many identical pieces. These pieces are attached to a surface, like putting stickers on a wall.\nThen, a tiny camera watches as a machine adds one letter at a time to the DNA chain. It’s like watching someone write a story, but in DNA language. The machine records this process, and scientists can use it to figure out the DNA’s secrets. This method is special because it can read long pieces of DNA in one go, like reading long sentences without stopping.\n\n\n\n\n\nPacBio Sequencing Components\n\n\n\n\nPacBio sequencing works a bit like making a detailed copy of a story. The scientists start with a special DNA template called an SMRTbell. Imagine this template as the outline of the story. They put it on a surface and use a special machine to make copies of it. These copies are like drafts of the story. Then, a tiny helper called a “polymerase” comes in and reads the story, one letter at a time. It’s like reading the book aloud to remember every detail.\nBut here’s the cool part: the machine doesn’t just read the story once. It goes over it several times, each time reading a bit more. These shorter readings are called subreads. Think of them as reading a book chapter by chapter.\nNow, after all these readings, the scientists put everything together like a puzzle. It’s like taking all those drafts and arranging them to get the complete story. This final version is called a Circular Consensus Sequence, which is like having the perfect version of the story after making sure all the words are correct. This method helps us read long pieces of DNA with a high level of accuracy, just like getting the full story right.\n\n2.3.1 PacBio Read Formats\nWhen PacBio finishes reading DNA, it’s like recording a video of the process. This video is saved as a “.mov” file, kind of like how you save a video on your phone. But to understand the DNA story better, scientists need to do more things. They use special software called “SMRT Tools,” which PacBio made and shared with everyone. With this software, they can do different tasks, like taking off the starting and ending parts of the video (SMRT bell adapters), and pulling out the most important parts, which are like key scenes in the movie (Circular Consensus Reads / Subreads).\nOnce they’ve done all this, they can change the video into a different format, like turning a video into pictures or text. They do this to make it easier to work with. They can turn the video into a “.bam” file, which is like a fancy organizer for the pictures, or a “.fastq” file, which is like turning the video into words. All of this helps scientists understand DNA better and find out its secrets. You can learn more about this software and how it works on the PacBio website.\n\n\n2.3.2 HiFi Reads\n\n\n\n\n\nHow HiFi Sequencing Works\n\n\n\n\nThe newest thing from PacBio is called HiFi sequencing. It’s like reading a really long story in a special way. Imagine having a super-long book with chapters that are 20 pages long.\nHiFi sequencing reads these chapters many times and figures out the best version. It’s like asking different people to read the same chapter and then picking the one that’s most accurate. This helps us understand long sections of DNA really well."
  },
  {
    "objectID": "chapters/week2.html#oxford-nanopore",
    "href": "chapters/week2.html#oxford-nanopore",
    "title": "2  Genomic Sequencing and Databases",
    "section": "2.4 Oxford Nanopore",
    "text": "2.4 Oxford Nanopore\n\n\n\n\n\nHow Oxford Nanopore Sequencing Works\n\n\n\n\nOxford Nanopore technology is a different way to read DNA. Instead of taking pictures, it watches electric signals over time. Think of it like watching a graph that goes up and down. This graph is called a squiggle.\nThey save this squiggle in a special file format called fast5. This file not only keeps the squiggle but also the letters that the computer guessed from the squiggle. It’s like writing down both what the graph looks like and what the scientist thinks it means. With new software they’re making, they can even go back to the squiggle and try to guess the letters again to make sure they got them right. It’s like looking at the graph again and trying to understand it even better.\n\n2.4.1 Uses for Long DNA Reads\nUsing Oxford Nanopore technology for DNA reading has different strengths depending on how many times the DNA is read.\nWhen they read it a few times (i.e., low coverage), it’s like filling in the missing pieces of a jigsaw puzzle or adding more information to an already started story. They can use these extra details to make an existing puzzle more complete (gap-filling), make a short story longer and clearer (scaffolding a short read assembly), or combine different pieces from different puzzles into one big picture (hybrid assembly).\nBut when they read the DNA many, many times (high coverage), it’s like writing a whole new story from scratch. They don’t need any other clues because they have everything they need. This is called “De novo assembly,” where they put together a complete picture of the DNA just by using these long reads. It’s like creating a new jigsaw puzzle using only the pieces from one box."
  },
  {
    "objectID": "chapters/week2.html#ion-torrent",
    "href": "chapters/week2.html#ion-torrent",
    "title": "2  Genomic Sequencing and Databases",
    "section": "2.5 Ion Torrent",
    "text": "2.5 Ion Torrent\n\n\n\n\n\nA Graph Obtained from an Ion Torrent Machine\n\n\n\n\nIon Torrent sequencing is like a special tool that’s great for certain types of projects. When scientists need to read pieces of DNA that are a bit longer (around 400-600 letters), Ion Torrent is like a superhero.\nImagine reading a longer chapter of a book instead of just a few sentences. This is really helpful for projects like studying tiny living things called microbes that live in different places, like our bodies or the environment. These projects, called microbiome studies, benefit a lot from Ion Torrent because it helps scientists understand these tiny creatures in more detail."
  },
  {
    "objectID": "chapters/week2.html#bgiseq-and-mgiseq",
    "href": "chapters/week2.html#bgiseq-and-mgiseq",
    "title": "2  Genomic Sequencing and Databases",
    "section": "2.6 BGISEQ and MGISEQ",
    "text": "2.6 BGISEQ and MGISEQ\n\n\n\n\n\nNanoballs in BGISEQ and MGISEQ\n\n\n\n\nBGI, a genomics institute, created special machines for reading DNA. One of them is called BGISEQ-500, kind of like an older model. It reads two lines of DNA that are 100 letters long each, making a total of around 520 million letters. Then there’s a newer model, MGISEQ-2000. It can read two lines that are 200 letters each, for a total of about 1 trillion letters! These machines were made to compete with another popular DNA reader, Illumina. They were designed to be really affordable, making DNA reading cheaper for everyone. It’s like offering a lower-cost way to explore the secrets of genetics."
  },
  {
    "objectID": "chapters/week2.html#hi-c-chip-seq-10x-and-bisulfite-sequencing",
    "href": "chapters/week2.html#hi-c-chip-seq-10x-and-bisulfite-sequencing",
    "title": "2  Genomic Sequencing and Databases",
    "section": "2.7 Hi-C, ChiP-seq, 10x, and Bisulfite Sequencing",
    "text": "2.7 Hi-C, ChiP-seq, 10x, and Bisulfite Sequencing\nThere are various ways to investigate specific things about DNA, like its unique features. The main difference comes in how the DNA is prepared in the lab, but the actual reading part uses a common Illumina platform. Think of it like different ways to prepare a special dish using the same cooking equipment. No matter which method is used, they all provide a type of output file called “fastq,” which is like a document containing DNA information. However, the way this information is used can vary depending on the specific method employed.\n\n2.7.1 Hi-C\n\n\n\n\n\nHow Hi-C Sequencing Works\n\n\n\n\nHi-C sequencing is a technique that helps scientists understand how different parts of DNA are arranged in space. It’s like making a map of how different rooms in a house are connected.\n\n\n\n\n\nHi-C Data\n\n\n\n\nThey capture this information by studying how different parts of the DNA are close to each other. This technique gives them a picture of how far apart or near different parts of the DNA are along the entire chromosome. It’s like learning about the layout of a house by seeing which rooms are close to each other.\n\n\n2.7.2 10x Genomics\n\n\n\n\n\nHow 10x Genomics Works\n\n\n\n\nIn 10x Genomics technology, each individual piece of DNA is given a special code, like a secret badge. Think of it like giving every player in a game their own unique mark. This mark is created using tiny gel beads in a special mixture. It’s as if each player gets a distinct symbol, helping scientists keep track of different pieces of DNA while they’re doing their research.\n\n2.7.2.1 In Unicellular RNA Sequencing\n\n\n\n\n\n10x Genomics for RNA Sequencing\n\n\n\n\nWhen using 10x Genomics for single-cell RNA sequencing, there’s a twist in how it works. Instead of using big pieces of DNA, they focus on individual cells. Imagine each cell is like a small character in a story. They put one cell into each tiny gel bead, kind of like each character in their own small bubble. Inside these bubbles, the cell’s RNA, which is like its story, is turned into a special type of DNA called cDNA. This process is like translating the cell’s story into a new language. This way, scientists can study the stories of many individual cells all at once, and see how they’re different or similar.\n\n\n\n2.7.3 Bisulfite Sequencing\n\n\n\n\n\nHow Bisulfite Sequencing Works\n\n\n\n\nIn bisulfite sequencing, DNA is treated with a special chemical called bisulfite. This chemical changes some parts of the DNA. Imagine it’s like using a magic potion on a drawing. When applied to the DNA, bisulfite changes cytosine to uracil, but it doesn’t affect 5-methylcytosine. It’s like turning some parts of the drawing into a new color while leaving other parts the same. This helps scientists understand which parts of the DNA have certain molecules attached to them.\n\n\n2.7.4 Chromatin Immunoprecipitation\n\n\n\n\n\nHow ChIP Works\n\n\n\n\nChromatin immunoprecipitation (i.e., ChIP) is a method used to study how proteins interact with DNA. Think of it as a way to find out which proteins are hanging out with specific parts of DNA. Here’s how it works:\n\nFirst, scientists fix the proteins they’re interested in, like transcription factors, to the DNA using a special chemical called formaldehyde. Imagine it’s like gluing the proteins to certain parts of the DNA.\nThen they carefully take out the DNA and break it into smaller pieces, like breaking a long necklace into smaller beads.\nNext, they use special antibodies that act like magnets to pull out the proteins they’re studying. It’s like using a magnet to pick up certain toys from a pile.\nAfter that, they heat everything up to undo the gluing caused by formaldehyde. This step is like melting the glue and separating the proteins from the DNA.\n\nAs a result, they end up with DNA fragments that were connected to the proteins they were interested in. It’s like getting clues about which proteins were spending time with specific parts of the DNA. This helps scientists understand how different proteins control and interact with genes and DNA."
  },
  {
    "objectID": "chapters/week2.html#online-data-repositories-for-sequenced-data",
    "href": "chapters/week2.html#online-data-repositories-for-sequenced-data",
    "title": "2  Genomic Sequencing and Databases",
    "section": "2.8 Online Data Repositories for Sequenced Data",
    "text": "2.8 Online Data Repositories for Sequenced Data\n\n2.8.1 Sequence Read Archive (i.e., SRA)\nThe Sequence Read Archive (i.e., SRA) is like a big library where scientists from around the world store their DNA and RNA sequencing data. It’s kind of like a safe place for important information. This repository is taken care of by a group called NIH, in the USA.\nInside the SRA, you’ll find the raw data from sequencing, which is like the original puzzle pieces of DNA or RNA. Think of it as the untouched information from which scientists make discoveries. Both DNA and RNA data are kept here, like storing books of different kinds in the same library.\nMany scientific journals require researchers to share their sequencing data in the SRA. This is important because it allows other scientists to check their work and try things out for themselves. It’s like sharing a recipe so others can cook the same dish. Anyone can download this data for free, helping scientists all over the world learn from each other. However, more detailed information like complete genomes and detailed explanations are stored somewhere else. Sometimes scientists need to process the raw data a bit more to make sense of it, like cooking the raw ingredients into a delicious meal.\n\n\n2.8.2 NCBI Genomes\n\n\n\n\n\nNCBI Genomes\n\n\n\n\nThe NCBI Genomes database is like a huge book that holds a lot of important details about different species. It not only tells you what species a living thing belongs to, but it also lets you know if its DNA has been fully read and studied.\nInside this database, you’ll find information about the species’ classification, kind of like its scientific family tree. It’s like knowing which branch of the animal kingdom it belongs to. Additionally, you can find out if the species’ DNA has been completely read and studied in-depth.\nHowever, this database usually contains only the main or reference version of the species’ DNA, and it comes with annotations that tell you where different genes are located. These annotations are stored in a special kind of file called a “.gff” file, which is like a map showing where different treasures (genes) are hidden in the DNA. So, this database is like a treasure trove of genetic information about different species, helping scientists and researchers understand their DNA better.\n\n\n2.8.3 EBI: Ensembl Project\n\n\n\n\n\nEnsembl Project Homepage\n\n\n\n\nThe EBI’s Ensembl Project is like a special tool that helps scientists explore and understand the DNA of different living things. It’s kind of like a map for navigating the genetic information of various species.\nThis project offers a genome browser that allows researchers to access annotated genomes of species that belong to specific groups in the animal kingdom. There are different sections, like Ensemble Bacteria, Protists, Fungi, Plants, Metazoa, and Vertebrates. It’s like having different shelves in a library for different types of books.\nFor each species in Ensembl, you’ll find at least the main version of its DNA and information about its genes. Think of it as knowing the basic story of each species. But sometimes, there’s even more information available, like extra chapters in a book. This project is a helpful tool for scientists to study and learn more about the genetics of various living things."
  },
  {
    "objectID": "chapters/week3.html",
    "href": "chapters/week3.html",
    "title": "3  Data Preprocessing and Quality Control",
    "section": "",
    "text": "Today’s discussion covers several important aspects of sequencing data analysis. First, it’s crucial to be aware of potential sources of errors that can occur during the sequencing process. Next, we’ll delve into preprocessing, which involves getting the raw sequencing data ready for analysis. Quality control measures will help ensure that the data is accurate and reliable. We’ll also explore methods for detecting contamination in the sequencing data. To put these concepts into practice, we’ll walk through an example analysis using Drosera capensis as our model organism."
  },
  {
    "objectID": "chapters/week3.html#sources-of-error-in-sequencing",
    "href": "chapters/week3.html#sources-of-error-in-sequencing",
    "title": "3  Data Preprocessing and Quality Control",
    "section": "3.1 Sources of Error in Sequencing",
    "text": "3.1 Sources of Error in Sequencing\n\n“Limitations of the sequencing platforms, and particular artifacts associated with sequences generated on these platforms, need to be understood and dealt with at various stages of the project including planning, sample preparation, run processing and downstream analyses.”\n– Addressing Challenges in the Production and Analysis of Illumina Sequencing Data\n\nData preprocessing and quality control are the two most important things in data analysis.\n\n\n\n\n\nSome Possible Errors During Sequencing\n\n\n\n\nThere’s a phrase called “garbage in, garbage out” or GIGO for short. What this basically means is that if you start with bad data, you will produce bad analysis. Similarly, if you start with poor-quality ingredients, you will not make good food. Because of this, you need to ensure that your data is workable before you do anything else.\n\n3.1.1 Problems with Sampling\nThe sample itself can introduce errors.\nFor instance, if you don’t know what you’re sampling or got your samples mixed up, this could influence the final results.\nOr, if your sample is contaminated or degraded (i.e., not taken care of well enough), this can also affect the final quality of the data.\n\n3.1.1.1 Possible Sources of Contamination\nThere are a few possible ones:\n\nMicrobes such as bacteria, viruses, parasites, and whatnot.\nMaybe the sampler themselves could have dirty hands and introduced something into the sample without knowing it?\nResearchers mixing their samples.\nUsing dirty tools.\n\nThere are more sources here.\n\n\n3.1.1.2 DNA Fragmentation\nThe environment can have a significant impact on ancient DNA samples. This degradation process causes the DNA to break into small fragments and leads to specific changes, such as the conversion of cytosines to uracil.\nTo tackle these challenges, scientists use various techniques to enrich fragments with uracils in ancient DNA samples. By analyzing the patterns of DNA fragmentation, researchers can clean and correct data, using tools like mapDamage. These methods help us make sense of ancient DNA, even when it’s been affected by the passage of time and environmental factors.\n\n\n\n3.1.2 Problems with Creating a Genomic Library\nThere are a few:\n\nAdapter Dimers\n\n\n\n\n\nIllustration of an Adapter Dimer\n\n\n\n\nAdapter dimers can reduce the amount of useful data that’s generated by the sequencing machine. An adapter dimer is a small bit of DNA that is mistakenly sequenced instead of the target DNA, so this decreases the amount of useful data at the end of the day.\nChimeras\n\n\n\n\n\nIllustration of two Chimeras\n\n\n\n\nA chimera is a DNA sequence that is artificially made by joining two more DNA sequences. This happens when adapters mistakenly link unrelated DNA fragments (i.e., especially the SMRTbell templates above).\nBad Insert Size\n\n\n\n\n\nIllustration of a Bad Insert Size\n\n\n\n\nThis refers to a badly-spaced gap between the DNA fragments at the end (i.e., the primers and adapters). The insert size - the “gap” - needs to be the right size.\nIf the insert size is too big or too small, this can lead to issues down the road.\nSequencing Artefacts\n\n\n\n\n\nExamples of Sequencing Artefacts\n\n\n\n\nA sequencing artefact is a fancy term to refer to anything during the sequencing process that can affect the final quality of the data - for instance, air bubbles and dust particles. These artefacts can lead to repetitive patterns.\nA common kind of artefact that occurs is when two DNA fragments produces two distinct clusters, hence affecting data analysis.\nDuplicated Reads\n\n\n\n\n\nExamples of Duplicated Reads\n\n\n\n\nThis refers to multiple copies of the same DNA sequence, and this can happen because of numerous factors.\nDuplicated Reads\n\n\n\n\n\nPlot Illustrating GC Content\n\n\n\n\nThis kind of bias happens in Illumina sequencing as the technology favors DNA sequences with different amounts of “G” and “C” nucleotides. This can lead to the uneven coverage of the genome. Furthermore, sequencing technologies like PacBio and Illumina don’t do well with long sequences of repeating DNA sequences.\nAt least PacBio is less error prone to GC content biases or DNA length. But, it does have a higher error rate (i.e., 13% - 15%) compared to other technologies."
  },
  {
    "objectID": "chapters/week3.html#preprocessing-techniques",
    "href": "chapters/week3.html#preprocessing-techniques",
    "title": "3  Data Preprocessing and Quality Control",
    "section": "3.2 Preprocessing Techniques",
    "text": "3.2 Preprocessing Techniques\n\n\n\n\n\nGeneral Workflow for Bioinformatics Pipelines\n\n\n\n\nThe above figure show general steps when it comes to working with raw data (i.e., .fq files).\n\n3.2.1 Adapter Removal\n\n\n\n\n\nIllustration of Adapter Removal\n\n\n\n\nAdapter removal refers to removing adapter sequences - short pieces of DNA used during sequencing. This step ensures that the data is as clean as possible.\n\n\n\n\n\nExamples of Adapters\n\n\n\n\nAdapters have specific sequences; software is designed to recognize, locate, and remove these adapters from the data.\nThese software can also handle some errors within the data and ensure that the adapters are removed from the data."
  },
  {
    "objectID": "chapters/week3.html#quality-control-with-fast-qc",
    "href": "chapters/week3.html#quality-control-with-fast-qc",
    "title": "3  Data Preprocessing and Quality Control",
    "section": "3.3 Quality Control with Fast QC",
    "text": "3.3 Quality Control with Fast QC\n\n3.3.1 FastQC\nThere are numerous statistics (by section name) to take note of when using FastQC to quality control data:\n\nBasic Statistics\n\n\n\n\n\nBasic Information in a FastQC Report\n\n\n\n\nThis part lists basic information about a raw data file.\nPer Base Sequence Quality\n\n\n\n\n\nQuality Scores Boxplot\n\n\n\n\nGenerally speaking, the longer the read, the lower the phred quality score.\nIn the above graph, we see that most boxplots have a median score of above 30, but there are also many whose minimum score (i.e., the bottom whisker) is below 20.\nPer Tile Sequence Quality\n\n\n\n\n\nQuality Scores Heatmap\n\n\n\n\nA blue square on this plot means that everything went well. Otherwise, the yellow, green, and red squares hint that there’s some issue with the quality of the data at that exact spot in the raw data.\nThe Y-axis is the phred score, but scaled up by 1000.\nPer Sequence Quality Scores\n\n\n\n\n\nQuality Scores Heatmap\n\n\n\n\nA blue square on this plot means that everything went well. Otherwise, the yellow, green, and red squares hint that there’s some issue with the quality of the data at that exact spot in the raw data.\nThe Y-axis is the phred score, but scaled up by 1000.\nPer Sequence Quality Scores\n\n\n\n\n\nPer Sequence Quality Plot\n\n\n\n\nThe Y-axis refers to the amount of DNA sequences that have that same mean phred score on the X-axis. That peak in the above density plot is the mean phred score of all DNA sequences.\nPer Sequence Quality Scores\n\n\n\n\n\nPer Sequence GC Content\n\n\n\n\nThis plot shows the percentages of bases along DNA sequencing reads.\nPer Sequence Quality Scores\n\n\n\n\n\nPer Base Sequence Content\n\n\n\n\nThis plot shows the percentages of bases along DNA sequencing reads.\nPer Base Sequence N Content\n\n\n\n\n\nPer Base Sequence N Content\n\n\n\n\nPer Base Sequence N Content\n\n\n\n\n\nPer Base Sequence N Content\n\n\n\n\nPer Base Sequence N Content\n\n\n\n\n\nSequence Duplication Levels"
  },
  {
    "objectID": "chapters/week3.html#identifying-possible-contaminations",
    "href": "chapters/week3.html#identifying-possible-contaminations",
    "title": "3  Data Preprocessing and Quality Control",
    "section": "3.4 Identifying Possible Contaminations",
    "text": "3.4 Identifying Possible Contaminations\n\n\n\n\n\nA Bimodal Distribution in a Set of Raw Data’s GC Content Distribution\n\n\n\n\nThe bimodal distribution in the GC count read (i.e., that red density plot) is an indicator of possible contamination. One of the peaks could be indicative of something else.\n\n3.4.1 Using Databases to Identify Possibly Contaminated Data\nOne can match part of their data against reference databases - this is usually done with tools like BLAST or Diamond as they readily do this for you.\nWhat follows after is something called metagenomics analysis - the goal of this kind of analysis is to determine the origin of the DNA sequences. The National Center for Biotechnology Information (i.e., NCBI) has a lot of DNA sequences and each sequence belongs to a species; there’s also a taxonomy tree that organizes these species into families, genuses, and species.\n\n3.4.1.1 How Does it Work?\nWhen comparing sequences against reference ones, we look for matches - these are called hits. Each DNA sequence or read that we compare is tied to a specific organism.\nTo classify these reads, we identify something called the lowest common ancestor (i.e, LCA) in the species tree for each read. What this basically means is that we find the most specific taxonomic level where all the species that our read matches have similar DNA. Because of this, each read that we compare has a taxonomic classification as well.\nTools such as MEGAN can help automate this process.\n\n\n\n\n\nExample Approach to Identify Contaminated Sequences\n\n\n\n\nThe species that are closer (at least from an evolutionary viewpoint) tend to get more hits in this example. If the species that was sequenced isn’t in the NCBI, then the BLAST that was run on other species would contain the sequence.\n\n\n\n3.4.2 Using K-mers to Identify Contamination\n\n\n\n\n\nExample on How to Use K-Mers to Identify Outliers\n\n\n\n\nA k-mer is a fixed DNA fragment of length “k”. The above graphic shows how one can use K-mers to identify contaminated samples:\n\nGiven a the genome (from NCBI) of a particular species, split it into k-mers and then find the LCA of that k-mer.\nWe’ll also split our read into the same k-mers.\nWe find hits between the k-mers from our hits and the k-mers from the species’ genomes. We can then apply some sort of voting system to classify the read."
  },
  {
    "objectID": "chapters/week3.html#is-there-enough-data",
    "href": "chapters/week3.html#is-there-enough-data",
    "title": "3  Data Preprocessing and Quality Control",
    "section": "3.5 Is There Enough Data?",
    "text": "3.5 Is There Enough Data?\nIn order to do something called de novo assembly, we need to know if there’s enough data first. We can do this through flow-cytometry based size estimates to get a rough idea of a genome’s size.\nBut, one thing to keep in mind is that different individuals in the species may have different genetic makeups and also different genome sizes (e.g., chromosomal abnormalities).\nA polyploid is an organism that has multiple sets of chromosomes, and they are more common than we may think.\n\n3.5.1 K-mer Histogram\n\n\n\n\n\nExample of a K-mer Histogram\n\n\n\n\nThe K-mers that we get from our reads can be made into a histogram. We basically just count how many times each k-mer appears in the read and make it into a histogram.\n\n\n\n\n\nDetermining Genome Size from a K-mer Histogram\n\n\n\n\nIf the k-mers that are used to make the histogram are long enough, the mode - the peak with the highest - is the most abundant k-mer in the data.\nWe also use the k-mer histogram’s mode to estimate of the coverage: the average amount of times that a base in the genome is sequenced.\nThen, we divide the total amount k-mers that have been sequenced by the said mode - this should give an estimate of the genome size.\n\n\n\n\n\nA K-mer Histogram with no Mode\n\n\n\n\nIf a K-mer histogram doesn’t have any modes (like the one above), this means that the coverage isn’t high enough. So, we need to add in more data; this is because the genome is too big.\n\n\n3.5.2 Estimating the “K” in K-mers\nIf the K-mer is pretty short - say, \\(K = 4\\), then the possible K-mers that could appear will appear. In this case, there are \\(4^4 = 256\\) different combinations of 4-mers. Though, one thing to be wary of is that short K-mers don’t have uniqueness because they can occur in so many places in the data.\nThe last point is in contrast to long K-mers - the longer the K-mer, the lesser the amount of K-mers possible for that amount of \\(K\\).\n\n\n\n\n\nHypothetical Versus Observed NUmber of K-Mers\n\n\n\n\nBecause of the above points, the number of unique K-mers in our data should make a sort of parabola, and that peak in the middle is our optimal \\(K\\) value.\n\n\n3.5.3 Kmergenie\n\n\n\n\n\nExample Output of Kmergenie\n\n\n\n\nKmergenie is a kind of software that helps people estimate the genome size of their data using the k-mer analysis (i.e., the points said before this). Though, some software like MaSuRCA require the user to specify a “k” value beforehand.\nThere is also another software called jellyfish that also does the same thing as kmergenie - it helps find the optimal value of \\(K\\) in the K-mer histogram.\nAnd then there’s also another piece of software called SPAdes - this uses multiple K-mer values and usually gives good results. SPAdes pretty much gives the user the option to choose from several optimal values of \\(K\\) given their goals."
  },
  {
    "objectID": "chapters/week3.html#data-driven-quality-control",
    "href": "chapters/week3.html#data-driven-quality-control",
    "title": "3  Data Preprocessing and Quality Control",
    "section": "3.6 Data-Driven Quality Control",
    "text": "3.6 Data-Driven Quality Control\nWe need to conduct a check on the data to assess its quality - this sort of checking can highlight any issues with a specific library or a dataset (though, the common errors may include stuff like contaminated adaptors, low-quality data, and sequencing errors).\nThat said, there are usually two parts to this “checking”:\n\nTrim Reads\nWe need to “trim” or remove any low-quality data or bases from the data. This helps improve the quality of the data (obviously).\nMapping to Reference Genome\nThis allows us to understand how well our data maps to the expected results.\n\nThen, once all of this is done, we need to find any outliers using Data Science techniques. These anomalies could mean that there’s a sequencing problem, sample contamination, or even some other issue that needs to be looked into.\n\n3.6.1 Making a New Genome\n\n\n\n\n\nExample Genome Assembled\n\n\n\n\nIf we make a new genome, we usually do something called a quick draft assembly on it. This can let us know stuff like coverage and insert size distribution and can be helpful down the road.\nThat said, there are two terms that are must-knows:\n\nContigs\nThis just refers to a contiguous DNA sequence that doesn’t have any gaps or unsequenced regions. These “contigs” represent continuous segments of the genome and are usually obtained in the early stages of the assembly process.\nScaffold\nThis is a long structure that you get from joining multiple contigs together. Here, contigs are joined using “N”s - unsequenced bases. Scaffolds help link contigs and are a more comprehensive representation of the genome.\n\n\n\n3.6.2 GC and Average Coverage Plot\nDifferent species have varying genome lengths and GC percentages. These characteristics are unique to each species and can significantly impact analysis.\nDuring the sequencing process, reads are randomly sampled from the genomes being studied. This random sampling can lead to one of two thigns:\n\nSpecies with long genomes tend to have lower average coverage because there are more DNA base pairs to cover with the same number of sequenced reads.\nSpecies with shorter genomes tend to have higher average coverage with the same sequencing depth.\n\nThat said, the GC content of a genome can vary based on various factors, including the species’ lifestyle, environmental niche, and energy source. Different species may have distinct GC percentages."
  },
  {
    "objectID": "chapters/week4.html",
    "href": "chapters/week4.html",
    "title": "4  De Novo Sequencing",
    "section": "",
    "text": "General Bioinformatics Workflow\nTools such as MaSurCA (which will be used during the practical) are used to make genomes from raw data after the data has had its primers (from earlier Illumina sequencing) removed. Then, we use tools like quast to perform quality control on it.\nThat said, the main point of an assembly is shown in the picture above. Given a ton of overlapping DNA sequences, we want to try to reconstruct the actual genome - like putting together puzzle pieces to make a puzzle1."
  },
  {
    "objectID": "chapters/week4.html#laws-of-assembly",
    "href": "chapters/week4.html#laws-of-assembly",
    "title": "4  De Novo Sequencing",
    "section": "4.1 Laws of Assembly",
    "text": "4.1 Laws of Assembly\nThere are two main laws:\n\nFirst Law\n\n\n\n\n\nFirst Law of Genome Assembly\n\n\n\n\nLet’s say that we have two different reads (i.e., sequences) in our raw data: “A” and “B”. If the last few letters of “A” are similar to the beginning letters of “B”, then we can guess that “A” and “B” probably overlap with one another in the actual genome.\nSecond Law\n\n\n\n\n\nSecond Law of Genome Assembly\n\n\n\n\nLet’s say that we have a whole bunch of reads in our raw data. If these reads have a higher coverage (i.e., they’re read many times), then we will have more overlaps in our final genome.\nThird Law\n\n\n\n\n\nThird Law of Genome Assembly\n\n\n\n\nHow long our reads are and how repetitive those patterns are in the genome can make genome assembly challenging.\nSo, the best thing to do in this scenario is to just get a long read that cover(s) the entire repetitive sequence(s).\n\n\n\n\n\nOutcomes of Different Resolving Approaches\n\n\n\n\nDepending on how we choose to go about this issue, we can get different results. Because of this, we tend to just handle repeats that cannot be resolved by leaving them out altogether. But, we must keep in mind that if we do do this, then our assembly will be in fragments or contigs for short.\n\nHence, in de novo assembly, our ultimate goal is to really just find overlapping, short reads and put them into longer, continuous DNA sequences (i.e., like piecing together a puzzle from scratch). We use two different approaches for the most part:\n\nShortest Common Substring (i.e., SCS)\nOverlap Layout Concensus\nde Brujin Graphs\n\nBut regardless of which of the above that we end up doing in the end, one thing’s for certain: we use graph theory for all of these methods!"
  },
  {
    "objectID": "chapters/week4.html#overlap-graphs",
    "href": "chapters/week4.html#overlap-graphs",
    "title": "4  De Novo Sequencing",
    "section": "4.2 Overlap Graphs",
    "text": "4.2 Overlap Graphs\nThere are two main terminologies to know when it comes to graphs.\n\nNode\nThis just means a “read” or a “k-mer” in the raw data.\nEdge\nThis is a line connecting two nodes. If this line is an “arrow”, then we say that this edge is directed.\n\n\n\n\n\n\nExample of an Overlap Graph\n\n\n\n\nIn the above graph (i.e., an example), the graph represents the 6-mers of the DNA sequence: GTACGTACGAT. For simplicity’s sake, we only draw directed edges (i.e., the arrows) between nodes (i.e., the k-mers); the weight or number above the edge shows how many letters each k-mer overlaps with the other.\n\n4.2.1 Hamiltonian Paths\n\n\n\n\n\nExample of a Hamiltonian Path in a 6-mer Graph\n\n\n\n\nWe say that a graph is Hamiltonian if and only if we can visit all of its nodes at least once. In this same example, if we try to overlap the different K-mers to form the sequence GTACGTACGAT, we see the path that this sequence takes in the graph visits the 6-mers at least once!"
  },
  {
    "objectID": "chapters/week4.html#shortest-common-substring-i.e.-scs",
    "href": "chapters/week4.html#shortest-common-substring-i.e.-scs",
    "title": "4  De Novo Sequencing",
    "section": "4.3 Shortest Common Substring (i.e., SCS)",
    "text": "4.3 Shortest Common Substring (i.e., SCS)\n\n\n\n\n\nIllustration of the Shortest Common Substring Problem\n\n\n\n\nHere’s how this works in a nutshell:\n\nWe are given a whole bunch of K-mers.\nWe then smoosh all of these k-mers together to form one big, long string.\nOur job is to then find the shortest substring of that “big, long string” that contains the original k-mers as substrings. This “shortest substring” is called the shortest common substring (i.e., SCS).\n\nProf. Jarkko mentions that finding the SCS is NP complete: what this basically means is that the longer the “big, long string”, the more exponential the time to find the SCS becomes (hence making it all the more challenging).\n\n4.3.1 Greedy SCS Algorithm\n\n\n\n\n\nIllustration of the Greedy Shortest Common Substring Problem\n\n\n\n\nIf an algorithm is greedy, this basically means that it aims to maximize something about it.\n\n\n\n\n\nSecond Illustration of the Greedy Shortest Common Substring Problem\n\n\n\n\nIn this greedy SCS problem, we first start with the overlap graphs for the k-mers. Then, we combine the k-mers based on the weights of their edges until we get the substring (taking to get rid of the overlaps in the process).\n\n\n\n\n\nIllustration of the Greedy SCS Gone Awry\n\n\n\n\nHowever, just because an algorithm is greedy doesn’t always mean that it will lead to the best (i.e., optimal) outcome. If we look at the 6-mers of the overlap graph of the string a_long_long_long_time and try to do the greedy SCS problem for it, we see that we just end up with a_long_long_time. This isn’t the same string!\nInterestingly enough, if we use k-mers with \\(k = 8\\), we see that the final string that’s formed is actually correct."
  },
  {
    "objectID": "chapters/week4.html#overlap-layout-consensus-i.e.-olc",
    "href": "chapters/week4.html#overlap-layout-consensus-i.e.-olc",
    "title": "4  De Novo Sequencing",
    "section": "4.4 Overlap Layout Consensus (i.e., OLC)",
    "text": "4.4 Overlap Layout Consensus (i.e., OLC)\n\n\n\n\n\nIllustration of the OLC Process\n\n\n\n\nThe above graphic summarizes the steps involved in OLC.\n\n4.4.1 Overlap\n\n\n\n\n\nIllustration of an Overlap\n\n\n\n\nAn overlap refers to the ending of a string having at least a certain amount of matches with the beginning of another string. In the above example, we have two strings “X” and “Y”.\nIf we say that we want at least three matches, we start from the beginning of “Y” and see that the sequence “TAG” is found in the middle of “X” - the same could more or less be said for “GCC” too.\n\n4.4.1.1 Layout of the Overlap Graph\n\n\n\n\n\nIllustration of the Overlap Graph for 7-Mers of to_every_thing_turn_turn_turn_there_is_a_season\n\n\n\n\nThe above layout graph is super unruly and needs to be simplified.\n\n\n\n\n\nSimplifying the Overlap Graph\n\n\n\n\nSo, the first thing we need to do is to just simplify this graph. If we take a closer look at the original graph, we see that some edges can be inferred from other edges - in this case, the green arrows can be inferred from the blue ones.\n\n\n\n\n\nFurther Simplification of the Overlap Graph\n\n\n\n\nWe then remove these green edges - first, the ones that skip a node, and then next, the ones that skip two or more nodes.\n\n\n\n\n\nUnresolvable Repeats\n\n\n\n\nHowever, we see that the stuff in the red box cannot be resolved further.\n\n\n\n\n\nHaploid Assembly of a Genome\n\n\n\n\nSo, what scientists do is to get the contigs, line them up, and take the majority vote of the sequences. So, for example, if one nucleotide has mostly “A”s in a spot, then scientists assume that that particular nucleotide in space is an “A”.\n\n\n4.4.1.2 Caution\nThis entire process (i.e., the one in the preceding sub-sub-subsection) is super slow, especially when you realize that many sequencing datasets have hundreds of millions or billions of reads.\nAnd even if you could make the overlap graph, the graph would be ginormous!"
  },
  {
    "objectID": "chapters/week4.html#de-brujin-graphs",
    "href": "chapters/week4.html#de-brujin-graphs",
    "title": "4  De Novo Sequencing",
    "section": "4.5 De Brujin Graphs",
    "text": "4.5 De Brujin Graphs\n\n\n\n\n\nDe Brujin Graph for the String tomorrow and tomorrow and tomorrow\n\n\n\n\nThe meaning of an edge hasn’t changed here, but there are new temrs to know:\n\nMultigraph\nThis is a graph that has more than one node coming into a node.\nIndegree\nThis refers to the amount of arrows (i.e., directed edges) coming into a node.\nOutdegree\nThis refers to the amount of arrows (i.e., directed edges) coming out of a node.\nBalanced nodes\nThe indegree and the outdegree of a node are the same.\nSemi-balanced node\nThe indegree differs by the outdegree of the node by one.\nConnected graph\nThis just means that all nodes in a graph can be reached by some other node.\n\nAs a super short trick, if a directed graph (i.e., graph with arrows for edges) has two semi-balanced nodes at the very most, then that graph is Eulerian.\n\n\n\n\n\nDe Brujin Graph for AAABBBBA\n\n\n\n\nFirst, we split the genome AAABBBBA into 3-mers before splitting the 3-mers into 2-mers.\n\n4.5.1 Eulerian Walks\n\n\n\n\n\nEulerian Walk in the De Brujin Graph for AAABBBBA\n\n\n\n\nWe say that a graph has a Eulerian walk if and only if each edge can be crossed exactly once.\nGenerally speaking, we can find a Eulerian walk in linear time - meaning that the mode edges we have in a graph, the tougher and longer it is to find an Eulerian walk (obviously).\n\n\n4.5.2 Making a De Brujin Graph for a Genome\nFirst things first, we need to assume that each K-mer doesn’t have any errors and is only sequenced once.\n\n\n\n\n\nSplitting the String a_long_long_long_time into 5-Mers and then 4-Mers\n\n\n\n\nIn the above example, we make a de Brujin graph with the string a_long_long_long_time using 5-mers. We then begin by splitting each 5-mer into two pairs of of \\(k - 1\\)-mers - in this case, 4-mers.\n\n\n\n\n\nFinal Results of the String a_long_long_long_time into 5-Mers and then 4-Mers\n\n\n\n\nIn the end, our graph looks something like the one shown above. Also note that the finished graph is Eulerian as it only has two semi-balanced nodes at best.\n\n\n4.5.3 Problems with De Brujin Graphs\n\n\n\n\n\nPossible Outcomes of an Ambiguous De Brujin Graph\n\n\n\n\nIf there is a repeat in a sequence, this can cause issues. Here, we see that there are two equally-likely outcomes.\n\n\n\n\n\nErrors Affecting the Outcome of a De Brujin Graph\n\n\n\n\nAs in, errors like the following can make the graph not appear good:\n\nGaps in coverage (i.e., missing k-mers)\nCoverage differences\nErrors and differences between chromosomes."
  },
  {
    "objectID": "chapters/week4.html#error-correction",
    "href": "chapters/week4.html#error-correction",
    "title": "4  De Novo Sequencing",
    "section": "4.6 Error Correction",
    "text": "4.6 Error Correction\n\n\n\n\n\nMapping Errors to a K-mer Histogram\n\n\n\n\nIf we look at the K-mer histogram, we see that frequent k-mers tend to turn into infrequent ones.\nSo, here’s what we can do…\n\n\n\n\n\nCorrecting Errors in a K-mer Histogram\n\n\n\n\nFor each k-mer in each read, we do the following:\n\nIf the k-mer count is less than some value, we look at the k-mer’s neighbors within some distance.\nIf the said neighbors have counts more than that “some value”, we replace the k-mer in question with the neighbor."
  },
  {
    "objectID": "chapters/week4.html#how-much-data-for-a-de-novo-assembly",
    "href": "chapters/week4.html#how-much-data-for-a-de-novo-assembly",
    "title": "4  De Novo Sequencing",
    "section": "4.7 How Much Data for a De Novo Assembly?",
    "text": "4.7 How Much Data for a De Novo Assembly?\n\n\n\n\n\nTheoretical Analyses of Gene Lengths for De Novo Assembly\n\n\n\n\nThe problem is that the entire assembly is probably longer than the repeats in the genome. We could sequence and also re-sequence a bacterial genome with read lengths of 20 to 30 base pairs or nucleotides.\nHowever, with longer genomes, significant proportions of the genome are uncovered."
  },
  {
    "objectID": "chapters/week5.html",
    "href": "chapters/week5.html",
    "title": "5  Annotation",
    "section": "",
    "text": "Genome evolution, which is how the genetic information in living things changes over time, happens in two main ways. These ways are like two different strategies that genes use to change and adapt.\nThe first way is called whole genome multiplication. Imagine if you had a favorite book, and you made a copy of the entire book. Then, you made another copy, so you had three identical books. In the same way, sometimes in genomes, the whole set of genes gets duplicated or even triplicated. This means there are extra copies of all the genes.\nThe second way is small scale duplication, and it’s a bit like making copies of individual chapters or pages from your favorite book. There are two types of small scale duplication: “segmental duplication” and “tandem duplication.”\nSegmental duplication is when certain sections of the genome, like chapters in a book, are copied more than once. This can lead to having extra copies of specific genes, which can be useful for evolution.\nTandem duplication is a little different. It’s like having two or more paragraphs on the same page that are exactly the same. In this case, genes are duplicated one right after the other in a row."
  },
  {
    "objectID": "chapters/week5.html#whole-genome-duplications",
    "href": "chapters/week5.html#whole-genome-duplications",
    "title": "5  Annotation",
    "section": "5.1 Whole Genome Duplications",
    "text": "5.1 Whole Genome Duplications\n\n\n\n\n\nIllustration of a Whole Genome Duplication Event\n\n\n\n\nIn the above example, we have a genome from two different organisms - one blue and one orange.\nIf the genomes of two organisms come together, this is called an autopolyploid. Otherwise, this is called an alloalyploid.\nWhen genome evolution happens through whole genome duplications, it’s like making an identical copy of the entire set of genes. Imagine you have a long list of items, and you make an exact duplicate of that list. Here’s what happens:\nRight after the duplication, the genes in both copies are in the same order, which scientists call “synteny.” It’s like having the same items in the same order on both lists.\nBut, as time goes by, some changes occur:\n\nFractionation\nThis is when the genome starts to change. Some genes get lost because they become redundant (like having two identical items on your list), and random mutations can also affect genes.\nGenome rearrangements\nThink of this as reshuffling the items on your list. The order of genes can change.\nSyntenic blocks\nEven after a long time, there are still some parts of the genome where a bunch of genes look very similar to another bunch. It’s like having sections of your list that are almost the same as sections in the duplicate list.\nDiploid form\nEventually, the genome organizes itself back into a diploid form. This means it goes back to having two sets of genes, like you have one original list and one duplicate list, but with some changes.\n\n\n5.1.1 Genome Duplication in Organisms\n\n5.1.1.1 Plants\n\n\n\n\n\nWhole Genome Duplication in Plants\n\n\n\n\nGenome duplications happen quite often in the evolution of plants. It’s like when you have a favorite plant and it suddenly makes extra copies of all its genetic instructions. Here are some examples:\nAncient Hexaploid Event in Eudicots: About 150 million years ago, a group of plants called Eudicots had a special event where their entire set of genes got duplicated not just once, but three times! It's like having three identical instruction manuals for a plant.\n\nGrape\nAfter that ancient event, some plants like grapes didn’t have any more big duplications. So, they just kept the three copies they had and didn’t make any new ones.\nPoplar\nOn the other hand, poplar, which is another type of plant, recently had a whole genome duplication event. This means it made a fresh copy of all its genes, so it now has two sets of instructions.\nArabidopsis\nArabidopsis, yet another type of plant, had two whole genome duplication events in its history. So, it’s like having three instruction manuals originally, then copying them twice more, for a total of six!\n\n\n\n5.1.1.2 Fish\n\n\n\n\n\nWhole Genome Duplication in Fish\n\n\n\n\nIn the world of minnows and carps, which belong to a group called Cyprininae, there are around 1,300 different species. Now, here’s something interesting: approximately 400 of these species are what we call “polyploid.”\nPolyploid is a special term that tells us these species have more than the usual set of genetic instructions. It’s like having extra copies of a recipe book. In the case of these minnows and carps, having extra genetic copies can sometimes be a helpful adaptation. It’s a bit like having extra tools in your toolbox—they can be useful for different situations.\n\n\n\n5.1.2 Comparing Genomes\n\n\n\n\n\nComparing Two Genomes Using a Dot Plot\n\n\n\n\nA dot plot is like a cool tool scientists use to compare different genomes, which are like the instruction manuals for living things. Here’s how it works:\nFirst, imagine you have a bunch of genes from two different organisms, and you want to see how similar they are. The dot plot helps you visualize this by showing dots where genes are similar.\nAt first, it looks a bit messy with lots of dots. But, at a basic level, it helps us find pairs of genes that are kind of like neighbors in the instruction manuals. These are called syntenic gene pairs, and they might be important for understanding how organisms are related.\nIf we want to get even more detailed, we can study these syntenic gene pairs more closely. It’s like zooming in on those neighbor genes to see exactly how they match up.\nOne way to find these syntenic blocks, which are groups of similar genes, is to use a special program called DAGchainer. It’s like a detective that helps us find these important blocks in the instruction manuals.\nDAGchainer uses a smart method called dynamic programming to do this. Think of it like a super organized way to solve puzzles and find important patterns in the genes. So, dot plots and programs like DAGchainer are tools scientists use to uncover the secrets hidden in genomes!\n\n\n5.1.3 Exploring Whole Genome Duplications with Synteny\n\n\n\n\n\nSynteny Plots\n\n\n\n\nImagine you have two sets of genes from different organisms, and you want to figure out when they had a big duplication event. Synteny is like a tool that helps us with this detective work.\nFirst, scientists do something called pairwise alignment, which is like comparing the two sets of genes to see which ones match up. These matching gene pairs are called syntenic gene pairs.\nNow, here’s the cool part: we can look at how many “synonymous substitutions” have happened in these gene pairs. Synonymous substitutions are like small changes in the genes that don’t really affect how the protein is made or its building blocks (amino acids).\nBy counting these mutations, it’s kind of like using a molecular clock. This clock tells us how much time has passed since the big gene duplication happened. Just like a clock helps us keep track of time, counting these mutations helps scientists figure out when the duplication event took place.\n\n\n5.1.4 Functional Biases on Gene Origins\n\n\n\n\n\nProf. Jarkko’s Graph\n\n\n\n\nWhen we talk about how genes work and where they come from, there are two main ways genes can be duplicated: Whole Genome Duplication (WGD) and Tandem Duplication. These two processes can affect how genes function.\nIn the case of Whole Genome Duplication (WGD), there’s something called the dosage-balance hypothesis. This means that genes that are really important and highly connected in the cell are usually kept around when gene duplication happens. Think of it like keeping the most important players on your sports team. These important genes often include things like transcription factors and regulators, which are like the coaches and referees of the cell.\nOn the other hand, with Tandem Duplication, it’s more about recent adaptations. Imagine if you have a group of friends, and you suddenly need to deal with a new challenge, like a surprise test. You might quickly form a study group with the friends who are best at that subject. Similarly, genes that are duplicated in tandem, one after the other, often include genes that help an organism cope with immediate changes in its environment. For example, if there’s a sudden threat from a pathogen, genes related to defense might get duplicated to help the organism adapt quickly.\n\n5.1.4.1 Why is this Important?\nAfter a special kind of gene duplication called Whole Genome Duplications (WGDs), the genes that stay around are usually super important. They’re like the VIPs of the cell—genes that help control things like when to grow, when to stop, and how to respond to signals. We call this idea the “dosage-balance hypothesis.”\nThe cool thing is that similar VIP genes can be found in almost all species, and they work in pretty similar ways. It’s like finding the same important tools in everyone’s toolbox. This makes it easier for scientists to find similar genes in different species, and we call these similar genes “orthologs.”\nNow, when genes are copied one after the other, like in Tandem Duplication, they often help an organism deal with changes in its environment. These are like the genes that help you adapt when something unexpected happens, like a sudden change in weather. These newly copied genes are very similar in their instructions.\nBut here’s where it gets tricky: these copied genes can be a bit different between individuals of the same species, and this can cause problems when scientists are trying to match up short pieces of genetic information (short reads) with the gene instructions.\n\n\n\n5.1.5 Implications for Genome Annotation\nFirst, think about the number of gene copies. It’s like saying that in some species, there might be more copies of a certain gene, while in others, there are fewer. It’s kind of like how some people have more toys than others. This variation in gene copies can make things a bit complicated.\nNow, when it comes to species, those that are more closely related, like siblings, tend to be more similar to each other in terms of their genes. But species that are very different, like distant cousins, might have genes that look quite different. It’s a bit like how you might look more like your brother or sister than a cousin you’ve never met.\nPolyploidy events are like times when a species had too many copies of its entire set of genes. Imagine if you suddenly had two toy chests filled with toys instead of one. It can be a bit tricky to figure out which toys are exactly the same in both chests, and this is kind of like the challenge scientists face when finding orthologs in species that have experienced polyploidy.\nLastly, with tandem duplications, it’s like having the same toy repeated in a row in your toy chest. When this happens with genes, it can be hard to predict because it looks like there’s just one type of toy in your chest, even though there are actually many copies of the same toy. So, for scientists trying to understand gene models, it can be a bit tricky to tell what’s going on."
  },
  {
    "objectID": "chapters/week5.html#quality-control-of-assembly",
    "href": "chapters/week5.html#quality-control-of-assembly",
    "title": "5  Annotation",
    "section": "5.2 Quality Control of Assembly",
    "text": "5.2 Quality Control of Assembly\n\n5.2.1 N50 Value\nWhen scientists are putting together the puzzle pieces of a genome, one of the most important things to check is how well those pieces fit together. This is called contiguity.\nContiguity is like looking at a jigsaw puzzle and making sure all the pieces are lined up nicely. You want to see how many separate pieces, called contigs or scaffolds, make up the genome.\n\n\n\n\n\nIllustration of the N50 Value\n\n\n\n\nNow, to tell how good the assembly is, we use a special number called the N50 value. This number tells us something really important: it shows the length where 50% of the genome assembly is made up of contigs or scaffolds longer than this length.\nThink of it like this: if you have a bunch of puzzle pieces, and you find the N50 value, it means that half of the puzzle pieces are at least that big. So, the higher the N50 value, the better the quality of the genome assembly because it means larger and more complete pieces are used to put together the genome.\n\n\n5.2.2 Conserved Single-Copy Genes\nWhen scientists are working on putting together a genome, one of the most crucial things they want to make sure of is that they capture all the important parts of the genes. Imagine if you were building a car, and you needed to make sure you had all the essential parts like the engine, wheels, and brakes. In genome assembly, those important parts are the genes.\nNow, here’s the tricky part: figuring out if you’ve got all the genes in their proper places can be hard without some extra help. It’s like trying to build a car without an instruction manual.\nBut scientists have a clever trick. They’ve looked at lots of different species and found a special group of genes that are found in nearly all genomes as single copies. This means there’s usually just one of each of these genes in the genome. It’s like finding common tools that are used in many different types of cars.\nThese single-copy genes are super important because they’re often the first ones to disappear if there are extra copies or if genes get shuffled around during evolution.\nTo help scientists check if they’ve got these essential genes in their genome assembly, they use software like CEGMA (which is a bit outdated now) and BUSCO. These programs act like detectives to see if all the crucial genes are present and in good shape, kind of like making sure you have all the essential car parts before you start driving.\n\n\n5.2.3 BUSCO\nImagine you have a big jigsaw puzzle, and you want to make sure you’re putting all the right pieces in the right places. In the world of genomes, those puzzle pieces are genes, and they need to be correctly identified and placed. This is where Benchmarking Universal Single-Copy Orthologs (i.e., BUSCO) comes in.\nBUSCO helps with genome annotation, which is like labeling and understanding the genes in a genome. It does this using a two-step approach:\nFirst, it uses something called tblastn to search for known single-copy protein sequences within the genome. Think of this as looking for specific shapes in your puzzle pieces that you know should be there.\nOnce it finds these matches, it uses them to teach a special tool called “Augustus” how to predict where other genes are in the genome. It’s like showing someone a few pieces of the puzzle so they can guess where the rest of the pieces should go.\nThen, Augustus starts predicting the genes and looks for matches to a set of genes that are known to be present as single copies in most genomes. These genes are kind of like the most important pieces of the puzzle.\nBUSCO gives a report that tells you the number of found full-length genes (genes that are complete and correct), duplicated full-length genes (genes that have extra copies), fractionated genes (genes that are split up), and missing genes (genes that are nowhere to be found).\nThe “Busco score” is a handy percentage that tells you how well your genome assembly is doing. If it’s over 95%, that’s really good and means you have a high-quality assembly. If it’s between 90-95%, that’s still good. But if it’s below 80%, it’s a sign that there might be issues with your genome assembly.\n\n5.2.3.1 Pros and Cons\nUsing BUSCO is like taking a quick test to check how well you’ve done a big job, like building a complex model. It’s become a standard method in genomics because it helps scientists figure out if they did a good job in understanding the genes of an organism quickly.\nNow, there are a few things to keep in mind when using BUSCO:\n\nNot the Whole Picture\nThe results from BUSCO might not tell you everything about the quality of the entire set of genes in the organism. It’s like if you looked at only a few parts of a car and assumed the whole car was in perfect shape.\nPredicting Easy Genes\nBUSCO focuses on finding genes that are considered “easy” to predict because they’re pretty much the same across many species. This means it might miss some of the more unique or hard-to-predict genes.\nReference Species\nBUSCO defines these “easy” genes based on a set of species used as a reference. However, not all organisms have the exact same genes, and in some cases, the genes might be so different that BUSCO can’t spot them.\nDuplicate Genes\nSometimes BUSCO might find extra copies of genes. This can happen because of problems with how the genome was put together (assembly problems) or because the organism naturally has extra copies due to recent genome duplications."
  },
  {
    "objectID": "chapters/week5.html#genome-annotation",
    "href": "chapters/week5.html#genome-annotation",
    "title": "5  Annotation",
    "section": "5.3 Genome Annotation",
    "text": "5.3 Genome Annotation\nLet’s break down three types of annotation tasks:\n\nGenome Annotation\nImagine you have a huge book with lots of words but no titles or chapters. Genome annotation is like giving titles to chapters in this book. Scientists try to figure out where the important genes are hiding in the DNA sequence of an organism’s genome. It’s like identifying the main characters in a story.\nStructural Annotation\nThis is about looking closer at the genes you’ve identified. It’s like studying the characters in a book and finding out if they have any special traits or abilities. In genetics, scientists try to spot specific patterns or “domains” in genes that are similar across different species. These domains are like superpowers for genes, and they help scientists understand what the genes do.\nFunctional Annotation\nOnce you know what the genes look like and what patterns they have, you want to figure out what they actually do. It’s like reading the book to learn about the characters’ roles. Scientists try to guess the function of a gene based on its patterns and shapes. They do this by finding genes in other species that are very similar in structure or sequence (like finding characters in other books that are a lot like the ones you already know). Then, they assume that these similar genes have similar functions. To be really sure, they might do experiments, like RNA sequencing or mutant tests, to check if their guess is right.\n\nSo, in a nutshell, genome annotation1 is like naming characters in a book (identifying genes), structural annotation is like discovering special traits of these characters (identifying patterns in genes), and functional annotation is like figuring out what these characters do in the story (identifying gene functions). It’s all about understanding the genetic story of an organism!\n\n5.3.1 Coding Regions\n\n\n\n\n\nIllustration of Introns and Exons\n\n\n\n\nWhen scientists study a genome, they’re like detectives trying to find important clues. They want to identify the functional parts of the genome, which are like the chapters and important details in a story. These functional parts include things like the promoter (which tells genes when to start), exons (which are the important coding parts of genes), and introns (which are like extra, non-coding sections within genes).\nNow, introns are like tricky puzzles in this genome story. They make the job of predicting or figuring out where genes start and stop a bit harder. It’s like trying to read a book with extra sentences that don’t make much sense. These introns can confuse scientists because they don’t contain the actual instructions for making proteins, so it’s like having extra pages in a recipe book that you don’t need."
  },
  {
    "objectID": "chapters/week5.html#repeat-analysis-and-making",
    "href": "chapters/week5.html#repeat-analysis-and-making",
    "title": "5  Annotation",
    "section": "5.4 Repeat Analysis and Making",
    "text": "5.4 Repeat Analysis and Making\n\n5.4.1 Repetitive and Transposable Elements\nIn genomes, there are parts that repeat themselves, like when a song chorus repeats several times. These repeating patterns in the genome come in different types:\n\nSimple Repeats\nThese are like when you say the same word over and over. They don’t have a lot of information.\nTransposable Elements\nThese are like special sequences that can move around in the genome. They come in two flavors: autonomous (like the main boss) and non-autonomous (like the helpers).\n\nTo find these repeating patterns, scientists use computer programs. These programs are like detectives with a set of tools. They follow a step-by-step process, sort of like how you might go through a recipe to cook something. These tools help them find the repeating patterns in the genome.\nOne of the most commonly used detective programs is called RepeatMasker. It’s like a superstar detective because it combines the powers of different tools to get the job done. Think of it as having multiple gadgets in a detective’s toolkit. RepeatMasker uses two de novo repeat-finding tools (RECON and RepeatScout) to identify new repeats, a tandem repeat finder to spot specific kinds of repeats, and a database called Repbase that holds information about known repeats from other species. It’s like using a reference book to help with the detective work.\nApart from RepeatMasker, there are some other detective software programs like PiRATE and Repet. They work in similar ways but might have their own unique tricks and features to find repeating patterns in the genome.\n\n\n5.4.2 Transcriptomic Evidence\nScientists use a special technique called RNA-sequencing to learn which genes are active in a cell or organism at a particular time. It’s like listening in on a conversation to see who’s talking.\nThis technique helps create something called a transcriptome, which is like a detailed list of all the genes that are “talking” or being used by the cell. It’s the most accurate way to know which genes are active.\nBut here’s the catch: just because you’re listening in on a conversation doesn’t mean you hear everyone talking. Similarly, not all genes will be “talking” or active at the same time, so the transcriptome will be incomplete. Some genes are like shy individuals who only speak up in certain situations.\nRNA-sequencing can also tell us about different types of genes. Some genes are only active in specific tissues (like heart genes in heart tissue), some follow a daily schedule (diurnal genes), and some are only used during certain stages of growth (developmental state-specific genes). It’s like finding out who talks only in the library, who talks only in the morning, and who talks only at a party!\nAnd sometimes, genes can have different versions, like how a story can be told in different ways. These are called splice variants, and they can be specific to certain tissues.\nScientists use RNA-sequencing data in two main ways:\n\nThey can match the “conversations” (reads) they hear to a known “script” (the genome) to figure out which genes are active. It’s like finding out who’s talking by checking a script.\nOr they can piece together a new “script” (de novo assembly) from the conversations and then figure out which genes are active. It’s like creating a script from scratch based on what people are saying.\n\n\n\n5.4.3 Prediction from Sequence (i.e., ab initio)\nTo learn patterns from genes, Bioinformaticians can turn to machine learning to make something called a hidden Markov model (i.e., HHM).\n\n\n\n\n\nLayout of a Hidden Markov Model\n\n\n\n\nA HHM is a kind of computer model that was first created back in the late 1970s. It wasn’t originally meant for genetics or biology, though. Instead, it was designed for something quite different: speech recognition.\nThe idea behind an HMM is to break down a process into two main parts: hidden states and observations. In the case of speech recognition, the hidden states represent different sounds or letters that make up spoken words, while the observations are like the actual sound signals.\nEach hidden state in the model is connected to a specific acoustic signal. So, when there’s a change in the hidden state, it’s like switching to a different letter or sound. When you put all these changes together, you get a sequence, or a path, of hidden states that make up a word.\nIn speech recognition, the goal is to figure out the most likely path of hidden states based on the sounds we hear. So, HMMs help computers understand spoken words by finding the best match between what they “hear” (the observations) and the most likely sequence of sounds (the hidden states). It’s like trying to guess the word someone is saying based on the sounds you hear.\n\n5.4.3.1 In Genetics?\nIn the world of genetics, scientists also use Hidden Markov Models (HMMs), but for a different purpose: understanding genes.\nJust like in speech recognition, HMMs for genes have hidden states and observations. In this case, the hidden states represent different parts of a gene, such as exons (the important coding sections), splice donors/acceptors (which are like gene punctuation marks), introns (non-coding parts within a gene), and intergenic regions (spaces between genes).\nThe observations, in this context, are the sequences of DNA letters (A, T, C, G) that make up the gene. It’s like looking at the genetic “code” to understand how genes are put together.\nA gene, in this model, is like a path that starts at the first exon and goes through all the hidden states, following the rules of the model.\n\n\n5.4.3.2 Why Use ab initio Predictions?\nFirst, it’s important to understand that not all genes are active or “expressed” in the samples being studied. Think of it like a library where not all the books are being read at the same time. RNA sequencing only tells us about the genes that are actively “reading” or expressing themselves.\nAnother reason is that some genes don’t follow the usual rules of gene splicing. It’s like having a few books in the library that are written in a different style, and these books might be missed by the RNA sequencing approach.\nLet’s take the example of birch, a type of tree. When scientists used RNAseq for birch, they found about 20,000 sequences (isotigs). However, to get a more complete picture, they still needed to predict around 10,000 genes computationally. This is because RNA sequencing doesn’t always capture all the genes.\nNow, when it comes to picking which gene prediction tools to use, there are a few top contenders in the field, like Augustus and BRAKER. But there are also many other software options available, making it a bit like choosing between different tools for a job.\nIn practice, scientists often use a combination of these tools. For birch, they tried out about 10 different gene predictors and ended up using the four that worked the best. This is because different predictors have different ways of modeling genes, so some might be better at detecting certain types of genes compared to others.\n\n\n\n5.4.4 Gene Models in Other Species\nOne of the key ways to make sure gene predictions are accurate is by looking at external evidence. Think of it like checking your work with a trusted source.\nOne of the most reliable sources of external evidence is RNA sequencing data. This data is like a gold standard because it tells us exactly which genes are active and how they’re structured. It’s a bit like having the answers to a test. Scientists can compare their gene predictions with RNA sequencing data to see if they match up.\nThere are two ways to use RNA sequencing data. One is called de novo transcriptome assembly, which is like putting together a puzzle without a picture on the box. It’s useful when you don’t have a complete genome to work with. But sometimes, it’s a bit like solving a puzzle with missing pieces, and it can be hard to tell similar genes apart.\nAnother good source of evidence is old collections of Expressed Sequence Tags (ESTs). These are like clues left behind by genes. Scientists can use these clues to confirm their gene predictions.\nIt’s also helpful to look at genes in well-studied or closely related species. It’s like asking someone who’s good at a subject for help. If these genes are similar to the ones you’re predicting, it adds confidence to your predictions.\nHowever, predicting genes in gene families that are right next to each other (tandem repeats) can still be a tricky puzzle. It’s like trying to tell identical twins apart. Sometimes, even with external evidence, it’s hard to be certain.\n\n5.4.4.1 Aligning Protein Evidence to Genome\nImagine you have a jigsaw puzzle, and you want to find where certain pieces fit. When scientists have pieces of evidence like proteins or transcripts and want to see where they match in a genome, it’s a bit like finding the right spot for puzzle pieces.\nOne way to do this is using a tool called tblastn. Think of it as a detective tool. What it does is take the genome and look at it in different ways, kind of like trying to read a book from different angles. It’s searching for matches to a protein or transcript.\nHowever, there’s a limitation with tblastn. It doesn’t take into account something important called splicing, which is like how sentences are rearranged in a book to make sense. So, tblastn might miss some of the edges or boundaries of where the protein or transcript matches the genome. It’s like finding parts of a sentence but not realizing where one sentence ends and another begins.\nTo get the whole picture, scientists use other tools like Exonerate, PASA, and GeMoMa. These are like detective tools with special glasses that help them see the splicing parts better. They pay attention to the spots where the sentences (or genes) are joined together with special signals called donor and acceptor sites.\nThese splicing-aware tools also do something clever. They look for regions in the genome where there’s a high match to the protein or transcript evidence and then join these regions together. It’s like finding pieces of a puzzle that fit really well and realizing they belong together."
  },
  {
    "objectID": "chapters/week5.html#combining-evidence",
    "href": "chapters/week5.html#combining-evidence",
    "title": "5  Annotation",
    "section": "5.5 Combining Evidence",
    "text": "5.5 Combining Evidence\nWhen scientists want to figure out what a gene does, they turn to a tool called Interproscan. It’s like using a detective kit with different tools to uncover the gene’s secrets. Interproscan does two important jobs at once. It’s a bit like looking at both the shape and function of a key to understand what it can unlock. For genes, it checks their structure and function.\nOne thing it checks is something called “conserved protein domains.” Think of these as like the building blocks or patterns that genes are made of. Interproscan compares these patterns to a big database that includes PFAM, protein superfamilies, and PANTHER.\nAnother tool in its kit is BLAST, which is like a gene family tree detective. It helps find genes that are similar to the one being studied. These similar genes can give clues about what the gene does. Interproscan also looks at metabolic pathways (like the chemical processes in a cell) using tools like KEGG and Metacyc. It’s like figuring out how ingredients are used in a recipe. It also predicts Gene Ontology (GO) categories, which are like labels that describe what a gene is involved in.\nTo assign a gene’s function, Interproscan uses something called the guilt-by-association principle. It’s like assuming that if someone is often seen with a group of people doing a particular activity, they’re likely involved in that activity too. In genes, if a gene is similar to others in a certain pathway or category, it’s probably involved in the same kind of job."
  },
  {
    "objectID": "chapters/week5.html#final-verification",
    "href": "chapters/week5.html#final-verification",
    "title": "5  Annotation",
    "section": "5.6 Final Verification",
    "text": "5.6 Final Verification\nProf. Jarkko lists a sample checklist for this bit - just to ensure that the genome in question is a good one:\n\nStart with the Gene Model\nBegin with the predicted gene model as your reference. It’s like having a blueprint for a building.\nCompare with RNAseq and EST Data\nCheck if the RNA sequencing (RNAseq) and EST data match the gene model. It’s similar to making sure the building looks like the blueprint.\nExamine Protein Matches in Other Species\nSee if proteins from related species match your gene. It’s like comparing your building to others in the neighborhood.\nBLAST the Protein Sequence\nUse a tool called BLAST to compare your gene’s protein sequence to a big database of genetic information. This helps you find similar genes in various species and might give hints about gene function.\nGather Good Hit Sequences\nCollect the sequences that closely match your gene, especially from well-studied model species. Think of it as getting advice from experts.\nMultiple Sequence Alignment\nLine up the protein sequences and see if they match well. It’s like checking if puzzle pieces fit together. Look out for mistakes like frame shifts, incorrect splicing, or missing start/stop signals.\nWatch Out for “Ns”\nIf you see “N” in the sequence, it means there’s missing information. It’s like having a blank spot on the blueprint.\nManual Adjustments\nIf needed, make manual adjustments to the gene model to improve accuracy. It’s like fine-tuning the building plans to make everything fit perfectly.\nExperimental Verification\nFinally, validate the gene model with experimental data. This involves using RNAseq and targeted experiments to confirm that the gene functions as predicted. It’s like testing the building to ensure it works as intended."
  },
  {
    "objectID": "chapters/week6.html",
    "href": "chapters/week6.html",
    "title": "6  Transcriptomes: Experiment Design and RNA Sequence Data Processing",
    "section": "",
    "text": "Overall Workflow for Working with RNA Sequence Data\nThe above graphic shows the general flow of actions when it comes to working with RNA sequencing data (i.e., from the wet lab stuff to the actual data analysis later on)."
  },
  {
    "objectID": "chapters/week6.html#experimental-design",
    "href": "chapters/week6.html#experimental-design",
    "title": "6  Transcriptomes: Experiment Design and RNA Sequence Data Processing",
    "section": "6.1 Experimental Design",
    "text": "6.1 Experimental Design\nExperimental design is like setting up a plan for a science experiment. It’s super important because it helps us get the right kind of information to answer our questions. Imagine you’re trying to figure out something cool in biology – well, you need a good plan for your experiment.\nFirst, you start with your big question, the one you really want to answer. This is called your “research question.” It’s like the main thing you want to find out. Once you know your question, you need to make sure you do the experiment in the right way.\nThat’s where experimental design comes in. It’s like making a game plan for your experiment. You figure out what kind of data you need and how much of it. This is crucial because if you collect the wrong data or not enough, you might not get good answers.\n\n6.1.1 Principles of Experimental Design\nThere are a total of six to take note of:\n\nComparison\nComparison is like the main reason we do a science experiment. It’s the big “why” behind everything we’re testing. Imagine you want to know if something really works or if it’s different from how things normally are.\nIn science, we often compare two groups. It’s like having a “before” group and an “after” group. We call these groups different names, like “case vs. control,” “treatment vs. control,” or “mutant vs. wild-type.” These names tell us what we’re comparing.\nThe whole point of this comparison is to answer our main question. For example, let’s say we want to know if a new medicine really helps people get better. The “case” group would be the people getting the medicine, and the “control” group would be people not getting the medicine, so we can see if the medicine makes a big difference.\nReplication\nReplication is like a way to make sure our science experiments are trustworthy. When we measure things in experiments, there can be some mistakes or random errors. It’s kind of like when you play darts, and sometimes your throws are a bit off target even if you’re trying your best.\nTo deal with these errors, we do something called replication. It’s a bit like having a backup plan. We do the same experiment multiple times to see if we get similar results each time. We assume that each time we do the experiment, it’s like picking samples randomly from a big bag of marbles. This way, we can make sure our results aren’t just by chance.\nReplication is super important because it helps us figure out if the differences we see in our experiments are for real or if they could have happened just because of random stuff. The more times we repeat the experiment (we call these repeats “replicates”), the more sure we can be about our results.\nBut here’s the tricky part: doing replication, especially in some types of experiments like sequencing, can be a bit expensive. So, scientists often aim to do three or more replicates per condition to strike a balance between cost and getting reliable results.\nBlocking\n\n\n\n\n\nExample of a Blocking Experiment\n\n\n\n\nBlocking in science is like a clever way to make our experiments more accurate. Imagine you’re doing an experiment, and there are some things that you know can affect the results but aren’t really what you’re studying. Blocking helps you deal with those things.\nHere’s how it works: You group together similar things or “units” into special groups called “blocks.” These blocks are like teams of players in a game, and each team is as similar to each other as possible. So, if you’re studying plants, you might have one block of plants that are all the same age, another block of plants that have the same amount of sunlight, and so on.\nThe cool part is that by doing this, you’re making sure that any differences in your results are more likely to be because of the thing you’re actually studying. It’s like playing a game of soccer on a field with no bumps or slopes – it helps you focus on the game without worrying about uneven ground.\nRandomization\nRandomization in science is a bit like mixing things up randomly. Imagine you’re sharing candies with your friends, and you want it to be fair. So, instead of giving candies to your friends in a specific order, you close your eyes and give them candies randomly.\nIn experiments, randomization means doing things without any particular order or plan. It’s like flipping a coin to decide where each part of the experiment goes. For example, if you’re testing a new medicine, you don’t want to give it to one group of people just because they’re your friends. You want to make sure it’s all fair. So, you randomly choose who gets the medicine and who doesn’t.\nWhy is this important? Well, sometimes there are hidden things that can affect the results, things we might not even know about. Randomization helps us break any sneaky patterns or dependencies in the data. It’s like making sure everyone has an equal chance to be in any group, so we can be more sure that our results are because of what we’re testing, not because of something else.\nOrthogonality\nOrthogonality in science is a bit like doing different things in an experiment without them getting all mixed up. Imagine you’re painting a picture with different colors, and you want to know how much each color adds to your artwork. You don’t want the colors to blend together and make it hard to figure out their impact.\nSo, what scientists do is make sure that each thing they’re testing in an experiment can be measured separately, like having separate paintbrushes for each color. This way, they can figure out the effect of one thing without it being influenced by the others.\nLet’s say you’re testing three different types of fertilizers on plants. You want to know how each fertilizer affects the plant’s growth. Orthogonality means you can figure out the impact of each fertilizer without them interfering with each other. It’s like saying, “Okay, I’ll test fertilizer A, see the results, and it won’t mess with the results of fertilizer B or C.”\nFactorial Experiments\nImagine you’re in a science experiment where you want to know how different things, like temperature and humidity, affect plant growth. Instead of changing only one thing at a time, like just temperature or just humidity, you decide to change both things together. That’s what we call a factorial experiment.\nFactorial experiments are like doing science experiments with multiple things happening at once. It’s like trying different combinations of factors to see how they all work together. So, in our plant experiment, you might try high temperature with low humidity, low temperature with high humidity, and so on.\nThe cool thing about factorial experiments is that they help us figure out not only how each factor affects things but also how they interact. It’s like saying, “When we change both temperature and humidity, do they work together to affect plant growth in a special way?”\n\n\n6.1.1.1 Why Are These Principles Important?\nThese principles are super important in science because they help us do experiments in a smart and reliable way. Think of them as rules that scientists follow to make sure their experiments are accurate and make sense.\n\nModels for Analyzing Data\nWhen we collect data from experiments, we need to analyze it to understand what’s happening. These principles help us build models or methods for analyzing the data. It’s like having a special tool to figure out the answers from all the numbers we collect.\nt-test and Two-way ANOVA\nThese are fancy names for methods we use to see if the results of our experiments are meaningful. The t-test helps us compare two groups to see if they are really different. The two-way ANOVA is like a tool to compare more than two groups. They help us know if what we see in our experiments is likely to be true or just random.\nEstimating Statistical Power\nThis is like having a magic crystal ball to tell us how many times we need to do an experiment to be sure of our results. It helps us plan ahead. For example, it tells us how many times we should flip a coin to be sure if it’s really fair or not.\n\n\n\n\n6.1.2 Experiment Designs for RNA Sequencing Data\nWhen conducting RNA sequencing experiments, it’s important to plan things carefully. You need to make sure you have the right kind of data to answer your scientific questions effectively. To do this, there are some basic things you should keep in mind.\nFirstly, you need to collect enough data points, which we call replicates. It’s generally recommended to have at least three replicates to ensure the results are reliable. Think of it like taking multiple measurements to be more certain of an answer.\nSecondly, you should have sensible controls in your experiment. These controls help you compare what happens when you change something (like a treatment) to what happens when nothing changes (the control group). Controls are like a reference point to understand your results better.\nNow, let’s talk about the amount of data you need. It’s essential to have enough sequencing coverage. Think of sequencing coverage as how many times you read each piece of RNA. More coverage can give you a clearer picture of what’s happening in your samples.\nNext, consider the type of RNA you’re interested in. There are different methods to extract RNA, like total RNA or poly-A enrichment. Depending on your research question, you’ll choose the method that suits you best. For instance, in mammalian cells, most of the RNA is ribosomal RNA (rRNA), transfer RNA (tRNA), and only a small part is protein-coding mRNA. But if you’re interested in non-coding RNA, like microRNA, poly-A enrichment may not capture those.\n\n6.1.2.1 Biological and Technical Replication\nData can vary in one of two ways:\n\nBiological Variations\n\n\n\n\n\nExample of a Biological Replication\n\n\n\n\nThis just refers to variations in the data because of the test subjects themselves. This is the stuff that we’re interested in.\nTechnical Variations\n\n\n\n\n\nExample of a Technical Replication\n\n\n\n\nThis is variation because of how the data was collected (i.e., the experiment’s methodology). This is usually stuff that we don’t want (i.e., or unwanted noise in fancy speak).\n\n\n\n6.1.2.2 RNA Samples in the Laboratory\nWhen it comes to the performance of experiments in the lab, there are a few crucial things to consider, especially when dealing with biological samples.\nFirstly, it’s important to keep in mind that once you’ve collected your cell samples, you often can’t separate different types of cells from each other later on. So, it’s best to have the purest samples possible from the start. Imagine trying to unmix colors once they’ve been mixed together – it’s quite challenging.\nAnother important factor is the potential degradation of mRNA, which is a type of genetic material. To prevent this, you need to be careful when collecting your samples. It’s essential to harvest the cells and store them quickly. Think of it like putting perishable food in the refrigerator to keep it fresh. Using things like liquid nitrogen or special RNA-preserving solutions (like RNAlater) can help protect the mRNA from breaking down.\nLastly, everyone in the lab needs to follow proper procedures and guidelines. This is like having a recipe when you’re cooking – you need to follow the steps precisely to get the best results. Proper guidance and training for lab personnel ensure that experiments are done correctly and reliably."
  },
  {
    "objectID": "chapters/week6.html#de-novo-rna-sequence-assembly",
    "href": "chapters/week6.html#de-novo-rna-sequence-assembly",
    "title": "6  Transcriptomes: Experiment Design and RNA Sequence Data Processing",
    "section": "6.2 De novo RNA-Sequence Assembly",
    "text": "6.2 De novo RNA-Sequence Assembly\n\n\n\n\n\nExample of an RNA Assembly\n\n\n\n\nWhen you’re tasked with something called de novo assembly, it’s a bit like putting together a jigsaw puzzle without the picture on the box to guide you. But in this case, it’s not about puzzles; it’s about assembling genetic information, which is similar to what scientists do when they put together a genome, which is like a biological instruction manual.\nTo help with this task, scientists use tools that are like special equipment for assembling genetic information. These tools are actually variations of the ones used for genome assembly, kind of like how you might use different tools to build different types of things. One important thing to know is that all of these tools use de Bruijn graphs, which are like diagrams that help organize the genetic pieces.\nNow, here’s a tricky part: when you’re doing de novo assembly for RNA, the result can be more fragmented compared to assembling a genome. Think of it like trying to put together a map of a city where the roads are broken into smaller pieces; it can be a bit more challenging. But scientists are continually improving these tools to make the process as accurate as possible.\n\n6.2.1 Some Issues to Consider\nFirstly, just like assembling genomes, RNA de novo assembly is quite computationally demanding. It requires a lot of computer power, including parallel processing and a substantial amount of memory. Think of it like needing a powerful computer to handle complex tasks, like running high-end video games.\nTo tackle this task, scientists use specific software tools designed for RNA assembly, such as Trans-ABySS, Trinity, Oases, and SOAPdenovo-trans. These tools are like specialized software programs created to help with the assembly process, much like using software for video editing or graphic design.\nOne challenging aspect of RNA de novo assembly is that it can be difficult to separate different types of splice variants when you don’t have a complete genome available for reference. Splice variants are like different versions of a gene’s instructions, and without the full genome, it’s harder to tell them apart.\nAfter the assembly, researchers often cluster the results into gene models. This is like organizing different puzzle pieces into groups that belong together, helping make sense of the genetic information.\nLastly, there are versions of RNA de novo assembly that are guided by a known genome. These versions use information from a reference genome to estimate the transcriptome, which is like having a map to help put the genetic puzzle together more accurately. It’s like using a reference book to solve a tricky crossword puzzle."
  },
  {
    "objectID": "chapters/week6.html#rna-sequencing-data-processing",
    "href": "chapters/week6.html#rna-sequencing-data-processing",
    "title": "6  Transcriptomes: Experiment Design and RNA Sequence Data Processing",
    "section": "6.3 RNA Sequencing Data Processing",
    "text": "6.3 RNA Sequencing Data Processing\n\n\n\n\n\nGeneral Workflow for Dealing with RNA Sequencing Data\n\n\n\n\nThe first steps to this are all super similar to the previous weeks’ lecture on quality control and read trimming (i.e., lecture 3).\n\n6.3.1 Read Mapping\nWhen we talk about aligning reads to a genome, it’s like trying to find the right spot for puzzle pieces in a jigsaw puzzle, but with a twist. The problem is that the puzzle pieces (the reads) might have some mistakes or differences, and the puzzle board (the genome) can also have variations.\nThe big challenge here is finding the best, most accurate spot for lots and lots of puzzle pieces (reads) when they might not fit perfectly because of errors or differences in the genetic code. Imagine doing this with thousands of pieces – it can get pretty complicated!\nTo solve this puzzle, scientists use a general solution. They know that the puzzle board (genome) stays the same for every piece they’re trying to place. So, they create something like an index or a map that helps them find the right spots quickly. It’s similar to having a map of a city, and no matter how many people are looking for different places, they can all use the same map because the city doesn’t change.\n\n6.3.1.1 Aligning Reads to the Genome\nhen we talk about aligning reads to a genome, it’s like trying to match pieces from one puzzle (the reads) to another (the genome). There are a few ways to do this, like using a tool called BLAST, which is similar to trying to match puzzle pieces by hand. However, BLAST can be quite slow when dealing with a large number of reads, making it impractical for many scientific purposes.\nTo speed up the process, scientists have come up with a better solution, like using something called the Burrows-Wheeler transform. This is like having a special machine that can quickly and efficiently find the best matches between the puzzle pieces (reads) and the puzzle board (genome).\nScientists have implemented this Burrows-Wheeler transform in software tools such as BWA-mem, Bowtie2, and SOAP. These tools are like computer programs that use the special machine to find matches between the reads and the genome, making the process much faster and more practical for scientific research.\n\n\n\n6.3.2 Splicing Aware Aligners\nThe thing about tools that directly map reads to the genome is that they don’t really work for RNA sequencing (at least not when it comes to RNA sequencing data that hasn’t been processed in some way). So, there are tools like HISAT2, Tophat2, and STAR aligners.\n\n6.3.2.1 Tophat\n\n\n\n\n\nInner Workings of Tophat\n\n\n\n\nTophat is like a detective for your genetic data. Its main idea is to figure out which parts of your genetic material, called “reads,” can be matched to the genome. Think of the genome as a big instruction manual, and the reads are like pieces of text from that manual.\nWhat Tophat does is it finds the reads that perfectly match the genome. These matching reads represent the “exons” of genes, which are like specific paragraphs in the instruction manual. Tophat identifies these exons and helps scientists understand which parts of the genome are being actively used by the cell.\nBut here’s where it gets interesting. Tophat also looks at the reads that don’t match the genome. These unmatched reads are like clues in a mystery novel. They suggest that something special is happening, called splicing. Splicing is like rearranging paragraphs in the instruction manual to create different versions of the gene’s instructions. Tophat helps identify these splicing sites, which are crucial for understanding how genes work.\n\n\n6.3.2.2 STAR\nSTAR works in several ways:\n\nIndexing\nImagine you have a giant book, which is like the genome. STAR’s first step is to create an index for this book. This index is like a quick reference guide that helps you find specific information in the book much faster.\nNow, think of the genetic reads as sentences from the book. STAR’s job is to find the best match for these sentences in the book. It does this by looking for the longest sequence of words in the sentences that match something in the book.\n\n\n\n\n\nUnmapping Parts of STAR\n\n\n\n\nBut here’s where STAR does something clever. It doesn’t just stop at finding the best matches; it also looks at the parts of the sentences that don’t match anything in the book. It’s like finding sentences that are partly in another language or from a different book. These unmatched parts can tell you something important, like hints or clues.\nClustering and Stitching\nImagine you have a bag of puzzle pieces, and each piece is like a small part of a picture. STAR’s job is to group these pieces together in a smart way.\nFirst, STAR looks at the pieces that are close to each other, like putting together pieces that are next to each other in the puzzle. This is called clustering. It’s like collecting all the sky pieces, all the grass pieces, and so on, so you can start building sections of the puzzle.\nBut not all puzzle pieces fit perfectly right away. Some might have a small gap or a piece missing, similar to how genetic reads might have small differences. STAR doesn’t give up; it’s like a puzzle master who can adjust and fill in the gaps, even when there are mistakes or missing parts. This process of adjusting and matching is called stitching.\n\n\n\n\n6.3.3 Quantification for RNA Sequence Analyses\nThink of this process as putting together a jigsaw puzzle with some special helpers.\nFirst, you have your genetic reads, which are like puzzle pieces. Tools like TopHat, HISAT, or STAR are like expert puzzle solvers. Their job is to figure out how these reads match up with the reference genome, similar to how puzzle solvers match pieces to the picture on the puzzle box.\nOnce these tools have aligned the reads, they not only assign the reads to specific parts of genes (exons), but they also figure out how these gene parts connect, kind of like finding the links between pieces in a puzzle.\n\n\n\n\n\nWorkflow in Cufflinks\n\n\n\n\nNow, think of Cufflinks or Stringtie as the experts who take these partially solved puzzles and complete them. They estimate what the complete gene models should look like based on the connections found by the alignment tools. It’s like creating a full picture from the partially assembled puzzle.\nMoreover, Cufflinks or Stringtie also quantify how active each gene is, sort of like measuring how bright different parts of the puzzle are. This helps scientists understand which genes are working hard and which ones are not as active."
  },
  {
    "objectID": "chapters/week6.html#pseudoalignment",
    "href": "chapters/week6.html#pseudoalignment",
    "title": "6  Transcriptomes: Experiment Design and RNA Sequence Data Processing",
    "section": "6.4 Pseudoalignment",
    "text": "6.4 Pseudoalignment\nImagine you have a library of books, and each book represents a gene in the transcriptome. Instead of reading every word in the books (which can be time-consuming), this method is like flipping through the pages quickly to see which books are a good match for a specific sentence (the genetic read).\n\n\n\n\n\nIllustration of What Kallisto Index Does\n\n\n\n\nThe advantage of this approach is speed. It’s much faster because it doesn’t require reading every word in every book (gene) like traditional methods. Instead, it quickly figures out which books (transcripts) the sentence (read) might belong to.\nHowever, there are some downsides to this speedier approach. One drawback is that it may not provide as detailed information as the traditional method. It tells you which books (transcripts) the sentence (read) could belong to, but it doesn’t pinpoint the exact location within each book (transcript). So, you lose some precision.\nTo make this rapid searching possible, these tools employ something called de Bruijn graphs. Think of this as a clever way of organizing information for quick searches, like using an efficient filing system to find documents in a library.\nTwo popular tools for this approach are kallisto and salmon. They take two main steps: First, they create an index, which is like building a fast-reference guide for the library of books. Then, they perform the search, quickly identifying which books (transcripts) match the sentences (reads).\n\n6.4.1 Quantification\nImagine you have a bunch of puzzle pieces (genetic reads) and you’ve already figured out which parts of the puzzle (genes) these pieces belong to using a fast mapping method. However, there’s a bit of complexity here. Sometimes, one puzzle piece (read) can fit into multiple puzzles (gene models).\nThis is where pseudoalignment comes in, and it’s somewhat similar to what tools like Cufflinks do. It’s like trying to figure out how many pieces of each puzzle you have, but with a twist. Instead of directly counting the pieces, you use a sophisticated approach, kind of like a detective trying to solve a mystery.\nIn this approach, you create a probabilistic model, which is like a set of rules that help you estimate how many pieces of each puzzle you have. These rules are based on the idea of maximum likelihood, which means finding the most likely answer given the data you have. It’s a bit like making an educated guess about the number of pieces for each puzzle based on the clues you’ve gathered.\nIn practical terms, you want to find the best estimates for transcript abundances, which is like figuring out how many pieces belong to each puzzle. You do this by making sure that the observed coverage of gene parts (exons) in your reads matches the model’s predictions as closely as possible."
  },
  {
    "objectID": "chapters/week6.html#normalization",
    "href": "chapters/week6.html#normalization",
    "title": "6  Transcriptomes: Experiment Design and RNA Sequence Data Processing",
    "section": "6.5 Normalization",
    "text": "6.5 Normalization\nImagine you have two baskets, and each basket contains different types of fruits. The goal is to compare the number of fruits in each basket, but there are two challenges:\n\nDifferent Library Sizes\nIn one basket, there may be more fruits, and in the other, there could be fewer. This difference in the total number of fruits in each basket makes it tricky to compare them directly. It’s like trying to figure out which basket has more without considering their sizes.\nDifferent Gene Lengths\nNot all fruits are the same size; some are larger, and some are smaller. Similarly, genes in genetic data can have varying lengths. This complicates comparisons because you need to account for these differences in gene sizes when comparing the two baskets.\n\nNormalization in RNA-seq is like a magic tool that helps you address these problems. It’s like resizing the baskets and the fruits so that you can make a fair comparison.\nFor the first issue, normalization adjusts the basket sizes so that they’re the same, making it easier to compare the number of fruits in each. This way, you’re not unfairly favoring one basket over the other just because it started with more or fewer fruits.\nFor the second issue, normalization scales the size of the fruits (gene lengths) so that they are comparable. It’s like resizing the fruits to a common standard, ensuring that you’re not giving more weight to larger fruits when comparing the two baskets.\n\n6.5.1 Transcripts per Million (i.e., TPM)\nImagine you have a collection of different-sized recipe books (genes), and you want to know how popular each recipe book is in terms of the number of recipes inside (reads). However, the recipe books vary in size, making it challenging to compare them directly.\nTPM normalization is like a clever way of standardizing and making these numbers “prettier” for easy comparison.\nHere’s how it works:\n\nDivide Read Counts by Gene Length\nFirst, you count the number of reads that belong to each recipe book (gene). To make things fair, you divide these counts by the length of each recipe book in kilobases (KB). This gives you “reads per kilobase” (RPK), which is like figuring out how many recipes are in each kilobase of the book.\nSum RPK and Scale to a Million\nYou add up all the RPK values for all the recipe books (genes). This sum represents the total number of “recipes per kilobase” (sum RPK). To make the numbers look neater, you divide this sum by a million, creating a scaling factor (S).\nCalculate TPM\nFinally, to find the Transcripts per Million (TPM) for each recipe book (gene), you divide the RPK value for that gene by the scaling factor S. This normalization tells you how abundant each gene is relative to the total number of recipes per million kilobases.\n\nIn simpler terms, TPM is like a way of comparing how popular each recipe book is, taking into account their sizes. It’s the preferred method for RNA-seq normalization nowadays because it helps describe the relative abundance of reads assigned to each gene in a library. So, it’s like getting a clear picture of which recipe books are the most popular in your collection, considering their different sizes."
  },
  {
    "objectID": "chapters/week7.html",
    "href": "chapters/week7.html",
    "title": "7  Transcriptomes: Differential Expression to Enrichment Analyses",
    "section": "",
    "text": "Let’s explore how we go from raw read counts to identifying differential gene expression in a straightforward way.\nImagine you have a collection of different-sized baskets, each containing different numbers of apples (gene counts). Your goal is to figure out which baskets have more or fewer apples, but you need a consistent way to do this.\nNow, in the world of gene expression analysis, different software tools come with their own methods for counting genes, but they typically start with unnormalized count data, just like having apples in baskets without any adjustments.\nIn your analysis, when you use a method like pseudoalignment, it not only provides raw counts (the number of apples in each basket) but also normalized values called TPM (Transcripts per Million), which are like having apples per million baskets.\nHowever, when it comes to finding genes that are differentially expressed (meaning they have significantly more or fewer apples), we usually work with the raw count data, not TPM values. It’s like counting apples directly rather than adjusting for the number of baskets.\nHere’s a standard workflow:\nIn essence, you’re like a fruit analyst, taking the raw count data, adjusting it if needed, and using statistical models to figure out which baskets have significantly different numbers of apples. This is how we identify differential gene expression, helping us understand which genes are playing a more significant role in specific conditions or situations."
  },
  {
    "objectID": "chapters/week7.html#fundamental-metrics",
    "href": "chapters/week7.html#fundamental-metrics",
    "title": "7  Transcriptomes: Differential Expression to Enrichment Analyses",
    "section": "7.1 Fundamental Metrics",
    "text": "7.1 Fundamental Metrics\n\n7.1.1 Fold Change\nLet’s discuss the concept of “Fold Change” and how it serves as a measure of differential gene expression.\nImagine you have two baskets of apples: one from a case scenario (like a treatment group) and the other from a control scenario (like a standard group). You want to know if the number of apples in the case basket is different from the control basket. Fold change is a simple way to express this difference.\nHere’s how it works:\n\nCalculating Fold Change\n\\[\\begin{equation}\n  \\log\\left(\\frac{\\text{Case}}{\\text{Control}}\\right) = \\log\\left(\\text{Case}\\right) - \\log\\left(\\text{Control}\\right)\n\\end{equation}\\]\nTo calculate fold change, you take the number of apples (gene counts) in the case basket and divide it by the number of apples in the control basket. This ratio tells you how many times more (or less) apples there are in the case compared to the control.\nLog2 Transformation\nFold changes are often reported in the log2 domain because it has some useful properties. When you take the logarithm of the fold change, it simplifies the interpretation. Also, it’s symmetric, meaning that if a gene is upregulated (more apples in the case), it will have a positive value, and if it’s downregulated (fewer apples in the case), it will have a negative value.\nRelationship to Difference\nThe \\(\\log2\\) of the fold change is related to the difference between the case and control. It’s like expressing the change in the number of apples as a difference in their logarithms.\nGeometric Mean\n\\[\\begin{equation}\n  \\log\\left(\\sqrt{\\text{Case} \\cdot \\text{Control}}\\right) = \\frac{\\log\\left(\\text{Case}\\right) + \\log\\left(\\text{Control}\\right)}{2}\n\\end{equation}\\]\nAnother way to express fold change is to use the geometric mean of the apples in the case and control baskets. This is like finding the average number of apples between the two baskets, but in the \\(\\log2\\) domain.\n\nHowever, fold change has limitations. It works well when you have only one measurement, but it’s not ideal when your data is very noisy or when you’re dealing with complex datasets. Noise can lead to misleading results, which is why more sophisticated statistical methods are often used for differential gene expression analysis in RNA-seq data.\n\n\n7.1.2 MA Plots\n\n\n\n\n\nExamples of MA Plots\n\n\n\n\nImagine you have data points on a graph, where one axis represents the log fold change (how much something has changed on a logarithmic scale, like we discussed earlier), and the other axis represents the mean expression (the average level of something, like the average number of apples in our baskets).\nNow, when you look at this scatterplot of log fold change, it might be a bit hard to interpret because it’s not as intuitive as seeing the actual fold change values directly. The MA plot comes to the rescue.\nHere’s how it works:\n\nTransforming the Axes\nThe MA plot essentially takes the scatterplot and rotates the axes by 45 degrees. This means that the log fold change values are now on one axis (the M-axis), and the mean expression values are on the other axis (the A-axis).\nM-A Axes\nThe M-axis typically represents the difference between the log fold change in the case and control groups, while the A-axis represents the average of the log fold change values.\nInterpretable Plot\nNow, instead of looking at log fold change values, you’re looking at something more intuitive – the actual fold change values (like we discussed earlier) on one axis, and the mean expression levels on the other axis. This makes it easier to see how much things have changed in terms of fold change, relative to the average expression."
  },
  {
    "objectID": "chapters/week7.html#testing-significance",
    "href": "chapters/week7.html#testing-significance",
    "title": "7  Transcriptomes: Differential Expression to Enrichment Analyses",
    "section": "7.2 Testing Significance",
    "text": "7.2 Testing Significance\nImagine you have two groups of apples, like a treatment group and a control group. You want to know if there’s a real difference in the number of apples between these two groups or if any difference could have occurred by chance.\nHere’s how significance testing works:\n\nConstruct a Null Hypothesis\nThis is like making an educated guess based on the idea that there’s no real difference between the two groups. In a two-sample case (like treatment vs. control), the null hypothesis would be something like “the mean number of apples in the treatment group is the same as the mean number of apples in the control group.” In a one-sample case, it might be “the mean number of apples is not different from zero.”\nTest the Hypothesis\nYou perform statistical tests to see if this null hypothesis holds. The tests rely on certain assumptions about the data, such as following a known distribution (e.g., Student’s t-distribution for comparing means) and having a test statistic with a known distribution under the null hypothesis.\nP-value\nAfter running the statistical test, you get a p-value. This p-value tells you the probability of obtaining the data you have (or more extreme data) by chance alone, assuming that the null hypothesis is true. In simple terms, it’s like asking, “How likely is it that the observed difference in apples between the two groups happened purely by random chance?”\nInterpreting the P-value\nIf the p-value is small, typically less than 0.01 or 0.05 (but this threshold can vary), it suggests that the difference you observed is unlikely to have occurred purely by chance. In other words, it’s statistically significant. This means you have evidence to reject the null hypothesis and conclude that there’s likely a real difference between the groups.\n\n\n7.2.1 P Values\n\n\n\n\n\nIllustration of a P-Value\n\n\n\n\nImagine you’re playing a game with a coin. You want to know if the coin is fair or biased, meaning it might not land on heads and tails with equal probability. To test this, you flip the coin a bunch of times and record the outcomes.\nNow, the null hypothesis here is that the coin is fair. In other words, there’s no bias; it’s just like any other regular coin. The alternative hypothesis is that the coin is biased.\nHere’s how p-values come into play:\n\nCalculating the p-value\nWhen you calculate a p-value, you’re essentially figuring out the probability that you would get the results you did (like getting heads or tails a certain number of times) if the null hypothesis were true. In other words, it’s like asking, “What’s the chance that the coin appears biased purely by coincidence?”\nInterpreting the p-value\nIf the p-value is small (typically less than 0.01 or 0.05), it suggests that the results you observed are unlikely to have occurred purely by chance. So, if you find a very small p-value, you might conclude that there’s evidence against the null hypothesis (the coin is fair), and you start to consider the alternative hypothesis (the coin is biased).\n\nIn gene expression analysis, p-values work similarly. They tell you the chance that a gene is categorized as differentially expressed (meaning its expression levels have changed significantly) purely by coincidence or random variation.\nFor example, if you’re comparing gene expression in two groups, a small p-value suggests that the difference you observed in gene expression is unlikely to have happened by random chance alone. It provides evidence that the gene is genuinely differentially expressed between the groups.\n\n\n7.2.2 Differences Between Conditions\nLet’s explore two common statistical tests used to determine if there is a significant difference between two conditions in gene expression analysis, the Student’s t-test and the Wilcoxon Rank Sum (or Wilcoxon Mann-Whitney) test.\nImagine you have data from two groups, like a treatment group and a control group, and you want to know if there’s a real difference between them.\n\nStudent’s t-test:\n\nTesting Means: The Student’s t-test is like a tool for comparing the means (averages) of the two groups. It tells you if the difference in means between the groups is statistically significant.\nTypes of t-tests: There are various versions of the t-test, like the two-sample t-test (comparing two groups), one-sample t-test (comparing one group to a known value), and variations that account for assumptions like equal or unequal variances between the groups.\nAssumption: The t-test assumes that the data is normally distributed, meaning it follows a typical bell-shaped curve around the mean. This assumption is important because the test relies on normal distribution properties.\n\nWilcoxon Rank Sum (or Wilcoxon Mann-Whitney) test:\n\nTesting Medians: The Wilcoxon Rank Sum test, also known as the Mann-Whitney test, is different from the t-test. It compares the location, specifically the medians, of the two groups. It tells you if the medians are significantly different.\nTypes of Wilcoxon tests: Like the t-test, you can use variations of the Wilcoxon test, including two-sample Wilcoxon (comparing two groups) and one-sample Wilcoxon (comparing one group to a known value).\nAssumption: The Wilcoxon test doesn’t rely on the assumption of normal distribution. Instead, it works well when the distributions are symmetric and similar.\nSensitivity: The Wilcoxon test is often considered less powerful (meaning it’s less likely to detect a true difference) compared to the t-test when the data is close to a normal distribution. However, it’s more robust when dealing with data that doesn’t meet the normal distribution assumption.\n\n\nIn summary, both the Student’s t-test and the Wilcoxon Rank Sum test are used to determine if there’s a significant difference between two conditions. The t-test focuses on means and assumes normal distribution, while the Wilcoxon test examines medians and doesn’t rely on normal distribution assumptions. The choice between them depends on your data and whether the assumptions of the t-test are met.\n\n\n7.2.3 Models to Analyze RNA Sequencing Data\n\n\n\n\n\nModel Transformation for RNA Sequencing Data\n\n\n\n\nWhen we’re working with RNA sequencing data, we use something called generalized linear models to transform our data to follow a Gaussian distribution (i.e., a bell curve) - especially error values in transcription to positive values.\nSuch a transformation often takes into account the fact that the data does not follow a Gaussian distribution. Several tools have different ways of doing this:\n\nUsing theory\nTools like EdgeR and DESeq2 (i.e., from BioConductor in R) use the Negative Binomial distribution or the Poisson distribution (e.g., PoissonSeq does this).\n\n\n\n7.2.4 Biological vs. Statistical Significance\nWhen we talk about biological significance and statistical significance, we’re trying to figure out if something we see in our data is not only mathematically significant but also meaningful in the real world of biology. You see, sometimes the numbers we get from our experiments might be statistically significant, which means they’re different enough to catch our attention, but they might be so tiny that they don’t actually matter in the bigger picture of how a living thing functions.\nTo decide if something is genuinely important in biology, we need to consider two things. First, we look at the average change or difference we observe, and second, we check if this change is statistically significant. Think of it like this: we’re not just interested in whether something is different; we also want to know if that difference is big enough to make a real impact.\nTo help us figure this out, scientists often use a rule of thumb: if the change is at least ±1 log2-ratio, it’s usually considered biologically meaningful. But here’s the tricky part: this “meaningful” threshold can vary depending on what kind of cells we’re studying (like brain cells versus liver cells) and what we’re investigating in our experiment (like subtle changes in how a gene works versus genes that react really strongly to something).\n\n\n7.2.5 Multiple Testing\nThe thing about high throughput bioinformatics is that we usually need to have a lot of tests to measure significances between different things. However, the problem with such tests is that genes that are said to be significantly expressed may actually be false positives in disguise.\n\n\n7.2.6 Classical Correction Methods\nImagine you’re in charge of testing a bunch of different things, like trying to figure out which light switches in a giant building work. If you just randomly flip switches and say, “This one works!” you might make mistakes. So, you set a rule: if you’re going to say a switch works, you want to be pretty darn sure it’s working, to avoid false alarms.\nIn the world of statistics, when we do lots of tests (like checking the switches), we want to make sure we’re not saying things are true when they’re not. That’s where these correction methods come in:\n\nSidak Correction and Bonferroni Correction\n\\[\\begin{align}\n  \\text{Sidak } \\alpha_e &= 1 - \\sqrt[m]{1 - \\alpha_e} \\\\\n  \\text{Bonferroni } \\alpha_e &= \\frac{\\alpha_e}{m}\n\\end{align}\\]\nThese are like rules we use to decide how confident we need to be before we say something is true (like a switch works). The idea is to set a threshold for each test. If the chance of being wrong (a false positive) is too high, we won’t call it true.\nFamily-wise Error Rate\nThis is just a fancy way of saying that we’re looking at the overall rate of false positives when we do many tests. We don’t want too many switches to be labeled as working when they’re not.\n\nNow, here’s the catch: these methods are sometimes seen as overly cautious. Why? Well, first, the tests we do aren’t always completely independent, meaning the results of one test might affect the next one. Second, they control for something a bit different: the chance that all the tests are correct, not just one. So, sometimes they might make us miss switches that actually work because we’re being too strict.\n\n\n7.2.7 False Discovery Rate\nImagine you’re a detective trying to solve a mystery. In this mystery, you’re looking for clues in different places, and you want to make sure you catch as many real clues as possible, but you’re okay with the idea that you might pick up a few fake clues along the way.\nNow, in the world of statistics, when we’re doing lots of tests or looking for lots of clues, we have to decide how strict we want to be about avoiding false leads (false positives).\nTraditionally, we used to focus on controlling the family-wise error rate (i.e., FWER), which meant we wanted to make sure we didn’t make any mistakes anywhere in our tests. It’s like saying, “I don’t want a single false clue in my investigation.”\n\\[\\begin{equation}\n  \\text{\\#False Discovery Rate} = \\frac{\\text{\\#False Positives}}{\\text{\\#True Positives + \\#False Positives}}\n\\end{equation}\\]\nBut sometimes, being too strict about this can make us miss out on real discoveries. So, instead of worrying too much about catching every single error, we can control the false discovery rate (i.e., FDR). This approach allows us to be a bit more flexible. It means that we’re okay with accepting a certain proportion of our significant findings as potentially false positives.\n\n7.2.7.1 Benjamini-Hochberg Correction\nImagine you’re a scientist working with a lot of data, like testing many genes in an experiment. You want to make sure that when you say a gene is significant, it’s not just a random chance finding. The Benjamini-Hochberg correction helps you do just that.\nHere’s how it works:\n\nChoose Your Significance Level: First, you decide how strict you want to be. This is like setting a rule that says, “I want to be this confident before I call something significant.” For example, you might choose a 5% significance level, which means you’re willing to accept a 5% chance of making a mistake.\nCompute Many Individual Tests: Next, you run lots of individual tests, one for each gene you’re studying. These tests tell you how likely it is that each gene’s results are just random.\nSort the p-values: Now, you have a bunch of p-values, which are like scores that tell you how likely it is that something is significant. You sort these p-values from smallest to largest, like arranging them in order.\nCompute Gene-Specific Thresholds: Here’s where the Benjamini-Hochberg correction comes in. You calculate a specific threshold for each gene based on its p-value and the significance level you chose in step 1. This threshold helps you decide which genes are significant.\n\nHere’s what’s cool about this method: It’s a bit like being a careful scientist who wants to be sure about their findings. It doesn’t just apply the same rule to all genes; it customizes the rules for each gene based on its own data. So, some genes might have to meet a higher bar to be called significant, while others have a lower bar, depending on their p-values and the chosen significance level.\nIn simple terms, the Benjamini-Hochberg correction helps you avoid calling something significant when it’s not, while also being flexible enough to account for the individual uniqueness of each gene in your study. It’s like being both cautious and fair in your analysis of a big dataset.\n\n\n7.2.7.2 Q Values\nImagine you’re exploring a big forest looking for hidden treasures, but you’re also worried about accidentally finding something that isn’t a treasure.\nNow, when scientists like to discover things in their experiments, they’re also concerned about making mistakes, like saying they found something important when it’s not actually there. That’s where q-values come in.\n\nEstimating True Negatives: Q-values help scientists figure out how many times they might have made mistakes by calling something significant when it’s not. You can think of it as estimating the number of times they thought they found a treasure, but it turned out to be just a rock. This is something that the Benjamini-Hochberg method doesn’t really focus on.\nQ-values vs. P-values: Q-values are a bit like p-values, which you can think of as scores for how likely something is to be important. But q-values go a step further. They take a bunch of these p-values and put them in order. Then, they help scientists understand how many of these “important” things might actually be mistakes."
  },
  {
    "objectID": "chapters/week7.html#differentially-expressed-gene-and-biology",
    "href": "chapters/week7.html#differentially-expressed-gene-and-biology",
    "title": "7  Transcriptomes: Differential Expression to Enrichment Analyses",
    "section": "7.3 Differentially Expressed Gene and Biology",
    "text": "7.3 Differentially Expressed Gene and Biology\n\n7.3.1 Tool Recommendations\n\n7.3.1.1 Sahraeian et. al\nWhen it comes to analyzing RNA-seq data, there are some recommendations from a guy called Sahraeian and his team that can help us understand which tools work well. Imagine you have a toolbox, and you want to know which tools are the best for the job.\n\nQuantification\nOne of the things they looked at was how well different tools work to measure gene expression. They found that tools like HISAT2 and TopHat are better at this job compared to STAR. Think of it like some wrenches being better at turning bolts than others.\nEfficiency Matters\nThey also found that some tools that don’t rely on traditional alignment methods (alignment-free tools) are quite efficient. However, when they combined a tool called StringTie with an efficient aligner like HISAT2, it turned out to be the most efficient approach. So, it’s like saying, sometimes it’s better to use two tools together to get the job done quickly.\nSpeed Differences\nSome tools were much faster than others, but they didn’t provide specific numbers in this study. Think of it as some tools being super-fast like a race car, while others are a bit slower like regular cars, but they didn’t give us the exact speed limits.\nDifferential Expression\nAnother important task in RNA-seq analysis is figuring out which genes are different between conditions. They found that a tool called edgeR did a great job at this and had a lower rate of falsely calling genes as different when they weren’t.\nBest Recommendation\nOverall, their top recommendation for differential expression analysis was a tool called DESeq2. It’s like saying, if you want to pick just one tool from your toolbox, DESeq2 might be the best all-around choice. But remember, the best tool can vary depending on the specific job or data you’re working with.\n\n\n\n7.3.1.2 Corchete et. al\n\n\n\n\n\nCorchete et. al’s Work\n\n\n\n\nIn this study by Corchete and their team, they explored various ways to process and analyze RNA-seq data to find differentially expressed genes. Think of it like they tried out many recipes to cook the same dish and wanted to know which one tasted the best.\n\nExploring Different Combinations\nThe researchers didn’t just stick to one way of analyzing the data; they tried a whopping 192 different methods or combinations of steps to see how they affected the results. Imagine if you were making a sandwich, and you tried using different types of bread, spreads, and fillings to find the most delicious combination.\nSamples and Validation\nTo make sure their findings were reliable, they used 18 samples from two different human cell lines. It’s like cooking the same meal multiple times to make sure you get consistent results. They also validated their findings by using a technique called qRT-PCR on the same samples, which is a bit like double-checking the taste of your dish to be absolutely sure it’s delicious.\nRecommended Preprocessing Steps\nAfter trying all these different recipes (or methods), they found that a combination of preprocessing steps worked well. It included tools like Trimmomatic, RUM, HTSeq Union, and TMM. Think of it as using specific ingredients and techniques in a recipe to make your dish turn out just right.\nThe Limma Trend\nAmong all the methods they tested, one called Limma trend consistently performed well. It’s like finding that a particular way of seasoning your dish always makes it taste great."
  },
  {
    "objectID": "chapters/week7.html#analysis-outcomes",
    "href": "chapters/week7.html#analysis-outcomes",
    "title": "7  Transcriptomes: Differential Expression to Enrichment Analyses",
    "section": "7.4 Analysis Outcomes",
    "text": "7.4 Analysis Outcomes\nThere are usually tens of, if not hundreds or even thousands of differentially expressed genes.\nWhen we’re working with RNA-seq data and trying to figure out which genes are significant, it’s like solving a puzzle. We have lots of genes, and for each one, we use a special test to check if it’s behaving differently under different experimental conditions. We also make sure our results are reliable by correcting the test results. What we end up with is a list of genes that seem to be important.\nBut here’s the thing: just having this list of genes might not give us the whole picture. It’s a bit like having puzzle pieces without knowing what the final picture looks like. And depending on the tool we use to analyze the data, the list of important genes can vary.\nNow, why are we doing all this in the first place? The main goal of RNA-seq is to understand how things change between different experimental conditions. We want to know what’s happening at a functional or molecular level. It’s like trying to figure out what’s going on inside a black box.\n\n7.4.1 Levels of Analysis\nWhen scientists are studying gene behavior, they look at things on different levels, a bit like zooming in and out with a microscope. It’s kind of like exploring a forest where you can start with the smallest details and gradually uncover the bigger picture.\n\nSingle Gene Level: At the smallest level, researchers focus on single genes. They want to know if a particular gene behaves differently when it’s exposed to a treatment or experiment compared to when it’s in its normal state (the control). It’s like looking closely at one tree in the forest to see if it’s growing differently because of something happening around it. This helps us understand how individual genes respond to changes and can be important for things like developing new drugs.\nMultiple Genes Level: Zooming out a bit, scientists ask questions about groups of genes. They wonder if there are sets of genes that work together or are somehow connected. For example, they might investigate if a bunch of genes all change their behavior in response to a treatment compared to a control. It’s like stepping back and observing a whole section of trees in the forest to see if they’re all affected in the same way. This helps us see broader patterns in how genes are working together.\nNetworks of Genes Level: Finally, when researchers zoom out even further, they’re looking at the big picture. They want to identify and understand the complex web of interactions between genes, which we call regulatory and signaling networks. It’s like studying the entire ecosystem of the forest, where you’re not just looking at individual trees but also how they all interact with each other, from the smallest plants to the largest animals. This level helps us grasp the underlying mechanisms that control gene behavior.\n\nSo, in essence, scientists use different “microscopes” to study genes, starting from the tiniest details and gradually uncovering larger and more complex interactions. It’s a bit like exploring a forest, from individual trees to the entire ecosystem, to truly understand how everything works together in the world of genetics.\n\n\n7.4.2 Gene Ontology (i.e., GO)\nWhen scientists want to understand how genes are connected to the jobs they do in our bodies, they use something called “Gene Ontology,” or GO for short. Think of it as a giant encyclopedia for genes that helps us make sense of what they do.\nNow, this GO encyclopedia isn’t just one big book; it’s divided into three separate parts, kind of like having different sections in a library:\n\nBiological Process: In this section, scientists describe what specific jobs or tasks genes are involved in when it comes to biological processes. It’s like reading about the roles of different workers in a big company.\nMolecular Function: Here, scientists explain the specific functions or abilities of genes at the molecular level. It’s like understanding the unique skills or talents of each gene, such as their ability to interact with other molecules.\nCellular Component: This section focuses on where genes are located or where they do their work inside cells. It’s like finding out which departments or rooms in a company each worker belongs to.\n\nBut that’s not all! There are also other ways to group genes based on their functions and properties. For example, we have pathways like KEGG, Metacyc, and Reactome, which are like different roads or routes genes can take in the body. We also have categories like protein families and even information about where genes are located on chromosomes, which is a bit like knowing the street addresses of different businesses in a city.\n\n\n7.4.3 How Does Gene Set Analysis Work?\nImagine you’re trying to figure out if a group of friends are all good at playing a game, but you only have a few chances to watch them play. If you try to judge each friend’s skills one by one, it might be really hard to tell if they’re good or not because your observations could be a bit shaky.\nNow, what if you look at how the group as a whole plays the game? It’s like putting all their skills together and looking at the big picture. This can make it easier to see if they’re doing well or not. That’s the basic idea behind analyzing gene sets.\nWhen scientists have limited data, like not enough measurements for each individual gene, it can be tough to tell if a single gene is acting differently. But when they group genes together based on certain criteria (like genes that are related to a specific process), they can see if that whole group is changing in a noticeable way. It’s like looking at how the entire team of friends is playing the game rather than just focusing on one person. This approach helps scientists spot big changes in gene groups more easily.\nThis way of looking at things can help scientists find out if a bunch of genes are working together or changing together, kind of like friends who play the same game in a similar way. Plus, based on experimental evidence, it seems that these group effects are often consistent across different experiments, while individual gene effects can be a bit trickier to pin down and reproduce."
  },
  {
    "objectID": "chapters/week7.html#enrichment-analysis",
    "href": "chapters/week7.html#enrichment-analysis",
    "title": "7  Transcriptomes: Differential Expression to Enrichment Analyses",
    "section": "7.5 Enrichment Analysis",
    "text": "7.5 Enrichment Analysis\nThis is a way scientists figure out if certain groups of genes are doing something special in a given situation.\n\nSingular Enrichment Analysis: Imagine you have a list of genes that you think are acting differently under certain conditions. In this approach, you assume you already know which genes are different and you have a specific set of rules for what counts as a significant change. You then test each group of genes one by one to see if they’re involved in any special processes or functions. It’s like checking each item on a shopping list to see if it’s available at a store. This approach treats each group of genes independently.\nGene Set Enrichment Analysis: Now, think of a scenario where you don’t have strict rules for what counts as a significant change. Instead, you compare the entire group of genes you’re interested in to lots of random groups to see if there’s something unique about them. It’s like comparing a special recipe to a bunch of random ones to see if it’s really unique and delicious. This approach doesn’t rely on specific thresholds; it’s more about comparing your group of genes to many random ones.\nModular Enrichment Analysis: In this approach, you already have a list of genes that you think are behaving differently, but you’re not sure how they’re related. So, you group these genes together based on how they’re described or annotated, kind of like sorting them into different boxes based on their similarities. Then, you see if each box (or module) of genes is involved in any special processes or functions. It’s like organizing your closet by grouping similar clothes together and then checking each group to see if they have a unique style. This approach focuses on the relationships between genes based on their descriptions.\n\n\n7.5.1 Singular Enrichment Analysis\nImagine you have a group of 30 genes, and out of these, a whopping 24 are labeled as “ribosomal proteins.” It might seem like something important is happening with ribosomal proteins in your experiment, right?\nBut hold on, there’s a twist. What if you knew that this group of 30 genes was originally part of a much larger group of 100 genes, and out of those, 90 were also “ribosomal proteins”? Suddenly, it doesn’t seem as extraordinary, does it?\nThat’s where the idea of considering the background comes into play. You see, just knowing how many genes are related to a particular biological process isn’t enough. You need to compare it to what you’d expect by chance. Are you seeing more of those genes than you’d anticipate? Or is it just a typical result?\nTo figure this out, scientists use a statistical test. It’s like having a way to check if the number of “ribosomal proteins” you found in your group of 30 genes is statistically significant, meaning it’s not just due to chance. It helps ensure that when you say something is happening with a biological process, you’re not making a false alarm based on random fluctuations.\n\n7.5.1.1 Fisher’s Exact Test\n\n\n\n\n\nTruth Matrix as Seen by Fisher\n\n\n\n\nLet’s take a journey back in time to understand Fisher’s Exact Test and its interesting history. Imagine a time when people were fascinated by unusual claims, like Muriel Bristol’s assertion that she could tell whether milk or tea was poured first just by tasting it.\n\\[\\begin{equation}\n  p = \\frac{(a+b)!(c+d)!(a+c)!(b+d)!}{n!a!b!c!d!}\n\\end{equation}\\]\nNow, enter Sir Ronald A. Fisher, a brilliant statistician. He wanted to put Muriel’s claim to the test in a scientific way. So, he came up with what’s known today as Fisher’s Exact Test. The idea behind this test was to determine if there was a significant connection between what Muriel called the “Truth” (the actual order of pouring milk or tea) and what “Lady says” (Muriel’s claims based on her taste).\n\n7.5.1.1.1 In Bioinformatics\nNow, let’s dive into how Fisher’s Exact Test is used in the world of gene sets, specifically in the context of gene expression analysis. Imagine you have two categories that you want to compare:\n\n“Gene belongs to the (GO/KEGG) category”: This category is all about genes that have specific labels or annotations, like being part of a certain biological process or pathway.\n“Gene is in the list of differentially expressed genes”: This category includes genes that you’ve identified as behaving differently under certain experimental conditions.\n\nNow, what Fisher’s Exact Test does here is pretty clever. It examines whether these two categories significantly overlap or not. In simpler terms, it helps you figure out if the genes with specific labels or annotations (the first category) tend to show up more often in the list of genes that behave differently (the second category) than you’d expect by random chance.\nIt’s like being a detective once again. You’re trying to find out if there’s a real connection between certain types of genes (those with specific labels) and genes that change their behavior under certain conditions. Fisher’s Exact Test helps you determine if this connection is more than just a coincidence, providing a statistical way to make sense of the data and draw meaningful conclusions in gene expression analysis.\n\n\n7.5.1.1.2 Pros and Cons\nLet’s explore the benefits and downsides of Fisher’s Exact Test, which is like a versatile tool in the world of statistics, but it comes with its own strengths and limitations.\n\nBenefits of Fisher’s Exact Test:\n\nSimple and Fast: One of its standout features is that it’s quite straightforward to use. You don’t need to be a statistics expert to apply it. Plus, it’s speedy, providing results quickly. It’s like having a simple and efficient tool in your toolkit.\nGood Performance: Fisher’s test already does a pretty good job at what it’s designed for. It’s like having a reliable and well-tuned instrument that usually gives you meaningful results.\nVersatility: You can use it in various situations and contexts. It’s not limited to a specific type of data or research area. Think of it as a Swiss Army knife in statistics that can be applied in different scenarios.\n\nDownsides of Fisher’s Test:\n\nSetting a Threshold: To use Fisher’s test effectively, you need to decide on a threshold for what counts as “differential expression.” This can sometimes be a bit subjective and may vary depending on the specific experiment.\nMultiple P-Values: When you apply Fisher’s test to multiple categories (like different gene sets), you end up with a bunch of p-values, one for each category. This can be overwhelming if you have hundreds of enriched categories with p-values less than 0.05. It’s like having a stack of puzzle pieces to put together.\nCorrection for Multiple Testing: Because you’re dealing with lots of p-values, you often need to correct for multiple testing. This step ensures that you don’t mistakenly think something is significant just by chance. However, when categories overlap, it can be challenging to account for this in the correction process.\nDifferent Gene Set Sizes: Not all gene sets are the same size. Some may have many genes, while others have only a few. Fisher’s test might treat them all equally, which can be a limitation when you’re comparing different sets. It’s like trying to compare the impact of big and small puzzle pieces without adjusting for their size.\n\n\nSo, Fisher’s Exact Test is like a trusty tool that gets the job done quickly and effectively in many situations. But, like any tool, it has its quirks, such as the need for threshold decisions and dealing with multiple p-values, making it important to use it thoughtfully and consider its limitations."
  },
  {
    "objectID": "chapters/week7.html#gene-set-enrichment",
    "href": "chapters/week7.html#gene-set-enrichment",
    "title": "7  Transcriptomes: Differential Expression to Enrichment Analyses",
    "section": "7.6 Gene Set Enrichment",
    "text": "7.6 Gene Set Enrichment\nImagine you’re looking at a big collection of genes, and you suspect that there might be small but coordinated changes happening in their expression levels. But here’s the thing: you don’t want to decide on a specific rule or threshold for what counts as a “big” change because it could vary from one experiment to another.\nThat’s where gene set enrichment tests come into play. The basic idea behind these tests is to do two things:\n\nCompute a Gene Set-Wise Test Statistic: For each category or group of genes (like those related to a specific biological process), you calculate a special number called a “test statistic.” This number summarizes how the genes in that category are behaving, kind of like a score that tells you if something interesting might be going on.\nCompare to Random Samples: Next, you compare the test statistic you calculated for the actual group of genes to what you’d expect if you randomly picked genes. It’s like checking if the score for your group of genes is way different from what you’d get by just picking genes at random.\n\nIf the test statistic for your group of genes stands out from what you’d expect by chance, it suggests there’s something significant happening in that category. It’s like finding a treasure chest among a bunch of empty boxes.\n\n7.6.1 Gene Set Enrichment Analysis\n\n\n\n\n\nGeneral Workflow for GSEA\n\n\n\n\nLet’s simplify the process of Gene Set Enrichment Analysis (GSEA) so that it’s easier to understand:\n\nSorting Genes: First, we line up all the genes based on how differently they’re behaving in an experiment. Some genes might be more active, while others are less active.\nCalculating Enrichment Score (ES): Now, we want to figure out if certain groups of genes, let’s call them “gene sets,” are doing something special. We go through the list of genes and do two things for each gene:\n\nIf the gene belongs to a particular gene set we’re interested in (let’s call it “G”), we add 1 to our score.\nIf it doesn’t belong to that gene set, we subtract 1 from our score. After we’ve done this for all the genes, we find the highest score, which we call the “maximum enrichment score” (MES).\n\nTesting It: Imagine we want to make sure our results aren’t just by chance. So, we create a “null hypothesis” by doing a pretend experiment 1,000 times:\n\nWe mix up the labels on our measurements (like shuffling cards).\nThen, we repeat steps 1 and 2, calculating the MES for this shuffled data.\n\nChecking Significance: Finally, we ask a question: “How many times did our pretend MES (from the shuffled data) turn out to be as extreme as or even more extreme than the real MES we found in our actual data?”\n\nIf the real MES is way more extreme than what we got in our pretend experiments, it suggests that the gene set we’re looking at is probably doing something important.\n\n\nSo, in simple terms, GSEA helps us find out if certain groups of genes are acting differently in our experiment compared to what we’d expect by random chance. If they are, it could mean they’re involved in something significant.\n\n\n7.6.2 Gene Set Analyses\n\n\n\n\n\nArrays and Genes in GSA\n\n\n\n\nLet’s break down Gene Set Analysis (GSA) in a way that’s easy to understand:\n\nGene Test Statistics: First, we look at each gene individually and calculate a special number called a “test statistic.” This number helps us understand how different each gene is behaving in our experiment. Think of it as a score for each gene.\nMax-Mean for Sets: Next, we’re interested in groups of genes, which we call “sets.” Instead of looking at each gene on its own, we want to know if whole sets of genes are doing something special. To figure this out, we find the maximum average score \\(\\max\\{\\bar{s}^{(+)}, \\bar{s}^{(-)}\\}\\) for each set. It’s like looking for the highest average score among a group of friends playing a game.\nStandardizing the Scores: We want to make sure our results are reliable, so we do a bit of math to make everything fair and comparable: \\(\\displaystyle S'_{\\text{max}} = \\frac{(S_\\text{max} - \\mu_S)}{\\sigma_S}\\). This step involves standardizing the scores to put them on the same scale, so we can easily compare different sets of genes.\nShuffling the Data: To be extra sure that our findings are meaningful and not just random, we do some pretend experiments. We mix up the data by shuffling the columns around, kind of like playing cards. Then, we calculate the max-mean scores for these shuffled data sets many times (let’s say B times).\n\nAs an extra tidbit of information, the p-values for the \\(S'_\\text{max}\\) shown above can be calculated using the following formula:\n\\[\\begin{equation}\n  p_s = \\frac{\\#(S'_\\text{max} < S^{'B})}{B}\n\\end{equation}\\]\nSo, in a nutshell, Gene Set Analysis helps us figure out if groups of genes are working together in a special way in our experiment. We start by looking at each gene’s behavior, then we check if sets of genes have higher average scores. We make sure our results are reliable by standardizing them and doing some pretend experiments with shuffled data. It’s like investigating whether certain teams of friends are winning a game together, but we want to be sure it’s not just luck!\n\n7.6.2.1 Problems with Gene Set Analysis\nLet’s simplify the challenges in gene set enrichment tests:\n\nSummarizing Gene Data: In these tests, we need to crunch down all the information about each gene into one simple number. This can be tricky, especially in complex experiments with lots of data points. It’s like trying to describe a whole book with just one word.\nMissing Important Clues: These tests might not catch every important detail about how genes work together. Imagine some pathways in our body are like a security system with gatekeeper genes that control what happens. Just looking at the overall expression of genes might not tell us if these gatekeepers are active or not.\n\nSo, while gene set enrichment tests are helpful, they have their limitations. They sometimes struggle to give us a clear picture of what’s happening in complex experiments, and they might miss some important clues about gene activity. It’s a bit like trying to understand a big puzzle, and we might need some extra pieces to see the whole picture. That’s all for today!"
  },
  {
    "objectID": "chapters/week8.html",
    "href": "chapters/week8.html",
    "title": "8  Transcriptomes: Network Analysis",
    "section": "",
    "text": "Illustration of a Transcription Factor\nIn genetics, there’s a fascinating puzzle to solve: how do our genes get turned on and off? Imagine your DNA as a book, and these genes are like chapters. There are two special areas in the DNA that play a big role in this story: promoters (P) and enhancers/silencers (E). Think of them as the “on” and “off” switches for a gene.\nNow, here’s where it gets interesting. Proteins, like tiny molecular superheroes called transcription factors (TFs), come into play. These TFs can attach themselves to the promoter and enhancer areas. The magic happens when these proteins team up. The combination of TFs determines whether a gene is going to be “read” and turned into a protein.\nIf a protein can attach itself to these promoter and enhancer areas, it means it has the power to control that gene. It’s like having control over a remote that decides whether the TV turns on or not. This protein becomes the boss of that gene.\nNow, things get a bit more abstract. Scientists talk about how changes in the boss’s mood can affect the story of the gene. If the boss protein gets more or less active, it can make the gene go from quiet to loud or vice versa. It’s like having a dimmer switch for the lights in a room – you can adjust how bright or dim they are.\nBut here’s the tricky part: in the lab, scientists still need to verify whether a protein can actually attach itself to these promoter and enhancer areas. It’s like having a bunch of keys, but you’re not entirely sure if they fit the locks. This is a crucial step in understanding how our genes are controlled, and scientists are working hard to solve this genetic puzzle."
  },
  {
    "objectID": "chapters/week8.html#ways-of-detecting-gene-function",
    "href": "chapters/week8.html#ways-of-detecting-gene-function",
    "title": "8  Transcriptomes: Network Analysis",
    "section": "8.1 Ways of Detecting Gene Function",
    "text": "8.1 Ways of Detecting Gene Function\nForward genetics starts from the phenotype and goes back to the genome. Reverse genetics starts from the genome and works its way back to the phenotype."
  },
  {
    "objectID": "chapters/week8.html#co-expression-and-co-expression-networks",
    "href": "chapters/week8.html#co-expression-and-co-expression-networks",
    "title": "8  Transcriptomes: Network Analysis",
    "section": "8.2 Co-Expression and Co-Expression Networks",
    "text": "8.2 Co-Expression and Co-Expression Networks\nIn genetics, there’s another interesting concept called co-expression. When scientists don’t know the exact regulatory network, they have to rely on a different approach. They can’t peek inside the genes’ instruction manual, so they start looking for patterns in how genes are turned on or off together, like a team of friends doing the same dance moves.\nThe idea here is pretty simple. Imagine you’re in a big group of people trying to follow a dance routine. If you and your buddy are close to each other in the group, you’re more likely to do the same dance steps at the same time. In the genetic world, this means that when different experimental conditions are applied, genes that are “close” to each other in the way they’re controlled tend to act in a similar way. We say they “correlate.” These genes also tend to associate with one another during clustering (e.g., K-means clustering).\n\n8.2.1 K-Means Clustering\n\n\n\n\n\nSteps Involved in K-Means Clustering\n\n\n\n\nImagine you have a big box of colorful marbles, and you want to sort them into different groups based on their colors. K-means is like a handy tool for this job. Here are some key things to know:\n\nVery Common: K-means is a popular method that many people use. It’s like a favorite tool in a craftsman’s toolbox because it’s useful in many situations.\nGood for Large Data Sets: If you have a massive collection of marbles (or data), K-means can still do its job efficiently. It’s like having a super-fast sorting machine for your marbles.\nVisualization Not Trivial: Sometimes, showing the results of K-means can be a bit tricky. However, if you have a bunch of marbles in many colors, you can represent them on a graph using something called principal components. It’s like making a cool chart that helps you understand how the marbles are grouped.\nFast and Low Memory Consumption: K-means doesn’t hog your computer’s memory, and it works pretty quickly. It’s like having a sorting machine that doesn’t slow down, even when you have lots of marbles to sort.\n\nLet’s take a closer look at some challenges that come with using K-means, a handy tool for grouping things. Here are the key issues:\n1. Choosing K: K-means requires us to decide how many groups (K) we want to create. This can be a bit tricky. Thankfully, there are some rules of thumb, like heuristics, to help us decide, but it’s not always easy. Think of it like trying to figure out how many boxes you need to sort your toys into – you don’t want too few or too many.\n2. Avoiding Local Minima: When you use K-means, you might end up with different results depending on where you start. It’s like solving a maze where you could get stuck in a corner. To overcome this, you can run the K-means algorithm multiple times and pick the best result. It’s like trying different paths in the maze to find your way out.\n3. Message Passing Algorithms: There’s another cool trick – K-means clustering algorithms that use something called “message passing.” This can help find better solutions. It’s like using a secret code or special clues to navigate the maze more effectively. For instance, there’s a method called “affinity propagation” developed by Frey and his team in 2007, which is like having a map to escape the maze.\n\n\n8.2.2 Hierarchical Clustering\nHierarchical clustering is a way to group things together in a very organized manner, and it works like building a tree from the ground up. Here’s how it goes:\nIn this approach, we always start by putting the two most similar things together into a new cluster. Imagine you have a bunch of marbles, and you want to organize them by color. You’d begin by pairing the two marbles that look the most alike.\nNow, when we’re making these clusters, there are a few ways we can decide which marble represents the whole group:\n\nWe can calculate the average of all the marbles in the cluster, like finding the average color.\nOr, we can pick the marble that looks the most similar to the others in the group.\nAlternatively, we could choose the marble that stands out the most, the one that’s least like the rest.\n\nHere’s where it gets interesting: depending on how we choose this representative and the method we use, we get different results. It’s like if we asked a few people to organize the marbles by color, they might come up with slightly different groups.\n\n8.2.2.1 Pros and Cons with Hierarchical Clustering\n\n\n\n\n\nExample of Hierarchical Clustering Used in Literature\n\n\n\n\nHierarchical clustering is a popular way to organize and show data, like putting things in order. It’s so commonly used because it’s not just about sorting data; it’s also a fundamental tool for creating visual representations. Think of it as arranging your toys neatly on a shelf.\nOne cool thing about hierarchical clustering is that it can be used in a two-way fashion. It doesn’t just sort one type of thing; it can sort both experiments and genes. It’s like having a special shelf where you not only arrange your toys but also group your books together.\nBut here’s the catch: while hierarchical clustering is super handy, it works best when you’re dealing with small amounts of data. If you have a massive collection of things to organize, it might not be the best choice. It’s like trying to fit too many toys on a tiny shelf – it gets a bit crowded. So, when you’ve got a smaller pile to sort, hierarchical clustering is your go-to tool for tidying things up and creating visual displays.\nBut there’s a challenge with this approach: how do we decide when to stop making clusters? We have a tree of clusters, but we need to know where to cut it. It’s like having a tree with many branches and trying to figure out where to make the final cuts to create the groups we want. That’s the tricky part in hierarchical clustering.\n\n\n\n8.2.3 Interpreting Co-Expression Networks\nWhen it comes to co-expression clusters, understanding what they mean can be like solving a puzzle. One way to look at it is through the *Guilt by Association principle. This means that if genes are co-expressed, they might be working together or at least involved in similar processes. It’s a bit like assuming that if you often see two friends together, they might be doing something fun or related, like playing the same video game.\nNow, if genes are co-expressed, it’s like they’re on the same team. They might be working together in the same biological pathway. Think of this as a group of friends joining forces to win a game or solve a big problem together.\n\n\n\n\n\nExample of a Distance Metric Used to Form Clusters\n\n\n\n\nTo understand co-expression clusters (i.e., related genes) better, we can also check for gene ontology enrichments. This is like checking if our group of friends shares common interests. It helps us figure out what kind of activities they enjoy together.\nAnother thing to consider is tissue/condition specificity. It’s like looking at when and where our friends like to hang out the most. In this case, we examine the experiments where the cluster’s gene expression is at its peak, which can tell us where these genes are most active.\nSometimes, we might also discover that co-expression clusters have something in common. This can be because they share enriched promoter motifs, which are like special symbols that tell us they belong to the same club. Additionally, if you find “transcription factors” in the same cluster, it’s like identifying key players or leaders in the group. These are the friends who organize the activities or call the shots in their little club of co-expressed genes.\n\n8.2.3.1 Problems with Networks\nExploring co-expression networks can be a bit like hunting for hidden treasures, but there are some challenges we need to tackle along the way. Here are some of the issues we encounter:\nFirst, the data can be noisy. This means there are lots of false alarms or “false positives.” It’s like searching for secret clues, but sometimes you find things that aren’t really connected to your treasure hunt. To deal with this, scientists use clever tricks like bootstrapping, randomization, or cross-validation. These methods help them estimate how often these false alarms happen and make their results more reliable.\nAnother tricky thing is that co-expression networks can’t always tell us if the connection between genes is a direct one or if it’s indirect. It’s like figuring out if your friend’s friend is your friend too, or if they’re just a friendly acquaintance. This direct or indirect link can make a big difference in understanding how genes work together.\n\n\n\n\n\nProblems with Co-Expression Networks\n\n\n\n\nSo, how do we uncover the secret codes that reveal the regulatory structure, like who’s in charge or who’s following whom? Well, scientists have a couple of strategies. In the “wet lab,” they carry out experiments by making changes, like knocking out a gene, to see how it affects the others. It’s like doing experiments in a chemistry lab to see what happens when you mix different ingredients.\nAlternatively, they use more advanced statistical models to find these hidden connections. Think of it as using a super detective’s magnifying glass to spot the subtle clues in the data. By overcoming these challenges, we can unlock the mysteries of co-expression networks and understand how genes influence each other.\n\n\n8.2.3.2 Extra Information\nWhen it comes to understanding genes and how they work, having some extra pieces of information can be incredibly helpful. Here are a few key things that come in handy:\n1. Identity of Transcription Factors and Their Binding Motifs: Imagine transcription factors as the directors of the genetic orchestra. They control which genes play their music. Knowing which transcription factors are in action and what kind of musical notes (or binding motifs) they follow helps us understand how genes are orchestrated.\n2. Knowledge About Functional and Structural Similarity of Genes: Genes are like puzzle pieces in a big genetic puzzle. If we know which pieces fit together because of their shape or function, it makes solving the puzzle easier. Understanding the similarities between genes – whether they perform similar functions or have similar structures – can reveal a lot about how they interact.\n3. Sequence and Binding Sites in the Promoter: Genes have specific “instructions” in their promoter regions that help control when and how they are activated. Knowing these sequences and binding sites is like having the blueprint for building a particular gadget. It tells us how the gene functions and what can influence it.\n4. Measurements About Protein-DNA Binding: Proteins, like transcription factors, often team up with DNA to make things happen. By measuring these protein-DNA interactions, it’s like capturing the moments when the genetic actors step onto the stage. This helps us understand who’s taking part in the genetic play and how they’re influencing the script.\n\n\n\n8.2.4 Possible Workflow\n\n\n\n\n\nPossible Workflow for Network Analysis by Segal et. al\n\n\n\n\n\n\n8.2.5 Advanced Models for Gene Regulation\nIn the fascinating world of gene regulation, there are some advanced models that scientists use to understand how genes are controlled. These models are like powerful tools in their toolkit. Here are a few of them:\n\nGraphical Gaussian Models: Think of these models as building a web of connections. Scientists create this web by testing how genes are related and whether they depend on each other. It’s like drawing lines between friends who influence each other in some way.\nRegression-Based Models: These models are a bit like fortune-tellers. They try to predict a gene’s behavior by looking at other factors, like the expression of regulators or how often certain binding sites appear. It’s as if they’re reading the signs and signals to predict what the gene will do next.\nBayes Nets, Network Models, Data Fusion Methods: These are like building a grand map of gene interactions. Bayes nets are like road signs that show us the most likely paths genes will take. Network models are like a city’s layout, helping us understand how genes connect. Data fusion methods combine information from different sources, like puzzle pieces, to create a clearer picture.\nMore Advanced (Probabilistic) Models: These models are like master chefs who mix various ingredients to create a unique dish. They combine ideas from the models above and add even more background information. This helps scientists create a comprehensive understanding of gene regulation.\n\nAll these models are tools that help scientists explore the intricate world of gene regulation, unraveling the complex relationships between genes and their controllers. It’s like being a detective, using different methods to solve the mystery of how our genes work and influence our lives."
  },
  {
    "objectID": "chapters/week9.html",
    "href": "chapters/week9.html",
    "title": "9  SNPs, Structural Variations, and Pan-Genomes",
    "section": "",
    "text": "Math Behind the Hardy-Weinberg Principle\n\n\n\n\nThe Hardy-Weinberg principle, established in 1908, helps us understand genotype frequencies in a population under specific conditions. Imagine a population where individuals choose mates randomly, there’s an infinite number of individuals, no genetic mutations occur, there’s no migration in or out, and there’s no natural selection favoring specific traits.\nLet \\(p(A)\\) be the probability of genotype \\(aa\\) happening, \\(p(AA)\\) the probabilty of genotype \\(AA\\) happening, and \\(p(Aa)\\) and \\(p(aA)\\) the probability of genotype of \\(Aa\\) happening. We can say the following given the above information (i.e., something similar to the quadratic formula):\n\\[\\begin{equation}\n  p(A)^2 + 2p(Aa) + p(a)^2 = 1\n\\end{equation}\\]\nIn this equilibrium, there are two alleles, let’s call them A and a, and their frequencies are represented by the symbols p and q. The key relationship is that p + q always equals 1. Furthermore, the frequencies of the possible genotypes, which are AA, Aa, and aa in a diploid organism, are represented by the equations p², 2pq, and q² respectively.\nTo break it down further, p and q are like the percentages of the major (A) and minor (a) alleles in the population. The possible genotypes are combinations of these alleles: AA means having two copies of the major allele, Aa means having one copy of each, and aa means having two copies of the minor allele.\nIn an ideal situation where the Hardy-Weinberg equilibrium holds, the expected frequencies of these genotypes can be calculated using the equations mentioned earlier. If the observed frequencies in a real population do not match these expected frequencies, it suggests that the conditions assumed by the Hardy-Weinberg equilibrium are not being met. This discrepancy can indicate factors such as non-random mating, a finite population size, mutation, migration, or natural selection affecting the gene pool."
  },
  {
    "objectID": "chapters/week9.html#main-processes-of-evolution",
    "href": "chapters/week9.html#main-processes-of-evolution",
    "title": "9  SNPs, Structural Variations, and Pan-Genomes",
    "section": "9.2 Main Processes of Evolution",
    "text": "9.2 Main Processes of Evolution\n\n\n\n\n\nMain Processes of Evolution\n\n\n\n\n\n9.2.1 Mutations\n\n\n\n\n\nIllustration of a Mutation\n\n\n\n\nGenomic mutations, or changes in our genetic material, are a common and ongoing process. These mutations can arise from various sources, including errors that happen during the replication of DNA, exposure to radiation, and interactions with certain chemical compounds. When we have mutations, these mutations are passed down to our offspring (should we have any).\nOne common type of mutation is known as a single nucleotide polymorphism, or SNP. These mutations involve the replacement of a single building block, or nucleotide, in the DNA sequence. SNPs are significant because they represent a single change in the genetic code and can contribute to the natural variation observed among individuals.\nIn essence, our DNA is dynamic, and changes can occur due to a variety of factors. These mutations, especially SNPs, play a role in shaping the diversity seen within populations and contribute to the genetic differences among individuals.\n\n9.2.1.1 Molecular Clock Hypothesis\nThe Molecular Clock Hypothesis, proposed by Zuckerkandl and Pauling in 1962, suggests a fascinating concept in evolutionary biology. They observed that the number of amino acid differences in a protein called hemoglobin between different lineages changes in a roughly linear manner with time, as estimated from fossil evidence.\nTo make this hypothesis work, they assumed that the number of mutations occurring per unit of time and per base pair (the building blocks of DNA) remains steady via the formula: \\[\\displaystyle \\frac{\\text{Number of Mutations}}{\\text{Base Pairs} \\times \\text{Number of Years}}\\]In other words, they suggested a constant rate of change, like a ticking clock, where the number of mutations happening per base pair per year stays roughly the same.\nBecause of the above, the genome in question is also assumed to evolve over time.\n\n\n9.2.1.2 Mutation Rate\nThe mutation rate is a measure of how many genetic changes, or mutations, occur on average per year or per generation. Scientists use two main methods to estimate this rate.\nOne way is through direct estimation, where researchers sequence the DNA of parents and their offspring, forming what’s called parent-child trios. By comparing the genetic makeup of parents and children, they can calculate the average number of mutations that accumulate over a certain period, either per year or per generation.\nThe second method is indirect estimation. Scientists compare the nucleotide differences between the genomes of two species and then scale this information using fossil evidence that indicates the time since these species diverged. This approach yields two different rates: the number of mutations per base pair per generation, directly estimated from parent-child trios, and the number of mutations per base pair per generation, indirectly estimated from fossil evidence.\nHowever, it’s crucial to note that these rates are meaningful only when they are comparable. To achieve this, scientists need to account for the generation time, which is the time it takes for one generation to replace the next. This adjustment ensures that both rates are on the same scale and can be accurately compared, allowing researchers to better understand and interpret mutation rates across different species or populations.\n\n\n9.2.1.3 Generation Time\nGeneration time refers to the average duration between successive generations within a population. It encompasses the period from infancy to the point when an individual becomes capable of producing offspring. This concept assumes a constant duration over extended periods and is averaged across the entire population.\nEstimating generation time can be challenging, even for humans. In our species, males can father children between the ages of approximately 14 to 100 years, while females can bear offspring from around 14 to 50 years. The variability in reproductive ages adds complexity to determining an exact average for the generation time.\nFor plants, the situation is even more intricate. Plants can produce seeds at any age after reaching maturity. However, not all seedlings survive to reach the age of sexual reproduction. Interestingly, older plants often produce more seeds. Moreover, many ecosystems maintain a seed bank in the soil, which means seeds can persist in a dormant state, potentially leading to the growth of new plants in the future.\n\n\n9.2.1.4 Site Frequency Spectrums\nSNPs can be examined by considering how many individuals share the same allele. If a particular allele is found in only one individual, it suggests a very recent mutation. On the other hand, if an allele is widespread and present in many individuals, it indicates that the mutation has existed in the population for a longer period, allowing several individuals to inherit it.\nWhen all individuals in a population share the same allele, it is described as “fixed.” This implies that the mutation has become a permanent part of the population’s genetic makeup.\n\n\n\n\n\nExample of a Site Frequency Spectrum\n\n\n\n\nScientists use a tool called a site frequency spectrum, which is essentially a histogram. This histogram illustrates how common different SNPs are in a population. By examining this spectrum, researchers can gain insights into the distribution of genetic variations within a population and understand the history and dynamics of mutations over time. It provides a visual representation of the frequencies of different alleles, helping researchers explore the genetic diversity and evolution of a population.\n\n\n9.2.1.5 Site Frequency and Genealogy Trees\n\n\n\n\n\nConverting a Genealogy Tree into a Site Frequency Spectrum\n\n\n\n\nA genealogy tree represents the ancestral relationships among a set of genes in a population. There is a direct mapping between a gene tree and a probability distribution, meaning that a specific genealogy tree corresponds to a particular probability of observing certain genetic variants in the population.\nHowever, it’s important to note that the mapping doesn’t work in reverse. This means that there can be multiple possible genealogy trees that lead to the same site frequency spectrum. In other words, different ancestral histories can result in the same distribution of genetic variants among individuals in a population.\nDespite this limitation, the site frequency spectrum still serves as a valuable tool for describing the population’s history. It provides insights into how common different genetic variants are in a population and helps researchers understand the dynamics of population size changes, migration events, and other factors that shape genetic diversity over time. While there may be multiple genealogy trees that can lead to the same site frequency spectrum, the spectrum remains a useful summary of the genetic variation within a population.\n\n\n\n9.2.2 Drift\nRandom drift refers to the changes in populations that occur as a result of random events. These events can have a significant impact on the genetic makeup of a population over time.\nOne instance of random drift is founder effects, which happen when a small group of individuals establishes a new population in a different area. The genetic characteristics of this small founding group can disproportionately influence the traits of the entire population, leading to a distinct genetic profile.\nPopulation size can also vary due to changes in environmental conditions. Fluctuations in population size can result in random changes in the prevalence of certain genetic traits. This is because smaller populations are more susceptible to the effects of chance.\nMigration, or the movement of individuals between populations, is another factor contributing to random drift. When individuals migrate, they bring their genetic material with them, potentially influencing the genetic diversity of both the source and destination populations.\n\n9.2.2.1 Neutral Theory of Evolution\nThe neutral theory of evolution, proposed by Motoo Kimura in 1968, suggests that a significant portion of the genetic variation within and between species arises from the random genetic drift of mutant alleles that are selectively neutral. In other words, these mutations don’t confer a survival advantage or disadvantage to individuals carrying them.\nAccording to this theory, deleterious mutations, or those that are harmful to an organism’s survival, are quickly eliminated by natural selection. On the other hand, mutations that are neither harmful nor beneficial (neutral mutations) are more likely to persist over time.\nGiven that a large portion of the genome is non-coding, meaning it does not directly contribute to the formation of proteins, most mutations occur in non-coding regions and are therefore neutral. Beneficial mutations, if they occur, have a higher likelihood of becoming fixed in a population rapidly, meaning that eventually, all individuals within that population will carry the beneficial mutation.\n\n\n\n\n\nMarbles in a Jar as an Example of Genetic Drift\n\n\n\n\nGenetic drift, a process influenced by chance rather than selection, plays a crucial role in this theory. A mutant allele can arise within a population and reach fixation (meaning all individuals in the population have that allele) due to random chance, rather than being driven by a selective advantage.\nFurthermore, the probability of a mutant allele reaching fixation is higher in smaller populations. This is because, in smaller populations, genetic drift has a more pronounced effect, influencing the frequency of alleles regardless of their selective advantage.\n\n\n9.2.2.2 Population Bottleneck\n\n\n\n\n\nIllustration of a Bottleneck\n\n\n\n\n\n\n9.2.2.3 Founder Effect\n\n\n\n\n\nIllustration of the Founder Effect\n\n\n\n\nThe founder effect is a specific type of population bottleneck that occurs when a small group of individuals, known as founders, establishes a new colony or population. This small founding group carries only a subset of the genetic diversity present in the larger original population.\nAs these founders reproduce and their descendants continue to interbreed within the new colony, there is a tendency for heavy inbreeding. Inbreeding involves mating between close relatives and can result in a reduction in genetic diversity.\nThe consequence of the founder effect is that the allele frequencies in the new population can differ significantly from those in the larger original population. Certain alleles may become more prevalent in the new population simply because they were present in the small group of founders.\nDue to the limited genetic diversity introduced by the founders, subsequent generations may experience reduced overall genetic diversity. This reduction can make the new population more susceptible to genetic disorders and less adaptable to changing environmental conditions.\n\n\n\n9.2.3 Selection\n\n\n\n\n\nIllustration of Selection and Fitness\n\n\n\n\nIn real-life populations, mate selection is not a random process. Instead, individuals with certain beneficial traits tend to have higher fitness, meaning they have a greater probability of producing more offspring that survive to reproductive age.\nThe concept of fitness in this context refers to the reproductive success of an individual, determined by the likelihood of passing on its genes to the next generation. Individuals with advantageous traits, whether in terms of survival, reproduction, or other factors, are more likely to contribute their genes to subsequent generations.\nAs a result, the genotypes represented by individuals with higher fitness become more prevalent in the population over time. Through natural selection, which favors traits that enhance an organism’s ability to survive and reproduce, these beneficial traits can increase in frequency within the population.\nOver many generations, the frequency of these advantageous traits may reach a point where they become fixed in the population, meaning that all individuals in the population carry those traits.\n\n9.2.3.1 At a Genetic Level?\nSelection at the genome level leaves discernible patterns in the sequence of an organism’s genetic material. When new mutations arise that confer a benefit, natural selection can lead to the rapid fixation of these advantageous alleles in the population. This occurs because individuals carrying these beneficial traits tend to have higher fitness, producing more offspring and increasing the frequency of the advantageous alleles in subsequent generations.\nIt’s important to note that similar patterns can also be produced by genetic drift, which includes events like population bottlenecks or founder effects. In genetic drift, certain individuals or genotypes survive and become more prevalent in the population purely by chance, rather than due to any selective advantage.\n\n\n9.2.3.2 Kinds of Selection\nDifferent types of selection play pivotal roles in influencing the genetic diversity and composition of populations. Purifying or negative selection acts as a quality control mechanism, selectively removing new alleles that are deleterious or harmful to an organism’s fitness. This stabilizing selection ensures the preservation of a population’s overall well-adapted genetic makeup by eliminating mutations that could compromise fitness.\nOn the contrary, balancing selection introduces a dynamic element by allowing alternative alleles to persist at higher frequencies than expected from genetic drift alone. This phenomenon often arises when heterozygotes, individuals with two different alleles for a specific gene, exhibit a higher fitness than homozygotes. Balancing selection contributes to the maintenance of genetic diversity within a population, showcasing the importance of heterogeneity in certain genetic traits.\nDirectional or positive selection propels evolutionary change by favoring a specific phenotype or allele state. This type of selection can be driven by environmental changes or adaptations that provide a selective advantage. Over time, the allele associated with the favored trait experiences an increase in frequency within the population, reflecting the dynamic nature of evolutionary forces.\n\n\n9.2.3.3 Genomic Hitchhiking\nGenomic hitchhiking is a phenomenon observed in the context of genetic evolution, particularly when examining the site frequency spectrum. In the examples discussed earlier, alleles beyond the initial 1:10 ratio were found to reach fixation. This occurrence stems from individuals, and their descendants, who experienced increased fitness due to beneficial mutations. In simpler terms, genomic hitchhiking is when an allele reaches fixation along with the advantageous alleles by chance.\nHowever, not all alleles reaching fixation necessarily contribute to the fitness increase. Some alleles may reach fixation purely by chance, a process known as genetic drift. This incidental fixation of alleles due to chance rather than selection is what defines genomic hitchhiking.\nIt’s important to note that in reality, genomic hitchhiking is constrained to alleles in close proximity to the beneficial mutation. This limitation arises from the process of recombination, where genetic material is exchanged during the formation of reproductive cells. Recombination tends to break up the associations between nearby alleles, preventing distant alleles from hitchhiking along with the beneficial mutation.\n\n\n\n9.2.4 Migration\n\n\n\n\n\nIllustration of Migration\n\n\n\n\nMigration, in the context of population genetics, refers to the movement of individuals between populations, leading to the exchange of genetic material. While a founder event, such as the establishment of a new colony by a small group of individuals, is one form of migration, the term migration more broadly encompasses the continuous gene flow between distinct populations.\nPopulations can evolve independently over time, and the frequencies of alleles may undergo different trajectories. Migration becomes a crucial factor in maintaining genetic diversity and influencing the genetic makeup of populations. When individuals migrate from one population to another, they bring with them the genetic diversity present in their source population.\nThis influx of new genetic material can have significant consequences for the target population. It introduces novel alleles and variations, potentially altering the allele frequencies and genetic characteristics of the population. Migration acts as a dynamic force that connects populations, preventing them from becoming genetically isolated and contributing to the ongoing evolution of species."
  },
  {
    "objectID": "chapters/week9.html#coalescent-theory",
    "href": "chapters/week9.html#coalescent-theory",
    "title": "9  SNPs, Structural Variations, and Pan-Genomes",
    "section": "9.3 Coalescent Theory",
    "text": "9.3 Coalescent Theory\n\n\n\n\n\nIllustration of Coalescence Theory\n\n\n\n\nThe concept of coalescence is a key idea in population genetics, particularly when studying the relationships among individual genomes. When mutations occur at a constant rate, analyzing the observed SNPs between two genomes can provide insights into the time to their most recent common ancestor.\n\n\n\n\n\nIllustration of the Most Recent Common Ancestor\n\n\n\n\nThis approach assumes a constant mutation rate, allowing researchers to make inferences about the historical relationships between individuals or populations based on the observed genetic variations.\nCoalescent theory is a model in population genetics that provides insights into the shared ancestry of gene variants sampled from a population. Unlike traditional evolutionary models that look forward in time, coalescent theory reverses the perspective and examines the process of merging alleles backward in time. This model traces the genetic history of a population by simulating coalescence events, where alleles merge into a single ancestral copy.\nIn this theoretical framework, a coalescent event represents the common ancestry of gene variants. The model simulates a random process, allowing researchers to reconstruct the historical relationships between alleles in a population. By studying coalescent events, scientists can estimate the time, measured in generations, since gene variants shared a common ancestor.\n\n9.3.1 Bottlenecks and Coalescence\n\n\n\n\n\nCoalesence During Bottlenecks\n\n\n\n\nA bottleneck occurs when a population undergoes a significant reduction in size, often due to environmental factors or other events. During a bottleneck, the number of individuals contributing to the next generation is small, and this has implications for coalescence events.\nIn the case of bottlenecks, where the population size is dramatically reduced, there are fewer potential ancestors for the next generation. This means that individuals from the bottlenecked population are more likely to share recent common ancestors compared to a non-bottlenecked population.\n\n\n9.3.2 Structural Variants\nThe advent of genome sequencing has revealed that genetic variation between individuals is not solely driven by point mutations (Single Nucleotide Polymorphisms or SNPs). Structural variants represent another layer of genomic diversity, involving changes larger than 1 kilobase (kb) in the genome.\n\n\n\n\n\nKinds of Structural Variants\n\n\n\n\nStructural variants encompass various alterations, including insertions, deletions, duplications, and inversions. These modifications can significantly impact the genetic landscape and contribute to the diversity observed among individuals.\nSeveral mechanisms underlie the generation of structural variants. Errors during DNA replication, where inaccuracies occur in copying genetic information, can result in structural changes. Transposable elements, which are sequences of DNA capable of moving around the genome, can also induce structural variation by inserting or removing segments of genetic material. Additionally, double-strand breaks in the DNA, often caused by external factors or internal cellular processes, can lead to structural changes when the breaks are repaired."
  },
  {
    "objectID": "chapters/week9.html#pan-genomes",
    "href": "chapters/week9.html#pan-genomes",
    "title": "9  SNPs, Structural Variations, and Pan-Genomes",
    "section": "9.4 Pan-Genomes",
    "text": "9.4 Pan-Genomes\n\n\n\n\n\nPan-Genome Framework\n\n\n\n\nThe pan-genome framework categorizes the genetic makeup of a species into three distinct components. First, the core genome encompasses genetic elements that are shared universally among all individuals within a species. These core components define the fundamental traits and characteristics that are common to the entire population, forming the genetic foundation of the species.\nIn addition to the core genome, the dispensable genome, often referred to as the cloud, comprises genetic elements shared by at least two individuals but not present universally across the entire population. These dispensable elements contribute to the diversity observed within the species, allowing for variability in genetic content among individuals.\nLastly, the unique genome, known as the shell, consists of genetic elements specific to and present in only one individual. These unique elements contribute to individual variability and distinctiveness within the population, highlighting the personalized nature of certain genetic traits."
  },
  {
    "objectID": "chapters/week10.html",
    "href": "chapters/week10.html",
    "title": "10  Re-Sequencing and SNP Calling",
    "section": "",
    "text": "Re-sequencing is a genomic sequencing approach that involves sequencing new individuals from a species for which there already exists a reference genome. This method allows researchers to identify variations, such as single nucleotide polymorphisms (SNPs) and insertions or deletions (indels), in the genomes of new individuals when compared to the established reference genome.\nTo align the sequence reads obtained from re-sequencing to the reference genome, read alignment methods are employed. These methods utilize data structures like suffix tries, which efficiently store and retrieve substrings of the reference genome. Examples of popular read alignment tools include Bowtie2 and bwa mem. These tools aid in accurately mapping the sequenced reads to the reference genome, enabling the identification of genetic variations.\nIn addition to traditional read alignment methods, there are pseudo alignment-based methods like minimap2. Pseudo alignment refers to the rapid mapping of reads to a reference without explicitly aligning each base. Such methods are particularly well-suited for handling long sequencing reads.\n\n\nOne of its primary goals is to unveil patterns and trends in genetic variation, contributing valuable insights into diverse aspects of population genetics and evolutionary biology. By identifying genetic adaptations and regions under natural selection, re-sequencing helps researchers understand how populations adapt to different environments.\nFurthermore, this technique provides a means to investigate gene flow between populations, offering crucial information about the connectivity and exchange of genetic material. The analysis of patterns of dispersal, including differences between sexes and historical migration histories, enhances our understanding of the demographic dynamics of populations over time. Moreover, re-sequencing plays a vital role in estimating the effective population size and its historical fluctuations, providing valuable data for unraveling the population’s demographic history.\nBeyond evolutionary and population biology, re-sequencing finds applications in forensics, where it is employed for individual identification and genetic profiling. Practical considerations for re-sequencing include a sufficient sample size, typically involving tens of individuals, and an appropriate coverage level, such as 16x. Although not as high as required for de novo genome assembly, this coverage ensures the reliable estimation of genetic differences between individuals and the accurate identification of genetic variations within the studied population.\n\n\n\nRe-sequencing, while a powerful technique, encounters challenges, especially in the context of heterozygous positions within diploid genomes. Adequate coverage is essential to sample both alleles at heterozygous positions, ensuring a comprehensive representation of genetic diversity. The probability of correctly sampling one allele is akin to a coin toss, with a 50% chance of sampling either the maternal or paternal allele.\nFor instance, consider a heterozygous position: with just four reads, the probability of all reads sampling only one allele is 0.125. As coverage increases to eight reads, the probability decreases to 0.0078125, and with 16 reads, it drops significantly to 3.051758e-05. Scaling this challenge to a real-world scenario, such as 10 million SNPs at 16x coverage, the probability of sampling all heterozygous positions with at least one read per allele is remarkably low at 2.9e-133.\nMoreover, when the average coverage is low, random variation can lead to larger parts of the genome having lower coverage. This variability across different regions in different individuals results in many sites having missing values, complicating the accurate assessment of genetic diversity.\n\n\n\nSingle Nucleotide Polymorphisms (SNPs) are a central focus in re-sequencing efforts, representing loci where an individual differs from the reference genome at a single nucleotide. The primary goal is to assess point mutations, and the subsequent identification and interpretation of SNPs involve specific procedures and data formats.\nIn the nomenclature of SNPs, the major allele is the more prevalent variant within a population, while the minor allele is less common. Understanding the evolutionary context of SNPs requires distinguishing between ancestral and derived alleles. The ancestral allele is the original variant present in the common ancestor of a population, a determination that often necessitates information from outgroup species. In contrast, the derived allele is the variant that has mutated from the ancestral state.\nReference alleles are those present in the haploid reference assembly, but it’s crucial to note that the allele in the reference can be any of the major, minor, ancestral, or derived alleles. This ambiguity emphasizes the importance of considering population data and outgroup information to discern the specific nature of the reference allele."
  },
  {
    "objectID": "chapters/week10.html#broad-institute-and-ngs-analysis",
    "href": "chapters/week10.html#broad-institute-and-ngs-analysis",
    "title": "10  Re-Sequencing and SNP Calling",
    "section": "10.2 Broad Institute and NGS Analysis",
    "text": "10.2 Broad Institute and NGS Analysis\nThe Broad Institute has played a pivotal role in genomics research by developing a suite of Java-based software tools tailored for the analysis of resequencing data. Among these tools is the Preprocessing Toolkit, Picard, which has been seamlessly integrated into GATK (Genome Analysis ToolKit) from version 4 onward. Picard serves as a comprehensive preprocessing toolkit, addressing tasks related to data preparation and manipulation to ensure the quality and integrity of sequencing data.\n\n\n\n\n\nGATK Workflow\n\n\n\n\nGATK, a key offering from the Broad Institute, specializes in variant analysis, focusing on the identification of genetic variations such as SNPs and indels. Its sophisticated algorithms enhance the accuracy of variant calling, making it a widely utilized tool in genomics research. For the analysis of structural variants, the Broad Institute provides GenomeSTRiP, a dedicated tool that enables the identification and exploration of larger-scale genomic variations, including insertions, deletions, and duplications.\nFacilitating the visualization of genomic data, the Integrated Genome Viewer (IGV) is another notable tool developed by the Broad Institute. IGV offers an interactive platform for researchers to explore and analyze various genomic features, including sequence data, variants, and annotations, providing a comprehensive understanding of genomic landscapes.\n\n10.2.1 Read Mapping\n\n\n\n\n\nRead Mapping in GATK Workflow\n\n\n\n\nQuality control (QC), trimming, and mapping to the genome are crucial steps in the analysis of Next-Generation Sequencing (NGS) data, ensuring the accuracy and reliability of downstream analyses.\nFor QC, tools like fastQC and fastp are commonly employed to assess the quality of raw sequencing data. These tools provide valuable insights into various quality metrics, aiding researchers in identifying potential issues and making informed decisions for data processing.\nTrimming, an essential preprocessing step, is carried out using tools like Trimmomatic or fastp. Trimming helps remove low-quality bases and sequencing adapters, enhancing the overall quality of the data and improving downstream analysis accuracy.\n\n\n\n\n\nHeaders in a SAM File\n\n\n\n\nRead alignment, the process of mapping sequenced reads to a reference genome, is typically performed using tools like bwa mem or Bowtie2. While both are popular choices, bwa mem is often preferred in practice. Additionally, pseudo alignment-based methods like minimap2 are gaining popularity, especially for handling long reads. Pseudo alignment allows rapid mapping without explicitly aligning every base, making it suitable for datasets with longer read lengths.\nThe outcome of the alignment process is typically a file in SAM (Sequence Alignment/Map) format, which is often converted to BAM (Binary Alignment/Map) for more efficient storage and manipulation.\n\n\n\n\n\nInformation on SamTools Headers\n\n\n\n\nThe @HD (Header) line within the SAM header contains vital information about the header itself, including the version number (VN) specifying the SAM format version and the sort order (SO) indicating how alignments are sorted within the file. This ensures clarity and compatibility in handling the data.\nThe @SQ (Sequence Dictionary) lines in the SAM header define the sequence dictionary, presenting information about the reference sequences or contigs used in the alignment. Each @SQ line provides details such as sequence name, length, and optional attributes. This sequence dictionary establishes the order of sequences in the alignment, facilitating accurate interpretation.\nFurthermore, the @RG (Read Group) lines in the SAM header convey information about read groups, allowing the grouping of reads with similar properties. Multiple @RG lines can be present, each specifying details like ID, sample name, library name, and platform information. This read group information is valuable for associating reads from the same sample, library, or sequencing run.\n\n10.2.1.1 SAM Flags\n\n\n\n\n\nSAM Flags\n\n\n\n\nFlags are really just pieces of binary information that tell you more about how the read mapping went. For instance, a binary flag of “1001” would mean that the flag in question is an un-mapped one; a flag of “1001” - on the contrary - would be an unmapped read with multiple sequences.\n\n\n10.2.1.2 BAM Files\nA Binary Mapping File (i.e., BAM) is the same file as a SAM file, but in binary format. BAM files are used because they are smaller and lead to faster results.\nBAM/SAM files play a pivotal role in genomics research, encompassing a wealth of information about both mapped and unmapped reads from sequencing data. This dual representation provides a comprehensive view of the alignment status of short DNA sequences to a reference genome, allowing researchers to explore the entirety of the sequencing dataset.\nOne notable feature of BAM/SAM files is the ability to extract information about reads specific to a particular genomic region of interest. This selective extraction facilitates targeted analyses, enabling researchers to focus on and explore detailed characteristics within specific genomic regions, enhancing the precision and depth of genomic investigations.\nIn addition to region-specific analyses, BAM/SAM files afford the flexibility to extract either all mapping reads or all unmapped reads separately. This distinction is particularly valuable for researchers conducting diverse analyses, ranging from the exploration of characteristics associated with successfully mapped reads to the investigation of reasons behind read alignment failures.\nFurthermore, while BAM/SAM files do not store the original Fastq files directly, they contain the necessary information for the reconstruction of the original Fastq sequences\n\n\n\n10.2.2 Prepration for SNP Calling\n\n\n\n\n\nWorkflow for Preparing Reads for SNP Calling\n\n\n\n\nWhen reads are initially obtained from a Fastq file, they are in random order, potentially scattered throughout the entire genome. To effectively call single nucleotide polymorphisms (SNPs) or conduct other genomic analyses, it is imperative to organize the reads based on their mapping coordinates.\nSorting the BAM file involves arranging the reads according to their genomic positions. This sorted order facilitates more straightforward and efficient access to specific genomic regions during subsequent analyses. For example, when calling SNPs, having reads in a sorted order allows the algorithm to navigate through the data in a coordinated manner, significantly improving the accuracy and speed of SNP identification.\nIndexing, another essential step, involves creating an index file associated with the sorted BAM file. This index file serves as a roadmap, enabling faster searches for specific genomic regions. With an indexed BAM file, the computational tools used in genomics analyses can quickly locate and retrieve relevant information, reducing the computational burden and accelerating the overall analysis process.\n\n10.2.2.1 Adding Read Group Information for BAM File\nIn genomic data processing, augmenting BAM files with read group information is a valuable step that involves adding metadata associated with individual reads. Read group information encompasses details such as sample identification, library origin, and sequencing run or flowcell information. This additional metadata proves particularly useful in correcting for sequencing artifacts, particularly in the identification and removal of PCR duplicates.\nThe inclusion of read group information in SAM/BAM files facilitates a more granular understanding of the sequencing data, allowing researchers to distinguish between reads originating from different samples, libraries, or sequencing runs. This level of detail becomes crucial when addressing potential biases introduced during the sequencing process, such as PCR duplicates.\nPCR duplicates, arising from the polymerase chain reaction amplification step, can distort downstream analyses if not appropriately handled. By incorporating read group information into the BAM file, researchers gain the ability to identify and mark PCR duplicates based on their distinct metadata. This, in turn, enables more accurate quality control and downstream analyses by mitigating the impact of duplicate reads.\n\n\n10.2.2.2 Removing Duplicates\nIn Illumina sequencing, the PCR amplification step can generate duplicate reads, introducing artifacts that artificially inflate the coverage of certain genomic regions. To address this, it is essential to identify and manage these duplicate sequences to ensure the accuracy of downstream analyses.\nMarking duplicate sequences involves the addition of information to the BAM file, specifically flagging reads that are likely to be PCR duplicates. This information serves as a crucial indicator during subsequent analyses, allowing researchers to distinguish between genuine biological signals and artifacts introduced by PCR amplification.\nIt’s important to note that the process of marking duplicates does not automatically remove them from the BAM file; instead, it adds a flag or annotation to identify these duplicates. The decision to actually remove them is typically left to the researcher, contingent upon specific analysis requirements. Removal of duplicates can be carried out by setting a specific command line option during the analysis, ensuring that these artifacts do not unduly influence downstream results.\n\n\n\n10.2.3 Estimating Genotype Likelihoods\n\n\n\n\n\nBSQR Before and After Recalibration\n\n\n\n\nBase Quality Score Recalibration (BQSR) is a critical step in genomic data analysis, especially when using tools like the Genome Analysis Toolkit (GATK). BQSR is designed to identify and rectify systematic errors introduced by the sequencing machine, enhancing the accuracy of base quality scores associated with individual nucleotides.\nDuring BQSR, the tool examines mapped reads in the dataset and considers various factors to identify systematic errors. These factors include the read group to which the read belongs, the quality score associated with the base, the machine cycle producing the base (position in the read), and the context in which the base is found, including the current base and the preceding base (dinucleotide).\nUtilizing this information, BQSR calculates empirical quality scores, quantifying how many errors were detected for each sequence context and position in the read. These calculated error rates are then compared to the expected quality scores associated with the sequencing data. Based on this comparison, BQSR adjusts the quality scores, aiming to correct inaccuracies while preserving the original nucleotide calls in the data. Importantly, the recalibration process focuses on improving the reliability of the quality scores assigned to each base without altering the actual nucleotide sequence calls.\n\n10.2.3.1 Call Variants per Sample\n\n\n\n\n\nWorkflow of Haplotype Caller\n\n\n\n\nThe HaplotypeCaller is a variant calling tool used in genomic data analysis, particularly for identifying both Single Nucleotide Polymorphisms (SNPs) and small insertions or deletions (indels) on a per-sample basis. This tool is characterized by several key features that contribute to its accuracy and versatility.\nOne notable aspect is its ability to perform local reassembly, a process where it identifies haplotypes, or sets of closely linked genetic variations. This approach allows the HaplotypeCaller to consider the context of the genomic region, providing a more accurate and comprehensive understanding of the variations present. This local reassembly is particularly advantageous in regions with complex variation patterns.\nUnlike locus-based variant callers, the HaplotypeCaller’s methodology includes information from multiple reads and can handle situations where reads may be unmapped. This is crucial for maximizing the utility of sequencing data, especially in regions where traditional locus-based callers might face limitations.\nFurthermore, the HaplotypeCaller is designed to work with polyploid organisms and pooled samples. Its flexibility in accommodating different genetic scenarios makes it well-suited for analyzing complex genomic data, allowing researchers to confidently call variants in a variety of experimental setups.\n\n10.2.3.1.1 Calculating Genotype Likelihoods\n\\[\\begin{equation}\n  p(b|A) =\n  \\begin{cases}\n      \\frac{e}{3} &: b \\not = A &\\text{(wrong call)} \\\\\n      1 - e &: b = A &\\text{(correct call)}\n  \\end{cases}\n\\end{equation}\\]\nFor each nucleotide (that we’ll call “b”), the likelihood score is based on its phred quality score. This phred quality score is transformed to a likelihood score using the above formula (where \\(A\\) is the true haplotype).\n\\[\\begin{equation}\n  p(b | G) = p(b | \\{A_1, A_2\\}) = \\frac{1}{2} \\cdot p(b|A_1) + \\frac{1}{2} \\cdot p(b|A_2)\n\\end{equation}\\]\nIn the case where a genotype is said to be a certain diploid kind (i.e., the \\(\\{A_1, A_2\\}\\)), the likelihood is pretty much the above.\n\\[\\begin{equation}\n  p(D|G) = \\prod_i\\left(\\frac{p(b_i|A_1) + p(b_i|A_2)}{2}\\right)\n\\end{equation}\\]\nWhere we have multiple overlapping reads, a naive Bayes assumption is used (i.e., the reads are different from one another).\n\n\n\n10.2.3.2 From Likelihoods to SNPs\n\n\n\n\n\nGoing from Likelihoods to SNPs\n\n\n\n\nIn the first step, the sample is processed by itself so that variants can be called.\n\n\n\n\n\nJoint Genotyping\n\n\n\n\nIn the realm of genomic variant calling, *Joint SNP calling**, facilitated by tools like GenotypeGVCFs, represents a collaborative approach that harnesses the strength derived from analyzing multiple samples simultaneously.\nThe fundamental concept behind joint genotyping is rooted in the idea that a Single Nucleotide Polymorphism (SNP) or insertion/deletion (indel) is most informative when it is shared across the entire population under investigation. The rationale behind this lies in the assumption that errors, particularly random ones, are less likely to occur in exactly the same manner in two independent samples. By jointly considering variants across multiple samples, the approach aims to distinguish true genetic variations from potential errors introduced during sequencing or analysis.\n\n\n\n10.2.4 VCF File Format\n\n\n\n\n\nMeta-Information VCF File Format\n\n\n\n\n\n\n\n\n\nHeader in a VCF File\n\n\n\n\nThis is a file that contains meta-information (see above), header information (which is denoted using a single octothorpe “#”), and SNP or indel calls."
  },
  {
    "objectID": "chapters/week10.html#variant-and-hard-filtering",
    "href": "chapters/week10.html#variant-and-hard-filtering",
    "title": "10  Re-Sequencing and SNP Calling",
    "section": "10.3 Variant and Hard Filtering",
    "text": "10.3 Variant and Hard Filtering\n\n10.3.1 Variant Filtering\nVariant filtering is a crucial step in genomic data analysis, aiming to distinguish true genetic variants from potential errors or artifacts introduced during the sequencing and variant calling processes. One sophisticated method for accurate variant filtering is the Variant Quality Score Recalibration (VQSR) method.\nVQSR is considered one of the most accurate variant filtering techniques, but it comes with specific requirements. To effectively implement VQSR, a high-quality set of verified variants is needed to train the model. This verified set serves as a reference to teach the algorithm to discern between true variants and artifacts, enhancing the precision of variant calls. However, it’s important to note that VQSR is most applicable to model species, such as humans, where extensive validated variant datasets are available.\nIn situations involving non-model organisms, where comprehensive verified variant sets may be lacking, an alternative approach is recommended. In such cases, hard filtering based on variant quality scores becomes a practical choice. This involves setting specific quality score thresholds to filter out variants that fall below the desired quality standards. While not as sophisticated as VQSR, hard filtering offers a pragmatic solution for non-model organisms, ensuring that variants meeting certain quality criteria are retained for downstream analyses.\n\n10.3.1.1 Hard Filtering\nHard filtering is a widely utilized method in genomic variant analysis to refine variant datasets by establishing specific criteria for filtering out variants that may signify errors or artifacts. Several common filtering parameters, along with their recommended criteria, contribute to this process.\nQualByDepth (QD), often set at a threshold of 2.0, calculates the ratio of variant quality (QUAL) to the unfiltered depth of non-reference samples. This criterion is applied to exclude variants with relatively low quality in relation to the depth of coverage.\nFisherStrand (FS), with a recommended threshold of 60.0, assesses strand bias through a Phred-scaled p-value. Variants exhibiting significant strand bias are filtered out, as this may indicate potential artifacts.\nRMSMappingQuality (MQ), set at a threshold of 40.0, represents the Root Mean Square of the mapping quality across all reads. Variants with poor mapping quality are filtered out to retain those with high mapping characteristics.\nMappingQualityRankSumTest (MQRankSum), with a threshold of -12.5, tests for differences in mapping qualities between reads with reference alleles and those with alternate alleles. Variants with significant differences are filtered out.\nReadPosRankSumTest (ReadPosRankSum), often set at -8.0, evaluates whether the alternate allele is predominantly observed near the ends of reads. Variants with this positional bias are filtered out.\n\n\n\n10.3.2 Excluding Filtered SNPs\n\n\n\n\n\nExamples of Filtered SNPs\n\n\n\n\nFollowing the application of filtering criteria to refine the dataset, the resulting VCF (Variant Call Format) file retains all Single Nucleotide Polymorphisms (SNPs), but their status is modified to indicate whether they passed or failed the applied filters. Specifically, SNPs are marked as either “filtered” or “PASS” based on whether they met the specified criteria during the filtering process.\nWhile these filtered SNPs remain present in the VCF file, users may choose to exclude or remove them from the dataset if desired. Various tools and software in the genomics toolkit provide functionality to extract specific subsets of variants based on their attributes, including their filtering status. By employing such tools, researchers can generate a VCF file that includes only the variants that passed the filtering criteria, omitting those marked as filtered.\n\n\n10.3.3 Annotating SNPs\n\n\n\n\n\nAnnotating SNPs\n\n\n\n\nSNP annotation is a crucial step in genomic analysis, especially when categorizing SNPs into non-coding, synonymous, or non-synonymous variants. This annotation process helps researchers identify potential causative SNPs and gain insights into their functional implications. One tool commonly used for SNP annotation is SnpEff.\nSnpEff serves as a powerful tool for annotating SNPs, providing a comprehensive set of annotations based on genomic information. To utilize SnpEff, researchers need to provide it with specific input files, including the VCF file containing the variant information, the genome sequence in FASTA format, and the genome annotation in GFF3 file format. These files collectively guide SnpEff in accurately annotating the SNPs based on their locations and potential functional impact.\nSnpEff offers various annotations for different SNP types, including intergenic, synonymous, and non-synonymous variants. It enhances the VCF file by adding a tag, typically labeled as EFF, to the INFO field. This tag includes information about the SNP type, enabling researchers to easily identify and categorize the variants based on their potential effects."
  },
  {
    "objectID": "chapters/week11.html",
    "href": "chapters/week11.html",
    "title": "11  Population Analysis",
    "section": "",
    "text": "SNPs, or Single Nucleotide Polymorphisms, are changes that occur in our genetic code over long periods. Imagine them as tiny mutations that happen naturally in our DNA as time goes on. In humans, the mutation rate - the rate at which these changes happen - is around 0.5 to 1 mutation per 100 million base pairs in just one generation. This means about 70 to 80 mutations can occur in each person.\nThese changes aren’t predictable and happen randomly, and they are quite uncommon. However, when we find the same SNPs shared among different individuals, it gives us clues about their shared ancestors.\nScientists use a theory called coalescent theory to understand this. This theory says that any two sets of genes or genomes will always have an ancestor they both come from.\nWhen two people share a lot of SNPs, it means they probably have similar ancestors in their family trees. On the other hand, if their SNPs are different, it shows that their genetic sequences have traveled a longer evolutionary distance apart.\nScientists can use this information to trace back and find the most recent common ancestor (MRCA) shared by two individuals or groups.\nThe neutral theory of evolution suggests that a lot of the changes or mutations happening in our genetic code don’t really affect how an organism survives or thrives. These mutations are considered neutral because they don’t give any advantage or disadvantage in the environment. Imagine these changes as happening randomly, without impacting an individual’s chances of survival.\nDrift, another concept in genetics, is responsible for explaining most of the patterns we see in SNPs (those small genetic differences). Drift refers to random changes in the frequency of certain genetic traits within a population over time.\nMost of these SNP patterns we observe in populations are because of shared ancestry. This means that when we look at the differences and similarities in these small genetic differences among people, we can understand how different groups or populations are related to each other.\nTo help visualize how populations are structured based on these SNPs, scientists use a method called Principal Component Analysis (PCA). They organize the SNP data into a matrix where each SNP is represented by numbers: 0, 1, or 2 (showing the number of alternative forms of a particular gene). By analyzing this data, they can see that individuals who have similar patterns of these genetic differences are grouped closer together."
  },
  {
    "objectID": "chapters/week11.html#key-concepts-and-terms",
    "href": "chapters/week11.html#key-concepts-and-terms",
    "title": "11  Population Analysis",
    "section": "11.1 Key Concepts and Terms",
    "text": "11.1 Key Concepts and Terms\n\n11.1.1 Admixture\n\n\n\n\n\nIllustration of an Admixture\n\n\n\n\nAdmixture is a concept that helps scientists divide or separate genetic data into groups that represent different ancestral populations. These groups have variations in the frequencies of certain forms of genes (alleles) across their genomes.\n\n\n\n\n\nMore on Admixtures\n\n\n\n\nIn this process, scientists aim to categorize the data into k groups, each representing a population with its unique allele frequencies. This method helps to understand how various genetic variations are distributed among different populations.\nAdmixture can be thought of as a specific type of Principal Component Analysis (PCA), a statistical technique used to simplify complex data.\n\n\n11.1.2 Deep Coalesence\n\n\n\n\n\nGrpahic Illustrating Deep Coalesence\n\n\n\n\nDeep coalescence refers to a situation where a specific genetic variation, like a SNP (Single Nucleotide Polymorphism), existed in a common ancestor of two different species. This means that this genetic difference, represented by the SNP, was present before these species branched off and became distinct from each other.\nWhen scientists identify a SNP that is shared between two different species, it suggests that this particular genetic variation dates back to a time before these species evolved separately. The “deep” aspect refers to the fact that this genetic variation’s origin predates the actual separation of these species into distinct evolutionary paths.\nIncomplete lineage sorting occurs when genetic variations, including SNPs, are inherited from a common ancestor but don’t align neatly with the species’ evolutionary tree. This happens because, over generations, genetic diversity is maintained within a population due to both ancestral genetic differences and random genetic drift. When these variations persist and get passed on to subsequent generations, they might not conform to the species tree we construct based on other evidence, like fossils or morphological features.\nBoth these phenomena—ancestral polymorphism (genetic differences in a common ancestor) and genetic drift (random changes in genetic frequency)—can cause incomplete lineage sorting. This results in the SNP pattern not following the expected species evolutionary tree because some genetic variations persist across lineages despite the species branching out from a common ancestor."
  },
  {
    "objectID": "chapters/week11.html#genetic-distances",
    "href": "chapters/week11.html#genetic-distances",
    "title": "11  Population Analysis",
    "section": "11.2 Genetic Distances",
    "text": "11.2 Genetic Distances\n\n\n\n\n\nIllustration of Recombination\n\n\n\n\nRecombination is a genetic process where sections of DNA, including SNPs (Single Nucleotide Polymorphisms), are shuffled or exchanged between chromosomes during cell division. This process results in new combinations of genetic variations.\nOne important aspect related to SNPs and recombination is linkage disequilibrium. Linkage disequilibrium refers to the non-random association or correlation of specific alleles at two different locations in the genome within a general population. This means that certain alleles at different locations tend to occur together more frequently than would be expected by chance.\nSNPs that are physically close to each other on the genome tend to exhibit higher linkage disequilibrium, meaning they are more likely to be inherited together. However, as the distance between SNPs increases along the genome, the degree of correlation or linkage disequilibrium between them decreases.\nThe concept of a haplotype block is related to linkage disequilibrium and recombination. A haplotype block refers to a region in the genome where specific sets of alleles are inherited together as a result of limited or no history of recombination. In these blocks, certain combinations of genetic variations tend to stay together across generations because they are less likely to be broken apart by the recombination process.\n\n11.2.1 Calculating Genetic Distances\nGenetic distance refers to a measure used to understand how closely related two individuals are based on their genetic information, particularly Single Nucleotide Polymorphisms (SNPs).\nOne way to calculate this relatedness is through the concept of Identity by State (IBS) distance. IBS distance looks at long stretches of the genome where two individuals share at least one allele (or genetic variant) at each location. By identifying these continuous regions where the genetic information matches between two individuals, scientists can gauge their genetic relatedness.\nIdeally, the best method to determine genetic distance would involve identifying haplotype blocks, which are regions of the genome where specific combinations of alleles tend to be inherited together. Comparing these blocks between two individuals’ genomes would give a precise measure of relatedness. However, this method requires information about the complete sequences of these blocks, which is often not available from standard sequencing data. This limitation arises because the individual reads obtained from sequencing are usually too short to cover entire haplotypes.\nAs a result, IBS distance calculates relatedness by considering scenarios where at least one of the underlying haplotypes could be shared between two individuals. The longer the matching regions of genetic information (indicating the potential sharing of a common haplotype block), the closer the genetic relatedness between the individuals.\n\n\n11.2.2 IBS Distances\n\n\n\n\n\nIllustration of how IBS Distances Work\n\n\n\n\nIdentity by State (IBS) distance is a method used to measure genetic relatedness between individuals by examining long shared segments of their DNA known as haplotype blocks. These haplotype blocks are regions in the genome where certain combinations of genetic variants tend to be inherited together.\nOne key characteristic of IBS distance is that as the genetic distance between individuals increases, the length of these shared haplotype blocks tends to decrease. In other words, when two individuals are more distantly related, they are less likely to share long stretches of identical genetic information.\nWhile IBS distance provides a useful measure of genetic relatedness based on these shared haplotype blocks, for a more accurate and detailed understanding of exact relationships between individuals, a closer analysis of these haplotype blocks becomes necessary.\n\n\n11.2.3 Tracking Genealogy\nPublic companies specializing in genetic testing, like 23andMe, circleDNA, and Nebula Genomics, use various technologies to analyze Single Nucleotide Polymorphisms (SNPs) in individuals’ DNA.\nThese companies employ different methods such as SNP chips (used by 23andMe), whole exome sequencing (used by circleDNA), and whole-genome sequencing (used by Nebula Genomics) to decode and interpret genetic information. They have amassed a substantial database, collectively having over 25 million people enrolled.\nWhen individuals send in their DNA samples, these companies compare the genetic information in these samples to others in their databases. By doing so, they can identify long segments of shared genetic information known as Identity by State (IBS) stretches, helping to trace relatedness and ancestry among different individuals.\nThis analysis enables these companies to determine potential relatives by identifying shared genetic markers. Moreover, they can provide insights into an individual’s ethnic background based on the genetic information obtained.\nAdditionally, these genetic testing services compare an individual’s DNA to known genetic markers associated with certain diseases or specific traits. This helps users understand their potential genetic predispositions to certain health conditions or their likelihood of expressing particular characteristics.\nHowever, it’s essential to note that while these associations between genetic markers and traits exist, they often have relatively weak correlations. This is sometimes referred to as “missing heritability,” where the identified genetic markers do not fully explain the inheritance or development of certain traits or diseases. There are likely other genetic or environmental factors that play significant roles, beyond what is currently identified through these associations\n\n11.2.3.1 Security Issues?\nThe convenience of genetic testing and tracking genealogy comes with potential security and privacy concerns:\nFirstly, when you submit your DNA to a company, your genetic information becomes part of their database. This information, including your SNPs (Single Nucleotide Polymorphisms), can be used to find new connections between your genetic makeup and certain traits or conditions. Big medical companies find large SNP databases incredibly valuable for their research purposes.\nThere’s a particular interest in identifying “natural gene knock-outs,” which are genetic variations that eliminate the function of a certain gene. Understanding these can shed light on how genes influence traits or diseases.\nMoreover, several companies like GEDmatch, MyHeritage, FamilyTreeDNA, and LivingDNA allow individuals to deposit their genetic data and compare it against their databases. This exchange of information can reveal extensive details about an individual’s genetic makeup.\nUnfortunately, these databases are susceptible to potential breaches in genetic privacy. Tailor-made data queries can exploit these databases, revealing more information than intended or agreed upon.\nAs legislation struggles to keep pace with this rapidly evolving field, there’s a concerning gap in regulation to protect individuals’ genetic privacy. The novelty of genetic testing and genealogical tracking means that the legal framework is not yet comprehensive enough to address the risks adequately."
  },
  {
    "objectID": "chapters/week11.html#species-versus-populations",
    "href": "chapters/week11.html#species-versus-populations",
    "title": "11  Population Analysis",
    "section": "11.3 Species versus Populations",
    "text": "11.3 Species versus Populations\nSpecies are distinct groups of organisms that are capable of interbreeding and producing fertile offspring. They are characterized by significant differences in their genetic makeup, physical characteristics, and behaviors. The variation observed between different species tends to be greater than the variation seen within populations of the same species.\nThe divergence between species arises from evolutionary processes over extended periods. A common ancestor shared by two different species existed far back in time, often millions of years ago. This prolonged separation allowed for substantial genetic and phenotypic differences to accumulate, leading to the development of reproductive barriers.\nReproductive barriers are mechanisms or traits that prevent two different species from interbreeding successfully or producing fertile offspring. Over time, these barriers become established and can include differences in mating behaviors, genetic incompatibility, physical barriers, or ecological factors.\nOne way of defining a species is by assessing the presence of reproductive barriers. When enough time has passed for these barriers to evolve and prevent gene flow between populations, scientists consider these populations as distinct species. In simpler terms, if groups of organisms have been separated long enough to develop mechanisms that prevent them from successfully interbreeding, they are typically classified as different species.\n\n\n\n\n\nIntrogression Illustration\n\n\n\n\nWhen individuals from two different species mate before the complete formation of reproductive barriers, a process called “introgression” can occur.\nIntrogression involves the transfer of genetic material between two distinct species through hybridization, where individuals from different species interbreed and produce offspring. In these cases, the genetic material from one species crosses into the gene pool of the other species.\nThis exchange of genetic material can lead to the incorporation of genes from one species into the genome of the other. Sometimes, this can result in hybrids that possess a mixture of characteristics from both parent species.\nThe extent and impact of introgression can vary. In some cases, if the hybrid offspring are not reproductively isolated from either parent species, they might continue to breed with one or both parent species. Over time, this process can lead to the spread of genetic material across the species boundary.\nIntrogression can have significant effects on the genetic diversity and evolutionary trajectories of species. It can potentially influence the adaptation and evolution of species by introducing new genetic variations into their gene pools. Additionally, introgression can blur the lines between distinct species, making it more challenging to delineate boundaries between them based solely on genetic markers\n\n11.3.1 Tests for Introgression?\nDo look at the slides to know more about this (i.e., prof. Jarkko talks about how the F3 introgression test works in more detail on that slide)!\n\n11.3.1.1 F3 Introgression Test\n\n\n\n\n\nF3 Introgression Test\n\n\n\n\nPositive evidence of introgression can be identified through statistical analyses that assess the patterns of genetic variation within genomes.\nWhen there’s no introgression between two populations or species, the genetic variations such as positive and negative values occur randomly throughout the genome. However, when introgression has taken place, certain statistical methods can reveal a skewed pattern in these genetic variations.\nOne such method involves calculating a statistic known as the F3 statistic and comparing it to an expected score based on the null hypothesis, which assumes random genetic variation. The distribution of this statistic under the null hypothesis can be determined by simulations or random sequences.\nIn cases where introgression has occurred, there tends to be an excess of negative values compared to positive values when calculating the F3 statistic across the genome. This skewed distribution with a prevalence of highly negative values indicates a signal of introgression.\nTo further evaluate the significance of this signal, statistical techniques can be applied to assess the likelihood of observing such a pattern under the null hypothesis. This evaluation can yield a p-value, indicating the probability of obtaining the observed pattern if there were no introgression. A low p-value suggests strong evidence against the null hypothesis and supports the presence of introgression.\nTo determine the direction of gene flow or introgression between two test populations, an additional species, typically an outgroup species (O), is introduced to serve as a reference point for ancestral allele states. This outgroup species is evolutionarily distant from the populations being studied and provides a baseline for understanding genetic changes that occurred before divergence.\n\n\n\n\n\nSetup for an Introgression Test\n\n\n\n\nThe setup involves several components:\n\nOutgroup species (O): This species, like chimpanzees for humans, serves as a reference to identify ancestral allele states. By comparing the genetic sequences of the outgroup species with those of the test populations, scientists can infer the ancestral alleles shared between the outgroup and the test populations before any introgression events.\nSecond species (W): This species is considered a possible source of introgression. By comparing the genetic sequences of the second species with the test populations, scientists can identify genetic variations that may have been introduced via introgression.\nTest populations (A and B): These are the populations being studied for introgression. By comparing their genetic sequences with the outgroup species and the second species, scientists can assess which genetic variations are shared with the outgroup (ancestral) and which are specific to the second species (potential introgression).\n\nBy analyzing the genetic variations shared between the outgroup species, the second species, and the test populations, researchers can deduce the direction of gene flow. If specific genetic variations are present in the second species (W) and also found in test population A but not in test population B, it suggests that gene flow occurred from species W into population A. Conversely, if these variations are found in population B but not in A, it indicates gene flow in the opposite direction.\n\n\n\n11.3.2 Tests for Directionality\n\n\n\n\n\nTesting for Directionality\n\n\n\n\n\n\n\nTesting for Directionality\n\n\n\n\n\n\n\nTesting for Directionality\n\n\n\n\nTo test for the directionality of introgression or gene flow between populations, scientists often search for Single Nucleotide Polymorphisms (SNPs) that exhibit two derived alleles.\nDerived alleles refer to genetic variants that are not present in the ancestral population or outgroup. When examining SNPs in test populations A and B and comparing them with an outgroup species, if a particular SNP shows two derived alleles (meaning both populations have a unique allele not found in the outgroup), it suggests a potential introgression event.\nThe key aspect scientists focus on is asymmetry in the distribution of these derived alleles between the two test populations. If, for example, the SNP has derived alleles that are present predominantly or exclusively in population A and not in population B, it indicates a higher likelihood of gene flow or introgression from another source into population A.\n\n\n\n\n\nTesting for Directionality via the ABBA-BABA Test\n\n\n\n\nThe ABBA-BABA test, also known as the D-statistic or Patterson’s D, is a method used in population genetics to detect introgression or gene flow between populations.\nThe test involves counting the occurrences of different allele configurations across multiple loci or genomic regions within four different populations or groups (A, B, W, and O). Specifically, researchers count how many times the pattern ABBA occurs versus the pattern BABA across these populations at various genetic loci.\nIn scenarios where genetic variations are randomly distributed or shaped only by drift (i.e., no gene flow between populations), the difference between the occurrences of ABBA and BABA should be close to zero when calculated across the entire genome.\nHowever, when there’s introgression or shared ancestry between populations, the ABBA and BABA counts will differ significantly from zero. A non-zero or highly non-zero value indicates a deviation from the expected pattern, suggesting potential gene flow or introgression between the studied populations."
  },
  {
    "objectID": "chapters/week11.html#population-history",
    "href": "chapters/week11.html#population-history",
    "title": "11  Population Analysis",
    "section": "11.4 Population History",
    "text": "11.4 Population History\nThe coalescent theory is a mathematical model used in population genetics to understand how gene variants sampled from a population trace back to a common ancestor. This theory helps us examine the historical patterns of genetic variation within populations.\nImagine looking backward in time, tracing the ancestry of gene variants present in a population. The coalescent theory explores this genetic history by simulating the merging of alleles, or gene variants, into a single common ancestral copy through a series of random coalescence events.\nThese coalescence events represent moments when gene variants from different individuals in a population merge into a single common ancestor. The model calculates the time or number of generations it takes for these variants to converge and share a common ancestral copy.\n\n\n\n\n\nIllustration of Coalesence Theory\n\n\n\n\nOne method involves segmenting the individual’s genome into regions with varying levels of heterozygosity, which refers to having different versions of a gene in an individual’s genome. Higher heterozygosity indicates regions where the genetic material originated from a more distant common ancestor, representing deeper coalescence events. In contrast, lower heterozygosity regions suggest genetic material that traces back to a more recent common ancestor.\nThese different segments or regions of varying heterozygosity provide insights into the historical recombination events that occurred in the population’s past. Recombination refers to the shuffling of genetic material during reproduction, resulting in new combinations of genetic variants.\nRegions with many splits or changes in heterozygosity levels signify past ancestral recombination events. The number of these recombination events is often linked to the historical population size. Larger populations tend to have more recombination events due to increased genetic diversity and more opportunities for genetic material to mix during reproduction.\n\n11.4.1 Site Frequency Spectrum\nThe site frequency spectrum (SFS) is an alternative method in population genetics used to analyze genetic variation within a population. It involves creating a histogram that shows the frequency of a particular allele (often a derived allele) across the population.\n\n\n\n\n\nSite Frequency Spectrum\n\n\n\n\nIdeally, the SFS is created by comparing the frequencies of alleles in the studied population with the ancestral genome. However, if the ancestral genome is not available, the “folded” spectrum is used. The folded spectrum combines the frequencies of derived alleles with their complementary ancestral alleles, which means fixated alleles (those that have completely replaced the ancestral form) and rare SNPs can’t be distinguished. This leads to a reduction in resolution as information about these alleles becomes limited.\nDifferent population histories result in distinct site frequency spectra. The goal with this method is to identify the historical scenario that is most likely to have produced the observed spectrum."
  },
  {
    "objectID": "chapters/week11.html#causative-mutations",
    "href": "chapters/week11.html#causative-mutations",
    "title": "11  Population Analysis",
    "section": "11.5 Causative Mutations",
    "text": "11.5 Causative Mutations\nCausative mutations refer to genetic alterations that directly impact the function or structure of proteins encoded by specific genes. These mutations can lead to changes in the amino acid sequence of a protein, which might affect its normal function or activity.\nWhen a mutation alters the sequence of amino acids in a protein, it can result in various outcomes. For instance, it might modify the protein’s shape or ability to interact with other molecules, affecting its function. In the most extreme cases, a mutation can introduce a premature STOP codon in the genetic code, causing the protein to be shorter than normal or non-functional.\nThese mutations that cause changes in protein structure or function are often referred to as “natural mutants.” However, it’s important to note that most of these mutations tend to reduce the fitness or health of an organism rather than improve it. In other words, they usually have negative effects on the organism’s ability to survive, reproduce, or function optimally within its environment.\nWhile some mutations might have neutral or even beneficial effects under certain conditions, many causative mutations are associated with detrimental outcomes. They can lead to various genetic disorders, diseases, or functional impairments in organisms by disrupting normal biological processes or cellular functions.\n\n11.5.1 Beneficial Mutations\nBeneficial mutations are genetic changes that confer advantages to an organism, increasing its fitness or survival in a particular environment. Unlike deleterious mutations that decrease fitness, beneficial mutations lead to positive selection, causing their allele frequencies to rise rapidly over successive generations within a population.\nImagine if “LAZY” were a beneficial mutation instead of a slightly deleterious one. In this case, individuals carrying the beneficial mutation would have a higher chance of surviving, reproducing, or thriving in their environment compared to others without the mutation. This advantage would lead to the rapid increase in the frequency of the mutated allele in the population.\nAs this advantageous allele becomes more prevalent, it undergoes a process known as genomic hitchhiking. Alleles that are physically close to the beneficial mutation on the chromosome also experience an increase in frequency because they are in linkage disequilibrium with the advantageous allele. Linkage disequilibrium refers to the non-random association of alleles at different loci.\nThe phenomenon of genomic sweeps occurs as a result of this process. Genomic sweeps involve regions in the genome with fewer genetic variations or Single Nucleotide Polymorphisms (SNPs) due to the fixation of alleles associated with the beneficial mutation. Essentially, all the alleles in that region have reached fixation, resulting in reduced genetic diversity in those specific genomic regions across the population.\n\n\n11.5.2 Sweep Analysis\n\n\n\n\n\nSweep Analysis\n\n\n\n\nDuring a selective sweep, a beneficial mutation increases in frequency and leads to a reduction in genetic diversity in the surrounding genomic area. One consequence is a decrease in the number of intermediate-frequency alleles in the region affected by the sweep.\nLinked genetic variations, particularly those associated with the beneficial allele, experience an increase in frequency due to the selective sweep. These intermediate alleles linked to the advantageous mutation tend to shift toward higher frequencies, transforming into high-frequency Single Nucleotide Polymorphisms (SNPs) within the affected region.\nAs the sweep progresses and the advantageous allele reaches fixation or near-fixation in the population, a new scenario emerges. Following the sweep, new mutations begin to accumulate in the region previously impacted by the selective sweep.\n\n11.5.2.1 One Possible Approach\n\n\n\n\n\nOne Method for Sweep Analysis\n\n\n\n\nThis method involves assessing the frequency of derived alleles—genetic variants that differ from the ancestral form—near the SNP subjected to selection. When a beneficial mutation undergoes a selective sweep, a notable increase in high-frequency derived alleles is observed, signifying that these SNPs have reached fixation in the population due to positive selection.\nConversely, the presence of low-frequency derived alleles indicates the emergence of new genetic variations subsequent to the selective sweep. During this process, the number of intermediate-frequency alleles typically decreases due to the impact of the sweep.\nTo perform this analysis, researchers calculate the site frequency spectrum (SFS) locally around the SNP of interest. The SFS represents the distribution of allele frequencies in a specific genomic region. By comparing this local SFS to the average SFS across the entire genome, scientists can identify deviations or shifts in allele frequencies caused by the selective sweep."
  },
  {
    "objectID": "chapters/week12.html",
    "href": "chapters/week12.html",
    "title": "12  Comparative Genomics",
    "section": "",
    "text": "When examining genomes, the analysis occurs at different levels to understand both the variations within a species for adaptation and the evolutionary differences between species.\nWithin a species, researchers focus on smaller-scale differences known as Single Nucleotide Polymorphisms (SNPs), which are tiny genetic variations at the single nucleotide level. These SNPs provide insights into population genetics, allowing scientists to study genetic diversity and adaptations within a population. On a larger scale, structural variants such as insertions, deletions, and rearrangements are explored. These structural variants are a part of pan-genome studies, which aim to uncover genetic diversity beyond the core genome of a species.\nWhen comparing genomes between different species, the emphasis is on evolutionary analysis. This involves investigating the evolution of genes or gene families across species to comprehend their divergence or conservation. Additionally, researchers construct phylogenies or evolutionary trees to illustrate the relationships between species based on genetic similarities and differences.\nVarious methods are employed in comparing genomes. Synteny analysis focuses on examining the order and arrangement of genes along chromosomes, providing insights into how genetic material is conserved or rearranged across species. The study of gene content, involving gene families and their expansions or contractions, sheds light on the evolutionary changes in genetic functions. Moreover, genomic alignment is utilized to align entire genomes, revealing regions of high conservation (ultraconserved regions) or areas susceptible to recombination (recombination hotspots).\nMolecular phylogeny is a method used to infer taxonomic relationships among species by analyzing genomic data. It involves several steps, including multiple sequence alignment and the estimation of gene trees.\nThrough multiple sequence alignment, researchers compare genetic sequences from different species to identify similarities and differences. This process helps in estimating gene trees, which illustrate the evolutionary relationships between different genes or genomic regions across species.\nSpecies phylogeny, inferred from these gene trees, reflects mutations that have reached fixation in distinct species over evolutionary time. Fixed mutations are alterations in the genetic code that have become prevalent and stable within specific species populations, distinguishing them from other related species.\nIn the context of identifying conserved regions in proteins, molecular phylogeny contributes to recognizing areas within proteins that remain highly conserved across species. These conserved regions are important because they often perform essential functions and have persisted throughout evolutionary history due to their significance."
  },
  {
    "objectID": "chapters/week12.html#substitution-rates-and-molecular-clocks",
    "href": "chapters/week12.html#substitution-rates-and-molecular-clocks",
    "title": "12  Comparative Genomics",
    "section": "12.1 Substitution Rates and Molecular Clocks",
    "text": "12.1 Substitution Rates and Molecular Clocks\n\n\n\n\n\nMutations\n\n\n\n\nWhen an organism has a mutation, it’s usually passed down from their generation to the next (assuming that they have offspring).\n\n12.1.1 Comparing Sequences\n\n\n\n\n\nSequences\n\n\n\n\nDynamic programming algorithms, such as the Needleman-Wunsch algorithm, offer a global solution for pairwise sequence alignment, crucial in bioinformatics. Dotplots are graphical representations aiding in sequence alignments.\nIn constructing a dotplot:\n\nGrid Representation: Imagine a grid where sequences are placed on the X and Y axes. The grid starts in the upper left corner and concludes at the lower right corner.\nMovement Options: The algorithm allows three possible moves: right, down, or diagonally down-right. These movements within the grid help build the alignment.\nScoring Matches and Gaps: Passing through a cell containing matching letters elevates the alignment score, contributing to the similarity of the sequences. Conversely, moving through an empty cell introduces a gap, incurring a penalty that impacts the alignment score.\n\nDotplots visually demonstrate matches (dots) in the grid where sequences align. A dot is plotted when a cell in the grid signifies a match, indicating similar segments between the sequences. Dotplots assist in identifying regions of similarity, divergence, gaps, or duplications between sequences, aiding biologists in understanding genetic relationships and structural similarities between DNA or protein sequences.\nHowever, one key thing to note about this approach is that it is an NP-complete problem.\n\n\n12.1.2 Finding Local Optimas\n\n\n\n\n\nExamples of Multiple Sequence Alignment Methods\n\n\n\n\nIn the context of multiple sequence alignment (MSA), achieving tractable solutions often involves finding local optima through two main strategies: progressive alignment and iterative alignment. Progressive alignment methods, such as CLUSTAL, begin by conducting pairwise alignments for every sequence pair within the dataset. These pairwise alignments enable the calculation of sequence distances, establishing a measure of similarity or difference between sequences. Utilizing these distances, a guide tree, representing the evolutionary relationships among sequences, is estimated. Subsequently, the sequences are aligned following the order specified by this guide tree, aiming to progressively align sequences based on their inferred evolutionary relationships.\nOn the other hand, iterative alignment methods (e.g., MUSCLE, MAFFT, PRANK, PASTA) employ a different approach. These methods often build upon initial alignments, frequently starting from a solution generated by algorithms like CLUSTAL. Rather than focusing solely on pairwise alignments, iterative alignment algorithms proceed by calculating new pairwise distances from the resulting multiple sequence alignment. A new guide tree is then estimated based on these updated distances. Using this updated guide tree, sequences are aligned once again, leveraging the refined evolutionary insights to improve the accuracy of the multiple sequence alignment. This iterative process continues, refining the alignment based on updated distance estimations and guide trees to converge towards a more accurate solution.\n\n12.1.2.1 Iterative Alignment Methods\n\n\n\n\n\nIllustration of the Iterative Alignment Method\n\n\n\n\nThe iterative alignment approach operates under the assumption that if the correct evolutionary history between species were known, the alignment process would be easier to solve, and conversely, an accurate alignment would facilitate inferring the correct evolutionary relationships.\n\n\n12.1.2.2 Estimating Phylogenies\nEstimating phylogenies involves several steps to infer the evolutionary relationships among species:\n\nGene Selection: Begin by selecting a set of genes that are present as single copies in each species under study. These genes provide a reliable basis for comparing evolutionary relationships across species.\nMultiple Sequence Alignment: Conduct individual multiple sequence alignments for each of the selected genes. This step involves aligning the sequences of these genes from different species to identify similarities and differences in their sequences.\nConcatenation: Combine or concatenate the results obtained from the individual gene alignments. This concatenation generates a composite dataset that merges the aligned sequences from all the selected genes into a unified alignment.\nTree Estimation: Utilize the concatenated alignment data to estimate a phylogenetic tree. Various methods, such as maximum likelihood or Bayesian inference, can be employed to construct the tree that represents the evolutionary relationships among the species.\nAlignment Refinement: Based on the estimated tree, refine the sequence alignments to improve accuracy. Adjust the sequence alignments according to the inferred evolutionary relationships depicted in the tree.\nIterative Process: Iteratively iterate between steps 4 and 5, refining the tree estimation and sequence alignment. Repeat this process until convergence, where the estimated tree and alignments become stable or reach a point of optimal accuracy, indicating a more reliable depiction of the species’ evolutionary history. This iterative approach allows for continuous refinement, enhancing the accuracy of both the phylogenetic tree and the sequence alignments used to infer it.\n\nEmpirically, the iterative methods of alignment and tree estimation usually converge, iteratively refining the alignment and evolutionary tree estimation process. This convergence suggests that these methods gradually improve alignment accuracy by iteratively refining the evolutionary relationships and sequence alignments.\nObservationally, iterative methods tend to outperform progressive alignment strategies."
  },
  {
    "objectID": "chapters/week12.html#synteny",
    "href": "chapters/week12.html#synteny",
    "title": "12  Comparative Genomics",
    "section": "12.2 Synteny",
    "text": "12.2 Synteny\n\n\n\n\n\nExample of a Synteny Plot\n\n\n\n\nIn the context of comparing genomic sequences, dot plots provide a visual representation of the similarities and differences between two genomes. The process begins by aligning the two genomic sequences, with approximate alignment methods like BLAT (a tool similar to BLAST, but faster with potentially less precise alignments).\nWhen creating a dot plot for genome comparison:\n\nx/y Axes: The x and y axes represent the gene order along the chromosomes of the two genomes being compared.\nDot Plot Representation: Each dot (or point) in the plot signifies a potential match between two corresponding gene pairs from the compared genomes. A dot is plotted when the level of similarity or match between these gene pairs surpasses a predefined threshold, usually based on a set BLAST (or similar alignment tool) threshold.\n\n\n\n\n\n\nExample of Synteny\n\n\n\n\nSynteny refers to the conservation of the same gene order between two or more genomes, indicating evolutionary relationships or shared ancestry. However, identifying the exact level of similarity in synteny can be challenging due to potential gaps or the insertion of new genes in the genomes being compared.\nTo overcome these challenges and accurately calculate gene-level synteny, researchers are developing specialized applications or tools. These tools incorporate predefined thresholds or criteria to assess the level of similarity in gene order between genomes. These thresholds account for variations such as gaps or the presence of newly inserted genes, enabling a more precise and nuanced evaluation of synteny at the gene level.\n\n12.2.1 Synonymous versus Non-Synonymous Loci\n\n\n\n\n\nSynonymous Versus Non-Synonymous Loci\n\n\n\n\nThe comparison between synonymous (Ks) and non-synonymous (Kn) loci involves examining pairs of genes that are syntenic after alignment. These categories are based on the type of mutations observed between genes.\n\nSynonymous Mutations (Ks): Synonymous mutations do not result in alterations to the protein sequence. These mutations typically occur in positions within the DNA sequence that code for amino acids but are situated in the third codon position. In the genetic code, alterations in the third position of a codon often don’t change the encoded amino acid due to redundancy (degeneracy) in the genetic code. Hence, mutations in these positions often do not affect the resulting protein sequence.\nNon-Synonymous Mutations (Kn): Non-synonymous mutations, in contrast, lead to changes in the protein sequence. These mutations alter the DNA sequence in a way that results in a change in the encoded amino acid within the protein sequence. Consequently, these mutations impact the structure or function of the protein produced from the gene.\n\nAs a simple descriptive statistic, researchers often calculate the number of synonymous and non-synonymous mutations within these syntenic gene pairs. This comparison helps in understanding the selective pressures acting on genes or gene regions. The ratio of synonymous to non-synonymous mutations (Ks/Kn ratio) can also provide insights into the evolutionary constraints and adaptive changes affecting these genes or genomic regions across different species or populations.\n\n12.2.1.1 Estimating Whole Genome Duplication Events\n\n\n\n\n\nExample of Using Ks Histograms to Estimate WGD Events\n\n\n\n\nSynonymous substitution rate (Ks) can be a useful metric in estimating the timing of whole genome duplication events. When collecting Ks values from syntenic gene pairs across a genome and plotting them in a histogram, it can serve as a demonstration of a molecular clock.\nWhole genome duplication events result in pairs of genes, stemming from a common ancestor. Subsequent to duplication, these gene pairs start accumulating mutations independently. Assuming that mutations accumulate at a relatively constant rate (as per the molecular clock hypothesis), genes derived from the same duplication event would show a similar Ks value distribution.\nIn this scenario, if all genes were duplicated at a single event, they have been accruing mutations since that common duplication, progressing at a uniform rate across the genome. Due to the stochastic nature of mutation accumulation, a distribution of Ks values around a central mean or median is expected. This distribution indicates the varying mutation rates across different gene pairs since their duplication, potentially revealing the timing of whole genome duplication events based on the spread or dispersion of Ks values. The wider the distribution, the more time has passed since the duplication event, leading to greater divergence and variability in Ks values among gene pairs.\n\n\n12.2.1.2 Estimating Time of Species Divergence\n\n\n\n\n\nExamples of Species Split Estimation\n\n\n\n\nBy identifying syntenic gene pairs between two distinct species, researchers can examine the synonymous substitution rate (Ks) for these gene pairs. The rationale behind this approach lies in the concept of coalescence to a common ancestor. If the two species being compared shared a common ancestor, genes in their genomes have been accumulating mutations independently since the time of their divergence.\nThe Ks values obtained from these syntenic gene pairs between the species serve as an indicator of the divergence time between the two lineages. Higher Ks values typically suggest a more extended period since the species’ divergence, signifying increased accumulation of synonymous mutations within these gene pairs. Conversely, lower Ks values indicate a more recent divergence between the species, with less time for synonymous mutations to accumulate in the identified gene pairs."
  },
  {
    "objectID": "chapters/week12.html#gene-family-evolution",
    "href": "chapters/week12.html#gene-family-evolution",
    "title": "12  Comparative Genomics",
    "section": "12.3 Gene Family Evolution",
    "text": "12.3 Gene Family Evolution\n\n\n\n\n\nIllustration of Key Terms\n\n\n\n\nWhile aligning orthologous genes between species might seem straightforward and might generally reflect the species tree, the reality is often more complex. Prof. Jarkko also begins this section by listing some key terminologies:\n\nHomologs: these are genes that share a common origin throughout their evolution (e.g., there’s a common ancestor in an evolutionary tree that has this gene).\nOrthologs: these are similar genes or DNA sequences that pretty much performs the same function (e.g., two different genes in two different organisms that both encode some portion of the ribosomes for protein synthesis).\nParalogs: these are homologs (in a single species) that arose solely via gene duplications.\n\n\n12.3.1 Common Ancestors and Lineage Splits\n\n\n\n\n\nBinary Species Tree Between Fish and Plants\n\n\n\n\nWhen we look at plants, we see that their evolutionary history are riddled with many instances of whole genome duplications and tandem duplications (likewise for their common ancestors).\n\n\n12.3.2 Gene Tree Reconciliation\n\n\n\n\n\nIllustration of Gene Tree Reconciliation\n\n\n\n\nGene tree reconciliation addresses the discrepancy that can arise between the gene tree (depicting the evolutionary relationships of genes) and the species tree (illustrating the evolutionary relationships among species). Due to various evolutionary events like gene duplications, losses, horizontal gene transfers, and evolutionary rate variations, gene trees may differ from the expected species tree.\nThis reconciliation process aims to find the most probable evolutionary history of genes given the known species tree. It involves computationally challenging tasks that seek to align or reconcile the gene tree with the species tree by identifying the minimum number of evolutionary events, such as gene duplications or losses, required to achieve congruence between the two trees.\nThe challenge lies in determining the optimal reconciliation scenario that minimizes the discrepancies between the gene tree and the species tree. This process is computationally intensive and typically involves algorithms that explore various possible scenarios of gene duplications, losses, and other events to find the fewest number of events necessary to reconcile the gene tree with the species tree."
  },
  {
    "objectID": "chapters/week12.html#computational-tools-for-comparing-gene-content-between-species",
    "href": "chapters/week12.html#computational-tools-for-comparing-gene-content-between-species",
    "title": "12  Comparative Genomics",
    "section": "12.4 Computational Tools for Comparing Gene Content Between Species",
    "text": "12.4 Computational Tools for Comparing Gene Content Between Species\nOrthogroups, also known as computational gene families, offer a more efficient and standardized method for defining gene families compared to manual approaches, which can be laborious and challenging due to the diverse nature of gene families.\nInstead of manually delineating each gene family, researchers use computational methods to establish orthogroups. The process involves:\n\nAll vs All BLAST: Conducting a comprehensive sequence similarity search using BLAST (Basic Local Alignment Search Tool) for all genes in a given genome. This generates a similarity matrix that details the pairwise similarities between all genes in the dataset.\nMarkov Clustering: Utilizing the similarity matrix generated from BLAST results, researchers apply clustering algorithms such as Markov Clustering (MCL). MCL helps identify groups of genes that exhibit comparable sequence similarities based on the BLAST results. This algorithm partitions genes into clusters where genes within a cluster share significant sequence similarities with each other.\n\nThe outcome of this computational process is the formation of orthogroups or computational gene families. Each orthogroup consists of genes clustered together based on their similarities as indicated by BLAST results. Genes within the same orthogroup are considered to be part of the same gene family, sharing a level of sequence similarity that suggests their evolutionary relatedness.\n\n12.4.1 STAG Method\nThe Species Tree with Average Gene (STAG) method utilized in OrthoFinder aims to construct a species tree based on orthogroups derived from gene trees. Here’s how it works:\n\nGene Tree Analysis: For each gene tree, STAG identifies the shortest distances between homologs in different species. It looks for the most closely related gene copies among species in these gene trees.\nSpecies Homolog Selection: STAG retains only one gene copy per species, selecting the copy that exhibits the shortest distances to homologs in other species. This step intends to choose the gene copy that is most representative of its homologs across various species.\nTree Estimation: Using the selected gene copies and their distances, STAG constructs an estimated tree that reflects the evolutionary relationships between species based on these gene copies.\nConsensus Species Tree: By amalgamating the individual estimated trees derived from numerous gene families, STAG creates a consensus species tree. This consensus tree represents an average or collective view of the evolutionary relationships among species.\n\nThe underlying assumption of the STAG method is that while mistakes might occur during the selection process of gene copies, leveraging information from a large collection of gene families can help average out these errors. This approach relies on the principle that errors or inaccuracies in the selection of individual gene copies are balanced out when considering a broad spectrum of gene families. Consequently, the consensus species tree derived from these collective gene families is expected to provide a more accurate representation of the evolutionary relationships among species.\n\n\n12.4.2 Tree Rooting\n\n\n\n\n\nIllustration of Tree Rooting\n\n\n\n\nRooting a phylogenetic tree involves determining the position of the most recent common ancestor (MRCA) and establishing the direction of evolution from that point. Several approaches exist for rooting trees:\n\nKnown Outgroup Species: Using a known outgroup species that is evolutionarily distant from the group under study. This outgroup is expected to share a common ancestor with the ingroup but diverged earlier, providing a basis for rooting the tree.\nGene Duplication Events (STRIDE): The STRIDE method, proposed by Emms and Kelly, leverages gene duplication events for tree rooting. It’s based on the principle that gene duplications are irreversible, meaning two genes cannot merge back into one. By identifying a root where the duplication events align with the timing constraints of evolution, this method seeks a root that best fits the pattern of gene duplication events across the tree.\n\nThe concept here is to pinpoint a root position on the phylogenetic tree where the inferred gene duplication events align with the expected evolutionary timeline. For instance, identifying a root where gene duplication events occur in a manner that is consistent with the known rates of evolution and divergence times among species.\n\n\n12.4.3 Birth-Death Model\nThe birth-death model provides a formal probabilistic framework for estimating the turnover rates of gene families, offering insights into significantly expanded or contracted gene families within an evolutionary context.\nHere’s an overview of the process:\n\nEstimating Birth/Death Rates: Using the birth-death model within a phylogenetic framework, the consensus birth and death rates of gene families are estimated across the evolutionary tree. This estimation involves determining the rates at which new genes are gained (birth rate) and existing genes are lost (death rate) within different gene families over time.\nIdentifying Deviations from Consensus: Once the consensus birth and death rates are estimated, the model is used to identify gene families that significantly deviate from this general consensus. These deviations indicate gene families that have undergone remarkable expansions or contractions compared to the expected turnover rates derived from the overall evolutionary pattern.\n\nThis approach allows for a quantitative assessment of how gene families evolve across a phylogeny. It enables the identification of specific gene families that have experienced exceptional evolutionary dynamics, exhibiting substantial expansions or contractions beyond the typical rates observed across the evolutionary tree. This information offers valuable insights into the evolutionary forces driving the diversity and dynamics of gene families within different species or lineages."
  }
]