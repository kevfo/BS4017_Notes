[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BS4017: High Throughput Bioinformatics",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "chapters/week1.html",
    "href": "chapters/week1.html",
    "title": "1  Introduction to Linux and the Command Line (untested)",
    "section": "",
    "text": "This week’s lecture aims to cover the following topics:\nLike the title of chapter implies, the information presented in this chapter will not be tested."
  },
  {
    "objectID": "chapters/week1.html#a-brief-introduction-to-computers",
    "href": "chapters/week1.html#a-brief-introduction-to-computers",
    "title": "1  Introduction to Linux and the Command Line (untested)",
    "section": "1.1 A Brief Introduction to Computers",
    "text": "1.1 A Brief Introduction to Computers\n\n1.1.1 Turing’s Machine\n\n\n\n\n\nIllustration of a Turing Machine\n\n\n\n\nA universal machine is a machine that can solve any sequence of problems that can be solved using a computer. An machine is called Turing complete if it can act like a Turing machine: a machine that is capable of following certain rules to solve problems in a stepwise fashion. Many of today’s programming languages are Turing-complete.\n\n\n1.1.2 Machine Language\nMachine language is the only thing that computers understand.\nThe central processing unit (i.e., CPU) is the so-called “brain” of the computer and uses bits to work. When one talks about a “64-bit: CPU, it means the groups of bits are 64 in length.\nComputers understand instructions in a language made of bits, but it’s really tough for people to read and figure out what’s going on. So, to make it easier, an operating system (i.e., OS) is necessary. This is like a middleman that helps us talk to the computer in a way we can understand better."
  },
  {
    "objectID": "chapters/week1.html#unix-os",
    "href": "chapters/week1.html#unix-os",
    "title": "1  Introduction to Linux and the Command Line (untested)",
    "section": "1.2 Unix OS",
    "text": "1.2 Unix OS\nThe Uniplexed nformation and Computer Service (i.e., Unix for short) OS - often pronounced as “eunuchs” was created at Bell Laboratories in the early 1970s to help make software.\n\n\n\n\n\nPhotograph of Ken Thompson and Dennis Ritchie\n\n\n\n\nIn 1969, it started as a bunch of instructions written in a language computers understand, and it was made by Ken Thompson. Then, between 1972 and 1974, Ken Thompson and Dennis Ritchie made a new version using a language called “C”. Back then, anyone could look at and use the code for free.\nBut in the early 1980s, the company AT&T decided to keep the code secret and started selling licenses for using Unix. It even led to different versions of Unix being made by different companies.\nInterestingly, Mac OS X, the operating system used in Mac computers, is a type of Unix too!\n\n1.2.1 Unix Philosophy\nThe design idea behind Unix programs can be summarized into three bullet points:\n\nWrite programs that are really good at doing one thing.\nWrite programs that can team up and help one another out.\nWrite programs that are good at dealing with text, because text is one way that everyone can talk to one another.\n\nThe command line is also the key to using a Linux system.\nThere are also nine “paramount precepts” as Mike Gancarz summarizes:\n\nRemember, small things are nice.\nEach program should be awesome at doing just one thing.\nBuild a simple version first to test.\nIt’s better if your stuff works on lots of different computers.\nKeep your information in plain text files.\nUse existing software to help you do more.\nUse special scripts to make things even better and work on different computers.\nDon’t trap people in your program; let them do what they want.\nMake each program a tool to help with tasks."
  },
  {
    "objectID": "chapters/week1.html#linux-os",
    "href": "chapters/week1.html#linux-os",
    "title": "1  Introduction to Linux and the Command Line (untested)",
    "section": "1.3 Linux OS",
    "text": "1.3 Linux OS\n\n“I’m doing a (free) operating system (just a hobby, won’t be big and professional like gnu) for 386(486) AT clones.”\n– Linus Torvalds, August 25th, 1991.\n\nLinux is like a free operating system that works with Unix (i.e., older systems). When Unix started costing money in the 1980s, people wanted a free option. Linux is like the core part of a computer system (i.e., the kernel), connecting the software to the hardware and different programs to each other. To make it a complete system, it’s joined with other software, and this whole package is called a distribution.\n\n1.3.1 Why Use Linux?\nIt’s trustworthy and stays steady with fewer problems. There’s hardly any viruses to worry about. It’s quick because it’s built really well. Plus, Linux is free - the software is open, which means it works well with other things and gets better quickly.\nIf you want to make your own programs, it’s easy with Linux because there are free tools and helpful information. One can also change the Linux OS code if they want to.\n\n1.3.1.1 Why Use Linux for Bioinformatics Data Analysis?\nLinux has lots of little tools made by many people that can each do a small part of the work. One can mix these tools together to create pipelines that do big tasks.\nIt’s also a fantastic platform for open-source software, which means one can use many programming languages and libraries without paying.\nBut, keep in mind that using Linux might not be as easy as some other options. one’ll have to type commands on a special line, and it only does exactly what one tells it to, not what you might want it to do automatically."
  },
  {
    "objectID": "chapters/week1.html#bash-and-the-command-line",
    "href": "chapters/week1.html#bash-and-the-command-line",
    "title": "1  Introduction to Linux and the Command Line (untested)",
    "section": "1.4 Bash and the Command Line",
    "text": "1.4 Bash and the Command Line\n\n1.4.1 What is Bash?\nWhen one uses a computer with Unix, they can talk to it through something called a terminal emulator. This is a “window” to type things.\nThe terminal helps one use a special interface called a shell, which is like a way to talk to the computer by typing commands. There are different types of shells, but “bash” is the most common. It’s been around since 1989 and is used in systems like Linux and Apple’s OS X.\n\n\n1.4.2 The Command Line\nThe command line is a place where one types in what they want the computer to do. It’s not as easy as clicking on icons like in a Graphical User Interface (i.e., GUI), but it has its benefits.\n\n\n\n\n\nCommand Line Appearance After Logging into Singapore’s National Supercomputing Center\n\n\n\n\nIt’s a bit harder for beginners because they need to learn the right words (commands), but it’s faster and lets them do more. One can write down everything they do in a text file, which can be helpful. It’s also great for working with text and making the computer do things over and over automatically.\n\n1.4.2.1 Example: Connecting to a Remote Server Using an Encrypted SSH Protocol\nSecure shell (i.e., SSH) and secure copy (i.e., SCP) are like special ways to talk to a computer securely. They were made by someone named Tatu Ylönen in 1995.\n\n\n\n\n\nDiagrammatic Explanation of How SSH Works\n\n\n\n\nImagine one is sending secret messages over an unsecure network. SSH makes a safe pathway using a special key that has a lot of numbers. This key comes in two parts: the public key, which can be shared, and the private key, which keeps things secret.\n\n\n\n1.4.3 Basic Bash Operations\n\n1.4.3.1 Fundamental Operations\nSome basic commands include:\n\nls - this lists all files and folders in the current working directory.\ncd - this changes the working directory.\ncp - this makes a copy of a file.\n\nCommands also have something called flags that modify the output. For instance, ls -la lists all files (including the hidden ones) in the current working directory in long form.\nCommands also take in arguments to complete the command - for instance, cp file1 ../folder1.\nThe man command displays helpful information about a command - for instance, man ls will list information about the ls command.\n\n\n1.4.3.2 Bash Variables\nIn bash, one can use variables like containers for information.\nFor example, one can say A=1 to put the number 1 in a variable called A. When one wants to use that number, they can add a “$” before the variable’s name, like “$A”.\n\n\n\n\n\nExamples of Assigning and Displaying the Values of Bash Variables\n\n\n\n\nTo show the value of a variable on the screen, they can use echo and write echo $A. Some variables are made by the computer, like PATH, which tells the computer where to look for programs. One can also add their own places to look by changing PATH.\n\n\n1.4.3.3 Redirects\nWhen you one runs a regular Unix command, it shows words on the screen. However, one can make those words go somewhere else too.\nFor example, if they want to list files and save the list in a file, they can write ls > listfile.txt. If they want to add more files to that list, they can write ls >> listfile.txt.\nThey can also make the words go to another command by using a pipe, like ls | grep listfile.\nYou can do this as many times as you need.\n\n\n\n1.4.4 Linux File System\nIn Unix, there aren’t “hard drives” like we usually think of them. Instead, there are directories. Think of these as special folders. ’\n\n\n\n\n\nExample of a Linux File System\n\n\n\n\nWhen one connects to a hard drive, it’s divided into pieces, and each piece is attached to a directory. So, the directory shows what’s in that piece of the hard drive.\nWhen one uses the “mount” command, it links a special part of the hard drive or even something from another computer to a directory. This way, the directory shows the stuff from that hard drive or computer.\n\n\n1.4.5 Permission Management in Linux\nIn Unix, there are three groups for who can do what with files. The first is the user who owns the file - they can keep things private.\nThen there’s a bigger group of users who can share files. Finally, there’s others, which means everyone else. There are three types of things you can do with files:\n\nread (r)\nwrite (w)\nexecute (x)\n\nThese permissions are like rules for each group saying what they can and can’t do with the files.\n\n\n1.4.6 Bash as a Turing-Complete Language\nThe bash shell isn’t just about doing things; it’s like a language for telling the computer what to do. The programs you make using the Bash language are called shell scripts. These scripts are like lists of instructions that the computer understands.\nThey’re translated and done by the computer right away, which makes Bash an “interpreted” language. Shell scripts are really good for quickly working with text and doing powerful things. For example, at NSCC, they have a system where you put your special list (script) in a line and the computer runs it when it’s ready.\n\n\n1.4.7 Useful Pointers When Using Linux\nProf. Jarkko also lists some tips when using Linux to work on tasks:\n\nYou can’t easily get back deleted files.\nSmall and capital letters matter in commands and file names.\nSome characters like #;& ” / ’ : < > | * ? $ ( ) { } [ ] and space do special things.\nIt’s safest to use only letters, numbers, _ (underscore), and . (dot) in file names.\nIf you use those special characters, put quotes around the name.\nFile names can be anything, like a mytext.txt file might not be text, but it’s good to follow conventions!"
  },
  {
    "objectID": "chapters/week1.html#how-does-programming-work",
    "href": "chapters/week1.html#how-does-programming-work",
    "title": "1  Introduction to Linux and the Command Line (untested)",
    "section": "1.5 How Does Programming Work?",
    "text": "1.5 How Does Programming Work?\n\n\n\n\n\nRelationships Between Levels of Programming Languages\n\n\n\n\nProgramming languages, like C/C++ and Visual Basic, or Python, R, and Matlab, use special tools to change the commands one writes into instructions the computer understands. These tools are like translators. Some languages use a compiler, which does the translation all at once, while others use an interpreter, which does it step by step.\n\n1.5.1 Compilers vs. Interpreters\nThere are two main ways to change one’s programming commands into computer language:\n\nCompiler:\nIt takes your list of commands (code) and changes it all into computer language at once. This gives one a file that they can run as a program. One can use it from the command line or through a nice interface.\nInterpreter:\nIt changes your commands into computer language one by one as they use them. They can give commands directly or run a list of them from a file called a script. This is like a set of instructions in a text file.\n\n\n\n1.5.2 Low versus High Level Languages\nLow-level programming languages are like really simple tools for computers. They only know how to do basic things, and they’re kind of like talking to the computer in its own language. They can work directly with memory and other computer parts. This makes them run really fast, but when one wants to do complex things, they have to write a lot of instructions, and it can be hard to find mistakes in their code.\nHigh-level programming languages are like using simpler words for computers. They’re farther away from the computer’s language, so they’re easier for people to understand. They work kind of like how humans talk, and they’re good for organizing things like objects. When one uses them, they can write less code because they can do complicated stuff in a simpler way. That’s why many people like using them for most things they create on computers."
  },
  {
    "objectID": "chapters/week2.html",
    "href": "chapters/week2.html",
    "title": "2  Genomic Sequencing and Databases",
    "section": "",
    "text": "Modern day genomic sequencing began with Sanger sequencing.\nSanger sequencing, created in 1977, is a method used to read the genetic code of living things. It’s like reading a very long sentence written in a language that cells use. This method can read up to 800 to 1000 ‘letters’ of this genetic sentence in one go. Think of it as reading a paragraph with 800 to 1000 words! And now, modern versions of this method can read 96 paragraphs all at once, making it faster and more efficient.\nStarting from 2004, progress in this field has been really fast. The first machinery that were used before that have become old-fashioned and aren’t used as much anymore."
  },
  {
    "objectID": "chapters/week2.html#sequencing-fundamentals",
    "href": "chapters/week2.html#sequencing-fundamentals",
    "title": "2  Genomic Sequencing and Databases",
    "section": "2.1 Sequencing Fundamentals",
    "text": "2.1 Sequencing Fundamentals\nThere are a few special words we need to know\n\nDepth\nThis is like how many times we read the same sentence in the book to be really sure we got it right.\nCoverage\nThis is like the average number of times we read different sentences in a part of the book.\nRead length\nThis is just how much of the book we can read at once. It’s like reading a long or short paragraph.\n\nWhen we’re reading this book, we want to make sure we cover every part equally, like giving the same attention to every page. This is called uniform coverage.\nThere are two main ways to read the book:\n\nOne way reads a lot of sentences quickly, but not all of them in detail.\nThe other way reads fewer sentences, but really understands them well.\n\nSo, you can choose to read a lot of the book or read less but really understand it deeply."
  },
  {
    "objectID": "chapters/week2.html#illumina-hiseq-and-novaseq",
    "href": "chapters/week2.html#illumina-hiseq-and-novaseq",
    "title": "2  Genomic Sequencing and Databases",
    "section": "2.2 Illumina HiSeq and NovaSeq",
    "text": "2.2 Illumina HiSeq and NovaSeq\n\n\n\n\n\nIllumina RNA Sequencing Machine Models\n\n\n\n\nCreating Illumina libraries is like preparing a special recipe for reading DNA.\nFirst, a long DNA strand and break it into smaller pieces. Then, the ends of these pieces are fixed to make them neat and ready. A special tail is then added to one end, like putting a ribbon on a gift.\nAfter that, tiny adapters, like bookends, are attached to both ends of the DNA pieces. This helps scientists to read them later. Only the right-sized pieces are desired, so any pieces that are too big or too small are removed.\nScientists make many copies of these DNA pieces so we have enough to read. Finally, they use a special machine to read the DNA letters one by one, just like flipping through the pages of a book really fast. This helps scientists learn about the DNA’s secrets and how it works!\n\n2.2.1 Typical Illumina Read Structure\n\n\n\n\n\nIllumina RNA Read Structures\n\n\n\n\nA usual Illumina read has the following structure:\n\nIllumina Forward\nImagine starting an exciting adventure book. The “Illumina Forward” is like the first page of the book, where the story begins. It’s the beginning point of our DNA reading journey.\nForward Target Primer\nThink of this like a special bookmark in the adventure book. It’s a tiny piece that helps find the exact spot to start reading about in the DNA. It’s like telling the story, “Hey, start reading right here!”\nTarget\nPicture this as a hidden treasure in the adventure story. The “target” is the part of the DNA that scientists really want to learn about. It’s like the exciting part of the story that holds a secret they’re curious about.\nReverse Target Primer\nJust like the “Forward Target Primer” helps scientists find the beginning, this is like another bookmark that helps scientists find the end of the part we’re interested in. It’s like saying, “Okay, stop reading here!”\nIllumina Reverse\nThis is like reaching the last page of a book. The Illumina Reverse marks the end of our reading journey for this section of DNA, letting scientists know that we’ve completed the sequence.\n\n\n\n2.2.2 Single End, Paired End, and Mate-Pairs\n\n\n\n\n\nExample of Mate Pair Construction to Construct a Full Genome\n\n\n\n\nIllumina is a method for reading DNA that’s like solving different puzzles. Imagine having pieces of a jigsaw puzzle.\nFirst, the scientist can look at just one side of each piece to see what’s there (i.e., single end). Or they can look at both sides of the pieces, but not the part in the middle (i.e., paired end). There’s also a special kind where they link bigger puzzle pieces together (i.e., mate-pair) to understand them better.\nIn the end, no matter which puzzle they choose, they get to read both sides and find out the average distance between them. These days, the most popular puzzle is the paired end, while the mate-pair puzzle is not used much.\nAlso, the smaller the puzzle piece, the harder it is to figure out where it fits in the bigger picture of the DNA. But having two sets of puzzle pieces helps scientists figure out where they go in the DNA picture more accurately.\n\n\n2.2.3 Barcoding\n\n\n\n\n\nExample of How a Barcode Can be Used\n\n\n\n\nIllumina sequencing can use something called **barcodes*&, which are like special tags added to each sequence. Think of them like labels on different books.\nImagine a library of books where and each book has a unique label. In the same way, the sequences get their own labels. If the labels are longer, it’s like having more specific tags for each book. This means you can put even more sequences together for reading, like having a huge shelf of books with similar tags.\nFor instance, a technology called 10x Genomics uses these longer labels to group sequences together, especially when looking at individual cells.\n\n\n2.2.4 FASTQ Formats\nIllumina returns data in a special text format called FASTQ. Imagine it’s like a recipe card for reading DNA. This card has four parts for each piece of DNA it reads:\n\nThe first part is like a name tag. It tells us which piece of DNA we’re looking at. Imagine it’s like a label on a box that says what’s inside.\nThe second part is the actual DNA sequence, like a secret code of letters (A, T, C, G, N). It’s like a coded message that we need to decode.\nThe third part is a repeat of the name tag. It’s like someone saying, “Hey, this is still the same box we talked about earlier.”\nThe fourth part is a set of quality codes. This helps us know how sure we are about each letter in the DNA sequence. It’s like having a confidence score for each letter.\n\nAnd these quality codes are kind of like a secret code too, but they’re similar to something called Phred. It’s like a scale that tells us how reliable each letter is in the DNA code, just like how we might trust different people’s opinions more or less.\n\n2.2.4.1 Phred Quality Scores\n\n\n\n\n\nPhred Score Meanings\n\n\n\n\nThe quality \\(Q\\) is calculated using the following formula:\n\\[\\begin{equation}\n  Q = -10\\log_{10}P\n\\end{equation}\\]\nThough, the scores are reported in something called ASCII to save space."
  },
  {
    "objectID": "chapters/week2.html#pacbio-sequencing",
    "href": "chapters/week2.html#pacbio-sequencing",
    "title": "2  Genomic Sequencing and Databases",
    "section": "2.3 PacBio Sequencing",
    "text": "2.3 PacBio Sequencing\nPacBio sequencing is a way to read DNA that’s like watching a movie frame by frame.\n\n\n\n\n\nHow PacBio Sequencing Works\n\n\n\n\nImagine a movie reel, but instead of film frames, there are DNA pieces. First, a special machine copies the DNA, making many identical pieces. These pieces are attached to a surface, like putting stickers on a wall.\nThen, a tiny camera watches as a machine adds one letter at a time to the DNA chain. It’s like watching someone write a story, but in DNA language. The machine records this process, and scientists can use it to figure out the DNA’s secrets. This method is special because it can read long pieces of DNA in one go, like reading long sentences without stopping.\n\n\n\n\n\nPacBio Sequencing Components\n\n\n\n\nPacBio sequencing works a bit like making a detailed copy of a story. The scientists start with a special DNA template called an SMRTbell. Imagine this template as the outline of the story. They put it on a surface and use a special machine to make copies of it. These copies are like drafts of the story. Then, a tiny helper called a “polymerase” comes in and reads the story, one letter at a time. It’s like reading the book aloud to remember every detail.\nBut here’s the cool part: the machine doesn’t just read the story once. It goes over it several times, each time reading a bit more. These shorter readings are called subreads. Think of them as reading a book chapter by chapter.\nNow, after all these readings, the scientists put everything together like a puzzle. It’s like taking all those drafts and arranging them to get the complete story. This final version is called a Circular Consensus Sequence, which is like having the perfect version of the story after making sure all the words are correct. This method helps us read long pieces of DNA with a high level of accuracy, just like getting the full story right.\n\n2.3.1 PacBio Read Formats\nWhen PacBio finishes reading DNA, it’s like recording a video of the process. This video is saved as a “.mov” file, kind of like how you save a video on your phone. But to understand the DNA story better, scientists need to do more things. They use special software called “SMRT Tools,” which PacBio made and shared with everyone. With this software, they can do different tasks, like taking off the starting and ending parts of the video (SMRT bell adapters), and pulling out the most important parts, which are like key scenes in the movie (Circular Consensus Reads / Subreads).\nOnce they’ve done all this, they can change the video into a different format, like turning a video into pictures or text. They do this to make it easier to work with. They can turn the video into a “.bam” file, which is like a fancy organizer for the pictures, or a “.fastq” file, which is like turning the video into words. All of this helps scientists understand DNA better and find out its secrets. You can learn more about this software and how it works on the PacBio website.\n\n\n2.3.2 HiFi Reads\n\n\n\n\n\nHow HiFi Sequencing Works\n\n\n\n\nThe newest thing from PacBio is called HiFi sequencing. It’s like reading a really long story in a special way. Imagine having a super-long book with chapters that are 20 pages long.\nHiFi sequencing reads these chapters many times and figures out the best version. It’s like asking different people to read the same chapter and then picking the one that’s most accurate. This helps us understand long sections of DNA really well."
  },
  {
    "objectID": "chapters/week2.html#oxford-nanopore",
    "href": "chapters/week2.html#oxford-nanopore",
    "title": "2  Genomic Sequencing and Databases",
    "section": "2.4 Oxford Nanopore",
    "text": "2.4 Oxford Nanopore\n\n\n\n\n\nHow Oxford Nanopore Sequencing Works\n\n\n\n\nOxford Nanopore technology is a different way to read DNA. Instead of taking pictures, it watches electric signals over time. Think of it like watching a graph that goes up and down. This graph is called a squiggle.\nThey save this squiggle in a special file format called fast5. This file not only keeps the squiggle but also the letters that the computer guessed from the squiggle. It’s like writing down both what the graph looks like and what the scientist thinks it means. With new software they’re making, they can even go back to the squiggle and try to guess the letters again to make sure they got them right. It’s like looking at the graph again and trying to understand it even better.\n\n2.4.1 Uses for Long DNA Reads\nUsing Oxford Nanopore technology for DNA reading has different strengths depending on how many times the DNA is read.\nWhen they read it a few times (i.e., low coverage), it’s like filling in the missing pieces of a jigsaw puzzle or adding more information to an already started story. They can use these extra details to make an existing puzzle more complete (gap-filling), make a short story longer and clearer (scaffolding a short read assembly), or combine different pieces from different puzzles into one big picture (hybrid assembly).\nBut when they read the DNA many, many times (high coverage), it’s like writing a whole new story from scratch. They don’t need any other clues because they have everything they need. This is called “De novo assembly,” where they put together a complete picture of the DNA just by using these long reads. It’s like creating a new jigsaw puzzle using only the pieces from one box."
  },
  {
    "objectID": "chapters/week2.html#ion-torrent",
    "href": "chapters/week2.html#ion-torrent",
    "title": "2  Genomic Sequencing and Databases",
    "section": "2.5 Ion Torrent",
    "text": "2.5 Ion Torrent\n\n\n\n\n\nA Graph Obtained from an Ion Torrent Machine\n\n\n\n\nIon Torrent sequencing is like a special tool that’s great for certain types of projects. When scientists need to read pieces of DNA that are a bit longer (around 400-600 letters), Ion Torrent is like a superhero.\nImagine reading a longer chapter of a book instead of just a few sentences. This is really helpful for projects like studying tiny living things called microbes that live in different places, like our bodies or the environment. These projects, called microbiome studies, benefit a lot from Ion Torrent because it helps scientists understand these tiny creatures in more detail."
  },
  {
    "objectID": "chapters/week2.html#bgiseq-and-mgiseq",
    "href": "chapters/week2.html#bgiseq-and-mgiseq",
    "title": "2  Genomic Sequencing and Databases",
    "section": "2.6 BGISEQ and MGISEQ",
    "text": "2.6 BGISEQ and MGISEQ\n\n\n\n\n\nNanoballs in BGISEQ and MGISEQ\n\n\n\n\nBGI, a genomics institute, created special machines for reading DNA. One of them is called BGISEQ-500, kind of like an older model. It reads two lines of DNA that are 100 letters long each, making a total of around 520 million letters. Then there’s a newer model, MGISEQ-2000. It can read two lines that are 200 letters each, for a total of about 1 trillion letters! These machines were made to compete with another popular DNA reader, Illumina. They were designed to be really affordable, making DNA reading cheaper for everyone. It’s like offering a lower-cost way to explore the secrets of genetics."
  },
  {
    "objectID": "chapters/week2.html#hi-c-chip-seq-10x-and-bisulfite-sequencing",
    "href": "chapters/week2.html#hi-c-chip-seq-10x-and-bisulfite-sequencing",
    "title": "2  Genomic Sequencing and Databases",
    "section": "2.7 Hi-C, ChiP-seq, 10x, and Bisulfite Sequencing",
    "text": "2.7 Hi-C, ChiP-seq, 10x, and Bisulfite Sequencing\nThere are various ways to investigate specific things about DNA, like its unique features. The main difference comes in how the DNA is prepared in the lab, but the actual reading part uses a common Illumina platform. Think of it like different ways to prepare a special dish using the same cooking equipment. No matter which method is used, they all provide a type of output file called “fastq,” which is like a document containing DNA information. However, the way this information is used can vary depending on the specific method employed.\n\n2.7.1 Hi-C\n\n\n\n\n\nHow Hi-C Sequencing Works\n\n\n\n\nHi-C sequencing is a technique that helps scientists understand how different parts of DNA are arranged in space. It’s like making a map of how different rooms in a house are connected.\n\n\n\n\n\nHi-C Data\n\n\n\n\nThey capture this information by studying how different parts of the DNA are close to each other. This technique gives them a picture of how far apart or near different parts of the DNA are along the entire chromosome. It’s like learning about the layout of a house by seeing which rooms are close to each other.\n\n\n2.7.2 10x Genomics\n\n\n\n\n\nHow 10x Genomics Works\n\n\n\n\nIn 10x Genomics technology, each individual piece of DNA is given a special code, like a secret badge. Think of it like giving every player in a game their own unique mark. This mark is created using tiny gel beads in a special mixture. It’s as if each player gets a distinct symbol, helping scientists keep track of different pieces of DNA while they’re doing their research.\n\n2.7.2.1 In Unicellular RNA Sequencing\n\n\n\n\n\n10x Genomics for RNA Sequencing\n\n\n\n\nWhen using 10x Genomics for single-cell RNA sequencing, there’s a twist in how it works. Instead of using big pieces of DNA, they focus on individual cells. Imagine each cell is like a small character in a story. They put one cell into each tiny gel bead, kind of like each character in their own small bubble. Inside these bubbles, the cell’s RNA, which is like its story, is turned into a special type of DNA called cDNA. This process is like translating the cell’s story into a new language. This way, scientists can study the stories of many individual cells all at once, and see how they’re different or similar.\n\n\n\n2.7.3 Bisulfite Sequencing\n\n\n\n\n\nHow Bisulfite Sequencing Works\n\n\n\n\nIn bisulfite sequencing, DNA is treated with a special chemical called bisulfite. This chemical changes some parts of the DNA. Imagine it’s like using a magic potion on a drawing. When applied to the DNA, bisulfite changes cytosine to uracil, but it doesn’t affect 5-methylcytosine. It’s like turning some parts of the drawing into a new color while leaving other parts the same. This helps scientists understand which parts of the DNA have certain molecules attached to them.\n\n\n2.7.4 Chromatin Immunoprecipitation\n\n\n\n\n\nHow ChIP Works\n\n\n\n\nChromatin immunoprecipitation (i.e., ChIP) is a method used to study how proteins interact with DNA. Think of it as a way to find out which proteins are hanging out with specific parts of DNA. Here’s how it works:\n\nFirst, scientists fix the proteins they’re interested in, like transcription factors, to the DNA using a special chemical called formaldehyde. Imagine it’s like gluing the proteins to certain parts of the DNA.\nThen they carefully take out the DNA and break it into smaller pieces, like breaking a long necklace into smaller beads.\nNext, they use special antibodies that act like magnets to pull out the proteins they’re studying. It’s like using a magnet to pick up certain toys from a pile.\nAfter that, they heat everything up to undo the gluing caused by formaldehyde. This step is like melting the glue and separating the proteins from the DNA.\n\nAs a result, they end up with DNA fragments that were connected to the proteins they were interested in. It’s like getting clues about which proteins were spending time with specific parts of the DNA. This helps scientists understand how different proteins control and interact with genes and DNA."
  },
  {
    "objectID": "chapters/week2.html#online-data-repositories-for-sequenced-data",
    "href": "chapters/week2.html#online-data-repositories-for-sequenced-data",
    "title": "2  Genomic Sequencing and Databases",
    "section": "2.8 Online Data Repositories for Sequenced Data",
    "text": "2.8 Online Data Repositories for Sequenced Data\n\n2.8.1 Sequence Read Archive (i.e., SRA)\nThe Sequence Read Archive (i.e., SRA) is like a big library where scientists from around the world store their DNA and RNA sequencing data. It’s kind of like a safe place for important information. This repository is taken care of by a group called NIH, in the USA.\nInside the SRA, you’ll find the raw data from sequencing, which is like the original puzzle pieces of DNA or RNA. Think of it as the untouched information from which scientists make discoveries. Both DNA and RNA data are kept here, like storing books of different kinds in the same library.\nMany scientific journals require researchers to share their sequencing data in the SRA. This is important because it allows other scientists to check their work and try things out for themselves. It’s like sharing a recipe so others can cook the same dish. Anyone can download this data for free, helping scientists all over the world learn from each other. However, more detailed information like complete genomes and detailed explanations are stored somewhere else. Sometimes scientists need to process the raw data a bit more to make sense of it, like cooking the raw ingredients into a delicious meal.\n\n\n2.8.2 NCBI Genomes\n\n\n\n\n\nNCBI Genomes\n\n\n\n\nThe NCBI Genomes database is like a huge book that holds a lot of important details about different species. It not only tells you what species a living thing belongs to, but it also lets you know if its DNA has been fully read and studied.\nInside this database, you’ll find information about the species’ classification, kind of like its scientific family tree. It’s like knowing which branch of the animal kingdom it belongs to. Additionally, you can find out if the species’ DNA has been completely read and studied in-depth.\nHowever, this database usually contains only the main or reference version of the species’ DNA, and it comes with annotations that tell you where different genes are located. These annotations are stored in a special kind of file called a “.gff” file, which is like a map showing where different treasures (genes) are hidden in the DNA. So, this database is like a treasure trove of genetic information about different species, helping scientists and researchers understand their DNA better.\n\n\n2.8.3 EBI: Ensembl Project\n\n\n\n\n\nEnsembl Project Homepage\n\n\n\n\nThe EBI’s Ensembl Project is like a special tool that helps scientists explore and understand the DNA of different living things. It’s kind of like a map for navigating the genetic information of various species.\nThis project offers a genome browser that allows researchers to access annotated genomes of species that belong to specific groups in the animal kingdom. There are different sections, like Ensemble Bacteria, Protists, Fungi, Plants, Metazoa, and Vertebrates. It’s like having different shelves in a library for different types of books.\nFor each species in Ensembl, you’ll find at least the main version of its DNA and information about its genes. Think of it as knowing the basic story of each species. But sometimes, there’s even more information available, like extra chapters in a book. This project is a helpful tool for scientists to study and learn more about the genetics of various living things."
  },
  {
    "objectID": "chapters/week3.html",
    "href": "chapters/week3.html",
    "title": "3  Data Preprocessing and Quality Control",
    "section": "",
    "text": "Today’s discussion covers several important aspects of sequencing data analysis. First, it’s crucial to be aware of potential sources of errors that can occur during the sequencing process. Next, we’ll delve into preprocessing, which involves getting the raw sequencing data ready for analysis. Quality control measures will help ensure that the data is accurate and reliable. We’ll also explore methods for detecting contamination in the sequencing data. To put these concepts into practice, we’ll walk through an example analysis using Drosera capensis as our model organism."
  },
  {
    "objectID": "chapters/week3.html#sources-of-error-in-sequencing",
    "href": "chapters/week3.html#sources-of-error-in-sequencing",
    "title": "3  Data Preprocessing and Quality Control",
    "section": "3.1 Sources of Error in Sequencing",
    "text": "3.1 Sources of Error in Sequencing\n\n“Limitations of the sequencing platforms, and particular artifacts associated with sequences generated on these platforms, need to be understood and dealt with at various stages of the project including planning, sample preparation, run processing and downstream analyses.”\n– Addressing Challenges in the Production and Analysis of Illumina Sequencing Data\n\nData preprocessing and quality control are the two most important things in data analysis.\n\n\n\n\n\nSome Possible Errors During Sequencing\n\n\n\n\nThere’s a phrase called “garbage in, garbage out” or GIGO for short. What this basically means is that if you start with bad data, you will produce bad analysis. Similarly, if you start with poor-quality ingredients, you will not make good food. Because of this, you need to ensure that your data is workable before you do anything else.\n\n3.1.1 Problems with Sampling\nThe sample itself can introduce errors.\nFor instance, if you don’t know what you’re sampling or got your samples mixed up, this could influence the final results.\nOr, if your sample is contaminated or degraded (i.e., not taken care of well enough), this can also affect the final quality of the data.\n\n3.1.1.1 Possible Sources of Contamination\nThere are a few possible ones:\n\nMicrobes such as bacteria, viruses, parasites, and whatnot.\nMaybe the sampler themselves could have dirty hands and introduced something into the sample without knowing it?\nResearchers mixing their samples.\nUsing dirty tools.\n\nThere are more sources here.\n\n\n3.1.1.2 DNA Fragmentation\nThe environment can have a significant impact on ancient DNA samples. This degradation process causes the DNA to break into small fragments and leads to specific changes, such as the conversion of cytosines to uracil.\nTo tackle these challenges, scientists use various techniques to enrich fragments with uracils in ancient DNA samples. By analyzing the patterns of DNA fragmentation, researchers can clean and correct data, using tools like mapDamage. These methods help us make sense of ancient DNA, even when it’s been affected by the passage of time and environmental factors.\n\n\n\n3.1.2 Problems with Creating a Genomic Library\nThere are a few:\n\nAdapter Dimers\n\n\n\n\n\nIllustration of an Adapter Dimer\n\n\n\n\nAdapter dimers can reduce the amount of useful data that’s generated by the sequencing machine. An adapter dimer is a small bit of DNA that is mistakenly sequenced instead of the target DNA, so this decreases the amount of useful data at the end of the day.\nChimeras\n\n\n\n\n\nIllustration of two Chimeras\n\n\n\n\nA chimera is a DNA sequence that is artificially made by joining two more DNA sequences. This happens when adapters mistakenly link unrelated DNA fragments (i.e., especially the SMRTbell templates above).\nBad Insert Size\n\n\n\n\n\nIllustration of a Bad Insert Size\n\n\n\n\nThis refers to a badly-spaced gap between the DNA fragments at the end (i.e., the primers and adapters). The insert size - the “gap” - needs to be the right size.\nIf the insert size is too big or too small, this can lead to issues down the road.\nSequencing Artefacts\n\n\n\n\n\nExamples of Sequencing Artefacts\n\n\n\n\nA sequencing artefact is a fancy term to refer to anything during the sequencing process that can affect the final quality of the data - for instance, air bubbles and dust particles. These artefacts can lead to repetitive patterns.\nA common kind of artefact that occurs is when two DNA fragments produces two distinct clusters, hence affecting data analysis.\nDuplicated Reads\n\n\n\n\n\nExamples of Duplicated Reads\n\n\n\n\nThis refers to multiple copies of the same DNA sequence, and this can happen because of numerous factors.\nDuplicated Reads\n\n\n\n\n\nPlot Illustrating GC Content\n\n\n\n\nThis kind of bias happens in Illumina sequencing as the technology favors DNA sequences with different amounts of “G” and “C” nucleotides. This can lead to the uneven coverage of the genome. Furthermore, sequencing technologies like PacBio and Illumina don’t do well with long sequences of repeating DNA sequences.\nAt least PacBio is less error prone to GC content biases or DNA length. But, it does have a higher error rate (i.e., 13% - 15%) compared to other technologies."
  },
  {
    "objectID": "chapters/week3.html#preprocessing-techniques",
    "href": "chapters/week3.html#preprocessing-techniques",
    "title": "3  Data Preprocessing and Quality Control",
    "section": "3.2 Preprocessing Techniques",
    "text": "3.2 Preprocessing Techniques\n\n\n\n\n\nGeneral Workflow for Bioinformatics Pipelines\n\n\n\n\nThe above figure show general steps when it comes to working with raw data (i.e., .fq files).\n\n3.2.1 Adapter Removal\n\n\n\n\n\nIllustration of Adapter Removal\n\n\n\n\nAdapter removal refers to removing adapter sequences - short pieces of DNA used during sequencing. This step ensures that the data is as clean as possible.\n\n\n\n\n\nExamples of Adapters\n\n\n\n\nAdapters have specific sequences; software is designed to recognize, locate, and remove these adapters from the data.\nThese software can also handle some errors within the data and ensure that the adapters are removed from the data."
  },
  {
    "objectID": "chapters/week3.html#quality-control-with-fast-qc",
    "href": "chapters/week3.html#quality-control-with-fast-qc",
    "title": "3  Data Preprocessing and Quality Control",
    "section": "3.3 Quality Control with Fast QC",
    "text": "3.3 Quality Control with Fast QC\n\n3.3.1 FastQC\nThere are numerous statistics (by section name) to take note of when using FastQC to quality control data:\n\nBasic Statistics\n\n\n\n\n\nBasic Information in a FastQC Report\n\n\n\n\nThis part lists basic information about a raw data file.\nPer Base Sequence Quality\n\n\n\n\n\nQuality Scores Boxplot\n\n\n\n\nGenerally speaking, the longer the read, the lower the phred quality score.\nIn the above graph, we see that most boxplots have a median score of above 30, but there are also many whose minimum score (i.e., the bottom whisker) is below 20.\nPer Tile Sequence Quality\n\n\n\n\n\nQuality Scores Heatmap\n\n\n\n\nA blue square on this plot means that everything went well. Otherwise, the yellow, green, and red squares hint that there’s some issue with the quality of the data at that exact spot in the raw data.\nThe Y-axis is the phred score, but scaled up by 1000.\nPer Sequence Quality Scores\n\n\n\n\n\nQuality Scores Heatmap\n\n\n\n\nA blue square on this plot means that everything went well. Otherwise, the yellow, green, and red squares hint that there’s some issue with the quality of the data at that exact spot in the raw data.\nThe Y-axis is the phred score, but scaled up by 1000.\nPer Sequence Quality Scores\n\n\n\n\n\nPer Sequence Quality Plot\n\n\n\n\nThe Y-axis refers to the amount of DNA sequences that have that same mean phred score on the X-axis. That peak in the above density plot is the mean phred score of all DNA sequences.\nPer Sequence Quality Scores\n\n\n\n\n\nPer Sequence GC Content\n\n\n\n\nThis plot shows the percentages of bases along DNA sequencing reads.\nPer Sequence Quality Scores\n\n\n\n\n\nPer Base Sequence Content\n\n\n\n\nThis plot shows the percentages of bases along DNA sequencing reads.\nPer Base Sequence N Content\n\n\n\n\n\nPer Base Sequence N Content\n\n\n\n\nPer Base Sequence N Content\n\n\n\n\n\nPer Base Sequence N Content\n\n\n\n\nPer Base Sequence N Content\n\n\n\n\n\nSequence Duplication Levels"
  },
  {
    "objectID": "chapters/week3.html#identifying-possible-contaminations",
    "href": "chapters/week3.html#identifying-possible-contaminations",
    "title": "3  Data Preprocessing and Quality Control",
    "section": "3.4 Identifying Possible Contaminations",
    "text": "3.4 Identifying Possible Contaminations\n\n\n\n\n\nA Bimodal Distribution in a Set of Raw Data’s GC Content Distribution\n\n\n\n\nThe bimodal distribution in the GC count read (i.e., that red density plot) is an indicator of possible contamination. One of the peaks could be indicative of something else.\n\n3.4.1 Using Databases to Identify Possibly Contaminated Data\nOne can match part of their data against reference databases - this is usually done with tools like BLAST or Diamond as they readily do this for you.\nWhat follows after is something called metagenomics analysis - the goal of this kind of analysis is to determine the origin of the DNA sequences. The National Center for Biotechnology Information (i.e., NCBI) has a lot of DNA sequences and each sequence belongs to a species; there’s also a taxonomy tree that organizes these species into families, genuses, and species.\n\n3.4.1.1 How Does it Work?\nWhen comparing sequences against reference ones, we look for matches - these are called hits. Each DNA sequence or read that we compare is tied to a specific organism.\nTo classify these reads, we identify something called the lowest common ancestor (i.e, LCA) in the species tree for each read. What this basically means is that we find the most specific taxonomic level where all the species that our read matches have similar DNA. Because of this, each read that we compare has a taxonomic classification as well.\nTools such as MEGAN can help automate this process.\n\n\n\n\n\nExample Approach to Identify Contaminated Sequences\n\n\n\n\nThe species that are closer (at least from an evolutionary viewpoint) tend to get more hits in this example. If the species that was sequenced isn’t in the NCBI, then the BLAST that was run on other species would contain the sequence.\n\n\n\n3.4.2 Using K-mers to Identify Contamination\n\n\n\n\n\nExample on How to Use K-Mers to Identify Outliers\n\n\n\n\nA k-mer is a fixed DNA fragment of length “k”. The above graphic shows how one can use K-mers to identify contaminated samples:\n\nGiven a the genome (from NCBI) of a particular species, split it into k-mers and then find the LCA of that k-mer.\nWe’ll also split our read into the same k-mers.\nWe find hits between the k-mers from our hits and the k-mers from the species’ genomes. We can then apply some sort of voting system to classify the read."
  },
  {
    "objectID": "chapters/week3.html#is-there-enough-data",
    "href": "chapters/week3.html#is-there-enough-data",
    "title": "3  Data Preprocessing and Quality Control",
    "section": "3.5 Is There Enough Data?",
    "text": "3.5 Is There Enough Data?\nIn order to do something called de novo assembly, we need to know if there’s enough data first. We can do this through flow-cytometry based size estimates to get a rough idea of a genome’s size.\nBut, one thing to keep in mind is that different individuals in the species may have different genetic makeups and also different genome sizes (e.g., chromosomal abnormalities).\nA polyploid is an organism that has multiple sets of chromosomes, and they are more common than we may think.\n\n3.5.1 K-mer Histogram\n\n\n\n\n\nExample of a K-mer Histogram\n\n\n\n\nThe K-mers that we get from our reads can be made into a histogram. We basically just count how many times each k-mer appears in the read and make it into a histogram.\n\n\n\n\n\nDetermining Genome Size from a K-mer Histogram\n\n\n\n\nIf the k-mers that are used to make the histogram are long enough, the mode - the peak with the highest - is the most abundant k-mer in the data.\nWe also use the k-mer histogram’s mode to estimate of the coverage: the average amount of times that a base in the genome is sequenced.\nThen, we divide the total amount k-mers that have been sequenced by the said mode - this should give an estimate of the genome size.\n\n\n\n\n\nA K-mer Histogram with no Mode\n\n\n\n\nIf a K-mer histogram doesn’t have any modes (like the one above), this means that the coverage isn’t high enough. So, we need to add in more data; this is because the genome is too big.\n\n\n3.5.2 Estimating the “K” in K-mers\nIf the K-mer is pretty short - say, \\(K = 4\\), then the possible K-mers that could appear will appear. In this case, there are \\(4^4 = 256\\) different combinations of 4-mers. Though, one thing to be wary of is that short K-mers don’t have uniqueness because they can occur in so many places in the data.\nThe last point is in contrast to long K-mers - the longer the K-mer, the lesser the amount of K-mers possible for that amount of \\(K\\).\n\n\n\n\n\nHypothetical Versus Observed NUmber of K-Mers\n\n\n\n\nBecause of the above points, the number of unique K-mers in our data should make a sort of parabola, and that peak in the middle is our optimal \\(K\\) value.\n\n\n3.5.3 Kmergenie\n\n\n\n\n\nExample Output of Kmergenie\n\n\n\n\nKmergenie is a kind of software that helps people estimate the genome size of their data using the k-mer analysis (i.e., the points said before this). Though, some software like MaSuRCA require the user to specify a “k” value beforehand.\nThere is also another software called jellyfish that also does the same thing as kmergenie - it helps find the optimal value of \\(K\\) in the K-mer histogram.\nAnd then there’s also another piece of software called SPAdes - this uses multiple K-mer values and usually gives good results. SPAdes pretty much gives the user the option to choose from several optimal values of \\(K\\) given their goals."
  },
  {
    "objectID": "chapters/week3.html#data-driven-quality-control",
    "href": "chapters/week3.html#data-driven-quality-control",
    "title": "3  Data Preprocessing and Quality Control",
    "section": "3.6 Data-Driven Quality Control",
    "text": "3.6 Data-Driven Quality Control\nWe need to conduct a check on the data to assess its quality - this sort of checking can highlight any issues with a specific library or a dataset (though, the common errors may include stuff like contaminated adaptors, low-quality data, and sequencing errors).\nThat said, there are usually two parts to this “checking”:\n\nTrim Reads\nWe need to “trim” or remove any low-quality data or bases from the data. This helps improve the quality of the data (obviously).\nMapping to Reference Genome\nThis allows us to understand how well our data maps to the expected results.\n\nThen, once all of this is done, we need to find any outliers using Data Science techniques. These anomalies could mean that there’s a sequencing problem, sample contamination, or even some other issue that needs to be looked into.\n\n3.6.1 Making a New Genome\n\n\n\n\n\nExample Genome Assembled\n\n\n\n\nIf we make a new genome, we usually do something called a quick draft assembly on it. This can let us know stuff like coverage and insert size distribution and can be helpful down the road.\nThat said, there are two terms that are must-knows:\n\nContigs\nThis just refers to a contiguous DNA sequence that doesn’t have any gaps or unsequenced regions. These “contigs” represent continuous segments of the genome and are usually obtained in the early stages of the assembly process.\nScaffold\nThis is a long structure that you get from joining multiple contigs together. Here, contigs are joined using “N”s - unsequenced bases. Scaffolds help link contigs and are a more comprehensive representation of the genome.\n\n\n\n3.6.2 GC and Average Coverage Plot\nDifferent species have varying genome lengths and GC percentages. These characteristics are unique to each species and can significantly impact analysis.\nDuring the sequencing process, reads are randomly sampled from the genomes being studied. This random sampling can lead to one of two thigns:\n\nSpecies with long genomes tend to have lower average coverage because there are more DNA base pairs to cover with the same number of sequenced reads.\nSpecies with shorter genomes tend to have higher average coverage with the same sequencing depth.\n\nThat said, the GC content of a genome can vary based on various factors, including the species’ lifestyle, environmental niche, and energy source. Different species may have distinct GC percentages."
  },
  {
    "objectID": "chapters/week4.html",
    "href": "chapters/week4.html",
    "title": "4  De Novo Sequencing",
    "section": "",
    "text": "General Bioinformatics Workflow\nTools such as MaSurCA (which will be used during the practical) are used to make genomes from raw data after the data has had its primers (from earlier Illumina sequencing) removed. Then, we use tools like quast to perform quality control on it.\nThat said, the main point of an assembly is shown in the picture above. Given a ton of overlapping DNA sequences, we want to try to reconstruct the actual genome - like putting together puzzle pieces to make a puzzle1."
  },
  {
    "objectID": "chapters/week4.html#laws-of-assembly",
    "href": "chapters/week4.html#laws-of-assembly",
    "title": "4  De Novo Sequencing",
    "section": "4.1 Laws of Assembly",
    "text": "4.1 Laws of Assembly\nThere are two main laws:\n\nFirst Law\n\n\n\n\n\nFirst Law of Genome Assembly\n\n\n\n\nLet’s say that we have two different reads (i.e., sequences) in our raw data: “A” and “B”. If the last few letters of “A” are similar to the beginning letters of “B”, then we can guess that “A” and “B” probably overlap with one another in the actual genome.\nSecond Law\n\n\n\n\n\nSecond Law of Genome Assembly\n\n\n\n\nLet’s say that we have a whole bunch of reads in our raw data. If these reads have a higher coverage (i.e., they’re read many times), then we will have more overlaps in our final genome.\nThird Law\n\n\n\n\n\nThird Law of Genome Assembly\n\n\n\n\nHow long our reads are and how repetitive those patterns are in the genome can make genome assembly challenging.\nSo, the best thing to do in this scenario is to just get a long read that cover(s) the entire repetitive sequence(s).\n\n\n\n\n\nOutcomes of Different Resolving Approaches\n\n\n\n\nDepending on how we choose to go about this issue, we can get different results. Because of this, we tend to just handle repeats that cannot be resolved by leaving them out altogether. But, we must keep in mind that if we do do this, then our assembly will be in fragments or contigs for short.\n\nHence, in de novo assembly, our ultimate goal is to really just find overlapping, short reads and put them into longer, continuous DNA sequences (i.e., like piecing together a puzzle from scratch). We use two different approaches for the most part:\n\nShortest Common Substring (i.e., SCS)\nOverlap Layout Concensus\nde Brujin Graphs\n\nBut regardless of which of the above that we end up doing in the end, one thing’s for certain: we use graph theory for all of these methods!"
  },
  {
    "objectID": "chapters/week4.html#overlap-graphs",
    "href": "chapters/week4.html#overlap-graphs",
    "title": "4  De Novo Sequencing",
    "section": "4.2 Overlap Graphs",
    "text": "4.2 Overlap Graphs\nThere are two main terminologies to know when it comes to graphs.\n\nNode\nThis just means a “read” or a “k-mer” in the raw data.\nEdge\nThis is a line connecting two nodes. If this line is an “arrow”, then we say that this edge is directed.\n\n\n\n\n\n\nExample of an Overlap Graph\n\n\n\n\nIn the above graph (i.e., an example), the graph represents the 6-mers of the DNA sequence: GTACGTACGAT. For simplicity’s sake, we only draw directed edges (i.e., the arrows) between nodes (i.e., the k-mers); the weight or number above the edge shows how many letters each k-mer overlaps with the other.\n\n4.2.1 Hamiltonian Paths\n\n\n\n\n\nExample of a Hamiltonian Path in a 6-mer Graph\n\n\n\n\nWe say that a graph is Hamiltonian if and only if we can visit all of its nodes at least once. In this same example, if we try to overlap the different K-mers to form the sequence GTACGTACGAT, we see the path that this sequence takes in the graph visits the 6-mers at least once!"
  },
  {
    "objectID": "chapters/week4.html#shortest-common-substring-i.e.-scs",
    "href": "chapters/week4.html#shortest-common-substring-i.e.-scs",
    "title": "4  De Novo Sequencing",
    "section": "4.3 Shortest Common Substring (i.e., SCS)",
    "text": "4.3 Shortest Common Substring (i.e., SCS)\n\n\n\n\n\nIllustration of the Shortest Common Substring Problem\n\n\n\n\nHere’s how this works in a nutshell:\n\nWe are given a whole bunch of K-mers.\nWe then smoosh all of these k-mers together to form one big, long string.\nOur job is to then find the shortest substring of that “big, long string” that contains the original k-mers as substrings. This “shortest substring” is called the shortest common substring (i.e., SCS).\n\nProf. Jarkko mentions that finding the SCS is NP complete: what this basically means is that the longer the “big, long string”, the more exponential the time to find the SCS becomes (hence making it all the more challenging).\n\n4.3.1 Greedy SCS Algorithm\n\n\n\n\n\nIllustration of the Greedy Shortest Common Substring Problem\n\n\n\n\nIf an algorithm is greedy, this basically means that it aims to maximize something about it.\n\n\n\n\n\nSecond Illustration of the Greedy Shortest Common Substring Problem\n\n\n\n\nIn this greedy SCS problem, we first start with the overlap graphs for the k-mers. Then, we combine the k-mers based on the weights of their edges until we get the substring (taking to get rid of the overlaps in the process).\n\n\n\n\n\nIllustration of the Greedy SCS Gone Awry\n\n\n\n\nHowever, just because an algorithm is greedy doesn’t always mean that it will lead to the best (i.e., optimal) outcome. If we look at the 6-mers of the overlap graph of the string a_long_long_long_time and try to do the greedy SCS problem for it, we see that we just end up with a_long_long_time. This isn’t the same string!\nInterestingly enough, if we use k-mers with \\(k = 8\\), we see that the final string that’s formed is actually correct."
  },
  {
    "objectID": "chapters/week4.html#overlap-layout-consensus-i.e.-olc",
    "href": "chapters/week4.html#overlap-layout-consensus-i.e.-olc",
    "title": "4  De Novo Sequencing",
    "section": "4.4 Overlap Layout Consensus (i.e., OLC)",
    "text": "4.4 Overlap Layout Consensus (i.e., OLC)\n\n\n\n\n\nIllustration of the OLC Process\n\n\n\n\nThe above graphic summarizes the steps involved in OLC.\n\n4.4.1 Overlap\n\n\n\n\n\nIllustration of an Overlap\n\n\n\n\nAn overlap refers to the ending of a string having at least a certain amount of matches with the beginning of another string. In the above example, we have two strings “X” and “Y”.\nIf we say that we want at least three matches, we start from the beginning of “Y” and see that the sequence “TAG” is found in the middle of “X” - the same could more or less be said for “GCC” too.\n\n4.4.1.1 Layout of the Overlap Graph\n\n\n\n\n\nIllustration of the Overlap Graph for 7-Mers of to_every_thing_turn_turn_turn_there_is_a_season\n\n\n\n\nThe above layout graph is super unruly and needs to be simplified.\n\n\n\n\n\nSimplifying the Overlap Graph\n\n\n\n\nSo, the first thing we need to do is to just simplify this graph. If we take a closer look at the original graph, we see that some edges can be inferred from other edges - in this case, the green arrows can be inferred from the blue ones.\n\n\n\n\n\nFurther Simplification of the Overlap Graph\n\n\n\n\nWe then remove these green edges - first, the ones that skip a node, and then next, the ones that skip two or more nodes.\n\n\n\n\n\nUnresolvable Repeats\n\n\n\n\nHowever, we see that the stuff in the red box cannot be resolved further.\n\n\n\n\n\nHaploid Assembly of a Genome\n\n\n\n\nSo, what scientists do is to get the contigs, line them up, and take the majority vote of the sequences. So, for example, if one nucleotide has mostly “A”s in a spot, then scientists assume that that particular nucleotide in space is an “A”.\n\n\n4.4.1.2 Caution\nThis entire process (i.e., the one in the preceding sub-sub-subsection) is super slow, especially when you realize that many sequencing datasets have hundreds of millions or billions of reads.\nAnd even if you could make the overlap graph, the graph would be ginormous!"
  },
  {
    "objectID": "chapters/week4.html#de-brujin-graphs",
    "href": "chapters/week4.html#de-brujin-graphs",
    "title": "4  De Novo Sequencing",
    "section": "4.5 De Brujin Graphs",
    "text": "4.5 De Brujin Graphs\n\n\n\n\n\nDe Brujin Graph for the String tomorrow and tomorrow and tomorrow\n\n\n\n\nThe meaning of an edge hasn’t changed here, but there are new temrs to know:\n\nMultigraph\nThis is a graph that has more than one node coming into a node.\nIndegree\nThis refers to the amount of arrows (i.e., directed edges) coming into a node.\nOutdegree\nThis refers to the amount of arrows (i.e., directed edges) coming out of a node.\nBalanced nodes\nThe indegree and the outdegree of a node are the same.\nSemi-balanced node\nThe indegree differs by the outdegree of the node by one.\nConnected graph\nThis just means that all nodes in a graph can be reached by some other node.\n\nAs a super short trick, if a directed graph (i.e., graph with arrows for edges) has two semi-balanced nodes at the very most, then that graph is Eulerian.\n\n\n\n\n\nDe Brujin Graph for AAABBBBA\n\n\n\n\nFirst, we split the genome AAABBBBA into 3-mers before splitting the 3-mers into 2-mers.\n\n4.5.1 Eulerian Walks\n\n\n\n\n\nEulerian Walk in the De Brujin Graph for AAABBBBA\n\n\n\n\nWe say that a graph has a Eulerian walk if and only if each edge can be crossed exactly once.\nGenerally speaking, we can find a Eulerian walk in linear time - meaning that the mode edges we have in a graph, the tougher and longer it is to find an Eulerian walk (obviously).\n\n\n4.5.2 Making a De Brujin Graph for a Genome\nFirst things first, we need to assume that each K-mer doesn’t have any errors and is only sequenced once.\n\n\n\n\n\nSplitting the String a_long_long_long_time into 5-Mers and then 4-Mers\n\n\n\n\nIn the above example, we make a de Brujin graph with the string a_long_long_long_time using 5-mers. We then begin by splitting each 5-mer into two pairs of of \\(k - 1\\)-mers - in this case, 4-mers.\n\n\n\n\n\nFinal Results of the String a_long_long_long_time into 5-Mers and then 4-Mers\n\n\n\n\nIn the end, our graph looks something like the one shown above. Also note that the finished graph is Eulerian as it only has two semi-balanced nodes at best.\n\n\n4.5.3 Problems with De Brujin Graphs\n\n\n\n\n\nPossible Outcomes of an Ambiguous De Brujin Graph\n\n\n\n\nIf there is a repeat in a sequence, this can cause issues. Here, we see that there are two equally-likely outcomes.\n\n\n\n\n\nErrors Affecting the Outcome of a De Brujin Graph\n\n\n\n\nAs in, errors like the following can make the graph not appear good:\n\nGaps in coverage (i.e., missing k-mers)\nCoverage differences\nErrors and differences between chromosomes."
  },
  {
    "objectID": "chapters/week4.html#error-correction",
    "href": "chapters/week4.html#error-correction",
    "title": "4  De Novo Sequencing",
    "section": "4.6 Error Correction",
    "text": "4.6 Error Correction\n\n\n\n\n\nMapping Errors to a K-mer Histogram\n\n\n\n\nIf we look at the K-mer histogram, we see that frequent k-mers tend to turn into infrequent ones.\nSo, here’s what we can do…\n\n\n\n\n\nCorrecting Errors in a K-mer Histogram\n\n\n\n\nFor each k-mer in each read, we do the following:\n\nIf the k-mer count is less than some value, we look at the k-mer’s neighbors within some distance.\nIf the said neighbors have counts more than that “some value”, we replace the k-mer in question with the neighbor."
  },
  {
    "objectID": "chapters/week4.html#how-much-data-for-a-de-novo-assembly",
    "href": "chapters/week4.html#how-much-data-for-a-de-novo-assembly",
    "title": "4  De Novo Sequencing",
    "section": "4.7 How Much Data for a De Novo Assembly?",
    "text": "4.7 How Much Data for a De Novo Assembly?\n\n\n\n\n\nTheoretical Analyses of Gene Lengths for De Novo Assembly\n\n\n\n\nThe problem is that the entire assembly is probably longer than the repeats in the genome. We could sequence and also re-sequence a bacterial genome with read lengths of 20 to 30 base pairs or nucleotides.\nHowever, with longer genomes, significant proportions of the genome are uncovered."
  },
  {
    "objectID": "chapters/week5.html",
    "href": "chapters/week5.html",
    "title": "5  Annotation",
    "section": "",
    "text": "Genome evolution, which is how the genetic information in living things changes over time, happens in two main ways. These ways are like two different strategies that genes use to change and adapt.\nThe first way is called whole genome multiplication. Imagine if you had a favorite book, and you made a copy of the entire book. Then, you made another copy, so you had three identical books. In the same way, sometimes in genomes, the whole set of genes gets duplicated or even triplicated. This means there are extra copies of all the genes.\nThe second way is small scale duplication, and it’s a bit like making copies of individual chapters or pages from your favorite book. There are two types of small scale duplication: “segmental duplication” and “tandem duplication.”\nSegmental duplication is when certain sections of the genome, like chapters in a book, are copied more than once. This can lead to having extra copies of specific genes, which can be useful for evolution.\nTandem duplication is a little different. It’s like having two or more paragraphs on the same page that are exactly the same. In this case, genes are duplicated one right after the other in a row."
  },
  {
    "objectID": "chapters/week5.html#whole-genome-duplications",
    "href": "chapters/week5.html#whole-genome-duplications",
    "title": "5  Annotation",
    "section": "5.1 Whole Genome Duplications",
    "text": "5.1 Whole Genome Duplications\n\n\n\n\n\nIllustration of a Whole Genome Duplication Event\n\n\n\n\nIn the above example, we have a genome from two different organisms - one blue and one orange.\nIf the genomes of two organisms come together, this is called an autopolyploid. Otherwise, this is called an alloalyploid.\nWhen genome evolution happens through whole genome duplications, it’s like making an identical copy of the entire set of genes. Imagine you have a long list of items, and you make an exact duplicate of that list. Here’s what happens:\nRight after the duplication, the genes in both copies are in the same order, which scientists call “synteny.” It’s like having the same items in the same order on both lists.\nBut, as time goes by, some changes occur:\n\nFractionation\nThis is when the genome starts to change. Some genes get lost because they become redundant (like having two identical items on your list), and random mutations can also affect genes.\nGenome rearrangements\nThink of this as reshuffling the items on your list. The order of genes can change.\nSyntenic blocks\nEven after a long time, there are still some parts of the genome where a bunch of genes look very similar to another bunch. It’s like having sections of your list that are almost the same as sections in the duplicate list.\nDiploid form\nEventually, the genome organizes itself back into a diploid form. This means it goes back to having two sets of genes, like you have one original list and one duplicate list, but with some changes.\n\n\n5.1.1 Genome Duplication in Organisms\n\n5.1.1.1 Plants\n\n\n\n\n\nWhole Genome Duplication in Plants\n\n\n\n\nGenome duplications happen quite often in the evolution of plants. It’s like when you have a favorite plant and it suddenly makes extra copies of all its genetic instructions. Here are some examples:\nAncient Hexaploid Event in Eudicots: About 150 million years ago, a group of plants called Eudicots had a special event where their entire set of genes got duplicated not just once, but three times! It's like having three identical instruction manuals for a plant.\n\nGrape\nAfter that ancient event, some plants like grapes didn’t have any more big duplications. So, they just kept the three copies they had and didn’t make any new ones.\nPoplar\nOn the other hand, poplar, which is another type of plant, recently had a whole genome duplication event. This means it made a fresh copy of all its genes, so it now has two sets of instructions.\nArabidopsis\nArabidopsis, yet another type of plant, had two whole genome duplication events in its history. So, it’s like having three instruction manuals originally, then copying them twice more, for a total of six!\n\n\n\n5.1.1.2 Fish\n\n\n\n\n\nWhole Genome Duplication in Fish\n\n\n\n\nIn the world of minnows and carps, which belong to a group called Cyprininae, there are around 1,300 different species. Now, here’s something interesting: approximately 400 of these species are what we call “polyploid.”\nPolyploid is a special term that tells us these species have more than the usual set of genetic instructions. It’s like having extra copies of a recipe book. In the case of these minnows and carps, having extra genetic copies can sometimes be a helpful adaptation. It’s a bit like having extra tools in your toolbox—they can be useful for different situations.\n\n\n\n5.1.2 Comparing Genomes\n\n\n\n\n\nComparing Two Genomes Using a Dot Plot\n\n\n\n\nA dot plot is like a cool tool scientists use to compare different genomes, which are like the instruction manuals for living things. Here’s how it works:\nFirst, imagine you have a bunch of genes from two different organisms, and you want to see how similar they are. The dot plot helps you visualize this by showing dots where genes are similar.\nAt first, it looks a bit messy with lots of dots. But, at a basic level, it helps us find pairs of genes that are kind of like neighbors in the instruction manuals. These are called syntenic gene pairs, and they might be important for understanding how organisms are related.\nIf we want to get even more detailed, we can study these syntenic gene pairs more closely. It’s like zooming in on those neighbor genes to see exactly how they match up.\nOne way to find these syntenic blocks, which are groups of similar genes, is to use a special program called DAGchainer. It’s like a detective that helps us find these important blocks in the instruction manuals.\nDAGchainer uses a smart method called dynamic programming to do this. Think of it like a super organized way to solve puzzles and find important patterns in the genes. So, dot plots and programs like DAGchainer are tools scientists use to uncover the secrets hidden in genomes!\n\n\n5.1.3 Exploring Whole Genome Duplications with Synteny\n\n\n\n\n\nSynteny Plots\n\n\n\n\nImagine you have two sets of genes from different organisms, and you want to figure out when they had a big duplication event. Synteny is like a tool that helps us with this detective work.\nFirst, scientists do something called pairwise alignment, which is like comparing the two sets of genes to see which ones match up. These matching gene pairs are called syntenic gene pairs.\nNow, here’s the cool part: we can look at how many “synonymous substitutions” have happened in these gene pairs. Synonymous substitutions are like small changes in the genes that don’t really affect how the protein is made or its building blocks (amino acids).\nBy counting these mutations, it’s kind of like using a molecular clock. This clock tells us how much time has passed since the big gene duplication happened. Just like a clock helps us keep track of time, counting these mutations helps scientists figure out when the duplication event took place.\n\n\n5.1.4 Functional Biases on Gene Origins\n\n\n\n\n\nProf. Jarkko’s Graph\n\n\n\n\nWhen we talk about how genes work and where they come from, there are two main ways genes can be duplicated: Whole Genome Duplication (WGD) and Tandem Duplication. These two processes can affect how genes function.\nIn the case of Whole Genome Duplication (WGD), there’s something called the dosage-balance hypothesis. This means that genes that are really important and highly connected in the cell are usually kept around when gene duplication happens. Think of it like keeping the most important players on your sports team. These important genes often include things like transcription factors and regulators, which are like the coaches and referees of the cell.\nOn the other hand, with Tandem Duplication, it’s more about recent adaptations. Imagine if you have a group of friends, and you suddenly need to deal with a new challenge, like a surprise test. You might quickly form a study group with the friends who are best at that subject. Similarly, genes that are duplicated in tandem, one after the other, often include genes that help an organism cope with immediate changes in its environment. For example, if there’s a sudden threat from a pathogen, genes related to defense might get duplicated to help the organism adapt quickly.\n\n5.1.4.1 Why is this Important?\nAfter a special kind of gene duplication called Whole Genome Duplications (WGDs), the genes that stay around are usually super important. They’re like the VIPs of the cell—genes that help control things like when to grow, when to stop, and how to respond to signals. We call this idea the “dosage-balance hypothesis.”\nThe cool thing is that similar VIP genes can be found in almost all species, and they work in pretty similar ways. It’s like finding the same important tools in everyone’s toolbox. This makes it easier for scientists to find similar genes in different species, and we call these similar genes “orthologs.”\nNow, when genes are copied one after the other, like in Tandem Duplication, they often help an organism deal with changes in its environment. These are like the genes that help you adapt when something unexpected happens, like a sudden change in weather. These newly copied genes are very similar in their instructions.\nBut here’s where it gets tricky: these copied genes can be a bit different between individuals of the same species, and this can cause problems when scientists are trying to match up short pieces of genetic information (short reads) with the gene instructions.\n\n\n\n5.1.5 Implications for Genome Annotation\nFirst, think about the number of gene copies. It’s like saying that in some species, there might be more copies of a certain gene, while in others, there are fewer. It’s kind of like how some people have more toys than others. This variation in gene copies can make things a bit complicated.\nNow, when it comes to species, those that are more closely related, like siblings, tend to be more similar to each other in terms of their genes. But species that are very different, like distant cousins, might have genes that look quite different. It’s a bit like how you might look more like your brother or sister than a cousin you’ve never met.\nPolyploidy events are like times when a species had too many copies of its entire set of genes. Imagine if you suddenly had two toy chests filled with toys instead of one. It can be a bit tricky to figure out which toys are exactly the same in both chests, and this is kind of like the challenge scientists face when finding orthologs in species that have experienced polyploidy.\nLastly, with tandem duplications, it’s like having the same toy repeated in a row in your toy chest. When this happens with genes, it can be hard to predict because it looks like there’s just one type of toy in your chest, even though there are actually many copies of the same toy. So, for scientists trying to understand gene models, it can be a bit tricky to tell what’s going on."
  },
  {
    "objectID": "chapters/week5.html#quality-control-of-assembly",
    "href": "chapters/week5.html#quality-control-of-assembly",
    "title": "5  Annotation",
    "section": "5.2 Quality Control of Assembly",
    "text": "5.2 Quality Control of Assembly\n\n5.2.1 N50 Value\nWhen scientists are putting together the puzzle pieces of a genome, one of the most important things to check is how well those pieces fit together. This is called contiguity.\nContiguity is like looking at a jigsaw puzzle and making sure all the pieces are lined up nicely. You want to see how many separate pieces, called contigs or scaffolds, make up the genome.\n\n\n\n\n\nIllustration of the N50 Value\n\n\n\n\nNow, to tell how good the assembly is, we use a special number called the N50 value. This number tells us something really important: it shows the length where 50% of the genome assembly is made up of contigs or scaffolds longer than this length.\nThink of it like this: if you have a bunch of puzzle pieces, and you find the N50 value, it means that half of the puzzle pieces are at least that big. So, the higher the N50 value, the better the quality of the genome assembly because it means larger and more complete pieces are used to put together the genome.\n\n\n5.2.2 Conserved Single-Copy Genes\nWhen scientists are working on putting together a genome, one of the most crucial things they want to make sure of is that they capture all the important parts of the genes. Imagine if you were building a car, and you needed to make sure you had all the essential parts like the engine, wheels, and brakes. In genome assembly, those important parts are the genes.\nNow, here’s the tricky part: figuring out if you’ve got all the genes in their proper places can be hard without some extra help. It’s like trying to build a car without an instruction manual.\nBut scientists have a clever trick. They’ve looked at lots of different species and found a special group of genes that are found in nearly all genomes as single copies. This means there’s usually just one of each of these genes in the genome. It’s like finding common tools that are used in many different types of cars.\nThese single-copy genes are super important because they’re often the first ones to disappear if there are extra copies or if genes get shuffled around during evolution.\nTo help scientists check if they’ve got these essential genes in their genome assembly, they use software like CEGMA (which is a bit outdated now) and BUSCO. These programs act like detectives to see if all the crucial genes are present and in good shape, kind of like making sure you have all the essential car parts before you start driving.\n\n\n5.2.3 BUSCO\nImagine you have a big jigsaw puzzle, and you want to make sure you’re putting all the right pieces in the right places. In the world of genomes, those puzzle pieces are genes, and they need to be correctly identified and placed. This is where Benchmarking Universal Single-Copy Orthologs (i.e., BUSCO) comes in.\nBUSCO helps with genome annotation, which is like labeling and understanding the genes in a genome. It does this using a two-step approach:\nFirst, it uses something called tblastn to search for known single-copy protein sequences within the genome. Think of this as looking for specific shapes in your puzzle pieces that you know should be there.\nOnce it finds these matches, it uses them to teach a special tool called “Augustus” how to predict where other genes are in the genome. It’s like showing someone a few pieces of the puzzle so they can guess where the rest of the pieces should go.\nThen, Augustus starts predicting the genes and looks for matches to a set of genes that are known to be present as single copies in most genomes. These genes are kind of like the most important pieces of the puzzle.\nBUSCO gives a report that tells you the number of found full-length genes (genes that are complete and correct), duplicated full-length genes (genes that have extra copies), fractionated genes (genes that are split up), and missing genes (genes that are nowhere to be found).\nThe “Busco score” is a handy percentage that tells you how well your genome assembly is doing. If it’s over 95%, that’s really good and means you have a high-quality assembly. If it’s between 90-95%, that’s still good. But if it’s below 80%, it’s a sign that there might be issues with your genome assembly.\n\n5.2.3.1 Pros and Cons\nUsing BUSCO is like taking a quick test to check how well you’ve done a big job, like building a complex model. It’s become a standard method in genomics because it helps scientists figure out if they did a good job in understanding the genes of an organism quickly.\nNow, there are a few things to keep in mind when using BUSCO:\n\nNot the Whole Picture\nThe results from BUSCO might not tell you everything about the quality of the entire set of genes in the organism. It’s like if you looked at only a few parts of a car and assumed the whole car was in perfect shape.\nPredicting Easy Genes\nBUSCO focuses on finding genes that are considered “easy” to predict because they’re pretty much the same across many species. This means it might miss some of the more unique or hard-to-predict genes.\nReference Species\nBUSCO defines these “easy” genes based on a set of species used as a reference. However, not all organisms have the exact same genes, and in some cases, the genes might be so different that BUSCO can’t spot them.\nDuplicate Genes\nSometimes BUSCO might find extra copies of genes. This can happen because of problems with how the genome was put together (assembly problems) or because the organism naturally has extra copies due to recent genome duplications."
  },
  {
    "objectID": "chapters/week5.html#genome-annotation",
    "href": "chapters/week5.html#genome-annotation",
    "title": "5  Annotation",
    "section": "5.3 Genome Annotation",
    "text": "5.3 Genome Annotation\nLet’s break down three types of annotation tasks:\n\nGenome Annotation\nImagine you have a huge book with lots of words but no titles or chapters. Genome annotation is like giving titles to chapters in this book. Scientists try to figure out where the important genes are hiding in the DNA sequence of an organism’s genome. It’s like identifying the main characters in a story.\nStructural Annotation\nThis is about looking closer at the genes you’ve identified. It’s like studying the characters in a book and finding out if they have any special traits or abilities. In genetics, scientists try to spot specific patterns or “domains” in genes that are similar across different species. These domains are like superpowers for genes, and they help scientists understand what the genes do.\nFunctional Annotation\nOnce you know what the genes look like and what patterns they have, you want to figure out what they actually do. It’s like reading the book to learn about the characters’ roles. Scientists try to guess the function of a gene based on its patterns and shapes. They do this by finding genes in other species that are very similar in structure or sequence (like finding characters in other books that are a lot like the ones you already know). Then, they assume that these similar genes have similar functions. To be really sure, they might do experiments, like RNA sequencing or mutant tests, to check if their guess is right.\n\nSo, in a nutshell, genome annotation1 is like naming characters in a book (identifying genes), structural annotation is like discovering special traits of these characters (identifying patterns in genes), and functional annotation is like figuring out what these characters do in the story (identifying gene functions). It’s all about understanding the genetic story of an organism!\n\n5.3.1 Coding Regions\n\n\n\n\n\nIllustration of Introns and Exons\n\n\n\n\nWhen scientists study a genome, they’re like detectives trying to find important clues. They want to identify the functional parts of the genome, which are like the chapters and important details in a story. These functional parts include things like the promoter (which tells genes when to start), exons (which are the important coding parts of genes), and introns (which are like extra, non-coding sections within genes).\nNow, introns are like tricky puzzles in this genome story. They make the job of predicting or figuring out where genes start and stop a bit harder. It’s like trying to read a book with extra sentences that don’t make much sense. These introns can confuse scientists because they don’t contain the actual instructions for making proteins, so it’s like having extra pages in a recipe book that you don’t need."
  },
  {
    "objectID": "chapters/week5.html#repeat-analysis-and-making",
    "href": "chapters/week5.html#repeat-analysis-and-making",
    "title": "5  Annotation",
    "section": "5.4 Repeat Analysis and Making",
    "text": "5.4 Repeat Analysis and Making\n\n5.4.1 Repetitive and Transposable Elements\nIn genomes, there are parts that repeat themselves, like when a song chorus repeats several times. These repeating patterns in the genome come in different types:\n\nSimple Repeats\nThese are like when you say the same word over and over. They don’t have a lot of information.\nTransposable Elements\nThese are like special sequences that can move around in the genome. They come in two flavors: autonomous (like the main boss) and non-autonomous (like the helpers).\n\nTo find these repeating patterns, scientists use computer programs. These programs are like detectives with a set of tools. They follow a step-by-step process, sort of like how you might go through a recipe to cook something. These tools help them find the repeating patterns in the genome.\nOne of the most commonly used detective programs is called RepeatMasker. It’s like a superstar detective because it combines the powers of different tools to get the job done. Think of it as having multiple gadgets in a detective’s toolkit. RepeatMasker uses two de novo repeat-finding tools (RECON and RepeatScout) to identify new repeats, a tandem repeat finder to spot specific kinds of repeats, and a database called Repbase that holds information about known repeats from other species. It’s like using a reference book to help with the detective work.\nApart from RepeatMasker, there are some other detective software programs like PiRATE and Repet. They work in similar ways but might have their own unique tricks and features to find repeating patterns in the genome.\n\n\n5.4.2 Transcriptomic Evidence\nScientists use a special technique called RNA-sequencing to learn which genes are active in a cell or organism at a particular time. It’s like listening in on a conversation to see who’s talking.\nThis technique helps create something called a transcriptome, which is like a detailed list of all the genes that are “talking” or being used by the cell. It’s the most accurate way to know which genes are active.\nBut here’s the catch: just because you’re listening in on a conversation doesn’t mean you hear everyone talking. Similarly, not all genes will be “talking” or active at the same time, so the transcriptome will be incomplete. Some genes are like shy individuals who only speak up in certain situations.\nRNA-sequencing can also tell us about different types of genes. Some genes are only active in specific tissues (like heart genes in heart tissue), some follow a daily schedule (diurnal genes), and some are only used during certain stages of growth (developmental state-specific genes). It’s like finding out who talks only in the library, who talks only in the morning, and who talks only at a party!\nAnd sometimes, genes can have different versions, like how a story can be told in different ways. These are called splice variants, and they can be specific to certain tissues.\nScientists use RNA-sequencing data in two main ways:\n\nThey can match the “conversations” (reads) they hear to a known “script” (the genome) to figure out which genes are active. It’s like finding out who’s talking by checking a script.\nOr they can piece together a new “script” (de novo assembly) from the conversations and then figure out which genes are active. It’s like creating a script from scratch based on what people are saying.\n\n\n\n5.4.3 Prediction from Sequence (i.e., ab initio)\nTo learn patterns from genes, Bioinformaticians can turn to machine learning to make something called a hidden Markov model (i.e., HHM).\n\n\n\n\n\nLayout of a Hidden Markov Model\n\n\n\n\nA HHM is a kind of computer model that was first created back in the late 1970s. It wasn’t originally meant for genetics or biology, though. Instead, it was designed for something quite different: speech recognition.\nThe idea behind an HMM is to break down a process into two main parts: hidden states and observations. In the case of speech recognition, the hidden states represent different sounds or letters that make up spoken words, while the observations are like the actual sound signals.\nEach hidden state in the model is connected to a specific acoustic signal. So, when there’s a change in the hidden state, it’s like switching to a different letter or sound. When you put all these changes together, you get a sequence, or a path, of hidden states that make up a word.\nIn speech recognition, the goal is to figure out the most likely path of hidden states based on the sounds we hear. So, HMMs help computers understand spoken words by finding the best match between what they “hear” (the observations) and the most likely sequence of sounds (the hidden states). It’s like trying to guess the word someone is saying based on the sounds you hear.\n\n5.4.3.1 In Genetics?\nIn the world of genetics, scientists also use Hidden Markov Models (HMMs), but for a different purpose: understanding genes.\nJust like in speech recognition, HMMs for genes have hidden states and observations. In this case, the hidden states represent different parts of a gene, such as exons (the important coding sections), splice donors/acceptors (which are like gene punctuation marks), introns (non-coding parts within a gene), and intergenic regions (spaces between genes).\nThe observations, in this context, are the sequences of DNA letters (A, T, C, G) that make up the gene. It’s like looking at the genetic “code” to understand how genes are put together.\nA gene, in this model, is like a path that starts at the first exon and goes through all the hidden states, following the rules of the model.\n\n\n5.4.3.2 Why Use ab initio Predictions?\nFirst, it’s important to understand that not all genes are active or “expressed” in the samples being studied. Think of it like a library where not all the books are being read at the same time. RNA sequencing only tells us about the genes that are actively “reading” or expressing themselves.\nAnother reason is that some genes don’t follow the usual rules of gene splicing. It’s like having a few books in the library that are written in a different style, and these books might be missed by the RNA sequencing approach.\nLet’s take the example of birch, a type of tree. When scientists used RNAseq for birch, they found about 20,000 sequences (isotigs). However, to get a more complete picture, they still needed to predict around 10,000 genes computationally. This is because RNA sequencing doesn’t always capture all the genes.\nNow, when it comes to picking which gene prediction tools to use, there are a few top contenders in the field, like Augustus and BRAKER. But there are also many other software options available, making it a bit like choosing between different tools for a job.\nIn practice, scientists often use a combination of these tools. For birch, they tried out about 10 different gene predictors and ended up using the four that worked the best. This is because different predictors have different ways of modeling genes, so some might be better at detecting certain types of genes compared to others.\n\n\n\n5.4.4 Gene Models in Other Species\nOne of the key ways to make sure gene predictions are accurate is by looking at external evidence. Think of it like checking your work with a trusted source.\nOne of the most reliable sources of external evidence is RNA sequencing data. This data is like a gold standard because it tells us exactly which genes are active and how they’re structured. It’s a bit like having the answers to a test. Scientists can compare their gene predictions with RNA sequencing data to see if they match up.\nThere are two ways to use RNA sequencing data. One is called de novo transcriptome assembly, which is like putting together a puzzle without a picture on the box. It’s useful when you don’t have a complete genome to work with. But sometimes, it’s a bit like solving a puzzle with missing pieces, and it can be hard to tell similar genes apart.\nAnother good source of evidence is old collections of Expressed Sequence Tags (ESTs). These are like clues left behind by genes. Scientists can use these clues to confirm their gene predictions.\nIt’s also helpful to look at genes in well-studied or closely related species. It’s like asking someone who’s good at a subject for help. If these genes are similar to the ones you’re predicting, it adds confidence to your predictions.\nHowever, predicting genes in gene families that are right next to each other (tandem repeats) can still be a tricky puzzle. It’s like trying to tell identical twins apart. Sometimes, even with external evidence, it’s hard to be certain.\n\n5.4.4.1 Aligning Protein Evidence to Genome\nImagine you have a jigsaw puzzle, and you want to find where certain pieces fit. When scientists have pieces of evidence like proteins or transcripts and want to see where they match in a genome, it’s a bit like finding the right spot for puzzle pieces.\nOne way to do this is using a tool called tblastn. Think of it as a detective tool. What it does is take the genome and look at it in different ways, kind of like trying to read a book from different angles. It’s searching for matches to a protein or transcript.\nHowever, there’s a limitation with tblastn. It doesn’t take into account something important called splicing, which is like how sentences are rearranged in a book to make sense. So, tblastn might miss some of the edges or boundaries of where the protein or transcript matches the genome. It’s like finding parts of a sentence but not realizing where one sentence ends and another begins.\nTo get the whole picture, scientists use other tools like Exonerate, PASA, and GeMoMa. These are like detective tools with special glasses that help them see the splicing parts better. They pay attention to the spots where the sentences (or genes) are joined together with special signals called donor and acceptor sites.\nThese splicing-aware tools also do something clever. They look for regions in the genome where there’s a high match to the protein or transcript evidence and then join these regions together. It’s like finding pieces of a puzzle that fit really well and realizing they belong together."
  },
  {
    "objectID": "chapters/week5.html#combining-evidence",
    "href": "chapters/week5.html#combining-evidence",
    "title": "5  Annotation",
    "section": "5.5 Combining Evidence",
    "text": "5.5 Combining Evidence\nWhen scientists want to figure out what a gene does, they turn to a tool called Interproscan. It’s like using a detective kit with different tools to uncover the gene’s secrets. Interproscan does two important jobs at once. It’s a bit like looking at both the shape and function of a key to understand what it can unlock. For genes, it checks their structure and function.\nOne thing it checks is something called “conserved protein domains.” Think of these as like the building blocks or patterns that genes are made of. Interproscan compares these patterns to a big database that includes PFAM, protein superfamilies, and PANTHER.\nAnother tool in its kit is BLAST, which is like a gene family tree detective. It helps find genes that are similar to the one being studied. These similar genes can give clues about what the gene does. Interproscan also looks at metabolic pathways (like the chemical processes in a cell) using tools like KEGG and Metacyc. It’s like figuring out how ingredients are used in a recipe. It also predicts Gene Ontology (GO) categories, which are like labels that describe what a gene is involved in.\nTo assign a gene’s function, Interproscan uses something called the guilt-by-association principle. It’s like assuming that if someone is often seen with a group of people doing a particular activity, they’re likely involved in that activity too. In genes, if a gene is similar to others in a certain pathway or category, it’s probably involved in the same kind of job."
  },
  {
    "objectID": "chapters/week5.html#final-verification",
    "href": "chapters/week5.html#final-verification",
    "title": "5  Annotation",
    "section": "5.6 Final Verification",
    "text": "5.6 Final Verification\nProf. Jarkko lists a sample checklist for this bit - just to ensure that the genome in question is a good one:\n\nStart with the Gene Model\nBegin with the predicted gene model as your reference. It’s like having a blueprint for a building.\nCompare with RNAseq and EST Data\nCheck if the RNA sequencing (RNAseq) and EST data match the gene model. It’s similar to making sure the building looks like the blueprint.\nExamine Protein Matches in Other Species\nSee if proteins from related species match your gene. It’s like comparing your building to others in the neighborhood.\nBLAST the Protein Sequence\nUse a tool called BLAST to compare your gene’s protein sequence to a big database of genetic information. This helps you find similar genes in various species and might give hints about gene function.\nGather Good Hit Sequences\nCollect the sequences that closely match your gene, especially from well-studied model species. Think of it as getting advice from experts.\nMultiple Sequence Alignment\nLine up the protein sequences and see if they match well. It’s like checking if puzzle pieces fit together. Look out for mistakes like frame shifts, incorrect splicing, or missing start/stop signals.\nWatch Out for “Ns”\nIf you see “N” in the sequence, it means there’s missing information. It’s like having a blank spot on the blueprint.\nManual Adjustments\nIf needed, make manual adjustments to the gene model to improve accuracy. It’s like fine-tuning the building plans to make everything fit perfectly.\nExperimental Verification\nFinally, validate the gene model with experimental data. This involves using RNAseq and targeted experiments to confirm that the gene functions as predicted. It’s like testing the building to ensure it works as intended."
  },
  {
    "objectID": "chapters/week6.html",
    "href": "chapters/week6.html",
    "title": "6  Transcriptomes: Experiment Design and RNA Sequence Data Processing",
    "section": "",
    "text": "Overall Workflow for Working with RNA Sequence Data\nThe above graphic shows the general flow of actions when it comes to working with RNA sequencing data (i.e., from the wet lab stuff to the actual data analysis later on)."
  },
  {
    "objectID": "chapters/week6.html#experimental-design",
    "href": "chapters/week6.html#experimental-design",
    "title": "6  Transcriptomes: Experiment Design and RNA Sequence Data Processing",
    "section": "6.1 Experimental Design",
    "text": "6.1 Experimental Design\nExperimental design is like setting up a plan for a science experiment. It’s super important because it helps us get the right kind of information to answer our questions. Imagine you’re trying to figure out something cool in biology – well, you need a good plan for your experiment.\nFirst, you start with your big question, the one you really want to answer. This is called your “research question.” It’s like the main thing you want to find out. Once you know your question, you need to make sure you do the experiment in the right way.\nThat’s where experimental design comes in. It’s like making a game plan for your experiment. You figure out what kind of data you need and how much of it. This is crucial because if you collect the wrong data or not enough, you might not get good answers.\n\n6.1.1 Principles of Experimental Design\nThere are a total of six to take note of:\n\nComparison\nComparison is like the main reason we do a science experiment. It’s the big “why” behind everything we’re testing. Imagine you want to know if something really works or if it’s different from how things normally are.\nIn science, we often compare two groups. It’s like having a “before” group and an “after” group. We call these groups different names, like “case vs. control,” “treatment vs. control,” or “mutant vs. wild-type.” These names tell us what we’re comparing.\nThe whole point of this comparison is to answer our main question. For example, let’s say we want to know if a new medicine really helps people get better. The “case” group would be the people getting the medicine, and the “control” group would be people not getting the medicine, so we can see if the medicine makes a big difference.\nReplication\nReplication is like a way to make sure our science experiments are trustworthy. When we measure things in experiments, there can be some mistakes or random errors. It’s kind of like when you play darts, and sometimes your throws are a bit off target even if you’re trying your best.\nTo deal with these errors, we do something called replication. It’s a bit like having a backup plan. We do the same experiment multiple times to see if we get similar results each time. We assume that each time we do the experiment, it’s like picking samples randomly from a big bag of marbles. This way, we can make sure our results aren’t just by chance.\nReplication is super important because it helps us figure out if the differences we see in our experiments are for real or if they could have happened just because of random stuff. The more times we repeat the experiment (we call these repeats “replicates”), the more sure we can be about our results.\nBut here’s the tricky part: doing replication, especially in some types of experiments like sequencing, can be a bit expensive. So, scientists often aim to do three or more replicates per condition to strike a balance between cost and getting reliable results.\nBlocking\n\n\n\n\n\nExample of a Blocking Experiment\n\n\n\n\nBlocking in science is like a clever way to make our experiments more accurate. Imagine you’re doing an experiment, and there are some things that you know can affect the results but aren’t really what you’re studying. Blocking helps you deal with those things.\nHere’s how it works: You group together similar things or “units” into special groups called “blocks.” These blocks are like teams of players in a game, and each team is as similar to each other as possible. So, if you’re studying plants, you might have one block of plants that are all the same age, another block of plants that have the same amount of sunlight, and so on.\nThe cool part is that by doing this, you’re making sure that any differences in your results are more likely to be because of the thing you’re actually studying. It’s like playing a game of soccer on a field with no bumps or slopes – it helps you focus on the game without worrying about uneven ground.\nRandomization\nRandomization in science is a bit like mixing things up randomly. Imagine you’re sharing candies with your friends, and you want it to be fair. So, instead of giving candies to your friends in a specific order, you close your eyes and give them candies randomly.\nIn experiments, randomization means doing things without any particular order or plan. It’s like flipping a coin to decide where each part of the experiment goes. For example, if you’re testing a new medicine, you don’t want to give it to one group of people just because they’re your friends. You want to make sure it’s all fair. So, you randomly choose who gets the medicine and who doesn’t.\nWhy is this important? Well, sometimes there are hidden things that can affect the results, things we might not even know about. Randomization helps us break any sneaky patterns or dependencies in the data. It’s like making sure everyone has an equal chance to be in any group, so we can be more sure that our results are because of what we’re testing, not because of something else.\nOrthogonality\nOrthogonality in science is a bit like doing different things in an experiment without them getting all mixed up. Imagine you’re painting a picture with different colors, and you want to know how much each color adds to your artwork. You don’t want the colors to blend together and make it hard to figure out their impact.\nSo, what scientists do is make sure that each thing they’re testing in an experiment can be measured separately, like having separate paintbrushes for each color. This way, they can figure out the effect of one thing without it being influenced by the others.\nLet’s say you’re testing three different types of fertilizers on plants. You want to know how each fertilizer affects the plant’s growth. Orthogonality means you can figure out the impact of each fertilizer without them interfering with each other. It’s like saying, “Okay, I’ll test fertilizer A, see the results, and it won’t mess with the results of fertilizer B or C.”\nFactorial Experiments\nImagine you’re in a science experiment where you want to know how different things, like temperature and humidity, affect plant growth. Instead of changing only one thing at a time, like just temperature or just humidity, you decide to change both things together. That’s what we call a factorial experiment.\nFactorial experiments are like doing science experiments with multiple things happening at once. It’s like trying different combinations of factors to see how they all work together. So, in our plant experiment, you might try high temperature with low humidity, low temperature with high humidity, and so on.\nThe cool thing about factorial experiments is that they help us figure out not only how each factor affects things but also how they interact. It’s like saying, “When we change both temperature and humidity, do they work together to affect plant growth in a special way?”\n\n\n6.1.1.1 Why Are These Principles Important?\nThese principles are super important in science because they help us do experiments in a smart and reliable way. Think of them as rules that scientists follow to make sure their experiments are accurate and make sense.\n\nModels for Analyzing Data\nWhen we collect data from experiments, we need to analyze it to understand what’s happening. These principles help us build models or methods for analyzing the data. It’s like having a special tool to figure out the answers from all the numbers we collect.\nt-test and Two-way ANOVA\nThese are fancy names for methods we use to see if the results of our experiments are meaningful. The t-test helps us compare two groups to see if they are really different. The two-way ANOVA is like a tool to compare more than two groups. They help us know if what we see in our experiments is likely to be true or just random.\nEstimating Statistical Power\nThis is like having a magic crystal ball to tell us how many times we need to do an experiment to be sure of our results. It helps us plan ahead. For example, it tells us how many times we should flip a coin to be sure if it’s really fair or not.\n\n\n\n\n6.1.2 Experiment Designs for RNA Sequencing Data\nWhen conducting RNA sequencing experiments, it’s important to plan things carefully. You need to make sure you have the right kind of data to answer your scientific questions effectively. To do this, there are some basic things you should keep in mind.\nFirstly, you need to collect enough data points, which we call replicates. It’s generally recommended to have at least three replicates to ensure the results are reliable. Think of it like taking multiple measurements to be more certain of an answer.\nSecondly, you should have sensible controls in your experiment. These controls help you compare what happens when you change something (like a treatment) to what happens when nothing changes (the control group). Controls are like a reference point to understand your results better.\nNow, let’s talk about the amount of data you need. It’s essential to have enough sequencing coverage. Think of sequencing coverage as how many times you read each piece of RNA. More coverage can give you a clearer picture of what’s happening in your samples.\nNext, consider the type of RNA you’re interested in. There are different methods to extract RNA, like total RNA or poly-A enrichment. Depending on your research question, you’ll choose the method that suits you best. For instance, in mammalian cells, most of the RNA is ribosomal RNA (rRNA), transfer RNA (tRNA), and only a small part is protein-coding mRNA. But if you’re interested in non-coding RNA, like microRNA, poly-A enrichment may not capture those.\n\n6.1.2.1 Biological and Technical Replication\nData can vary in one of two ways:\n\nBiological Variations\n\n\n\n\n\nExample of a Biological Replication\n\n\n\n\nThis just refers to variations in the data because of the test subjects themselves. This is the stuff that we’re interested in.\nTechnical Variations\n\n\n\n\n\nExample of a Technical Replication\n\n\n\n\nThis is variation because of how the data was collected (i.e., the experiment’s methodology). This is usually stuff that we don’t want (i.e., or unwanted noise in fancy speak).\n\n\n\n6.1.2.2 RNA Samples in the Laboratory\nWhen it comes to the performance of experiments in the lab, there are a few crucial things to consider, especially when dealing with biological samples.\nFirstly, it’s important to keep in mind that once you’ve collected your cell samples, you often can’t separate different types of cells from each other later on. So, it’s best to have the purest samples possible from the start. Imagine trying to unmix colors once they’ve been mixed together – it’s quite challenging.\nAnother important factor is the potential degradation of mRNA, which is a type of genetic material. To prevent this, you need to be careful when collecting your samples. It’s essential to harvest the cells and store them quickly. Think of it like putting perishable food in the refrigerator to keep it fresh. Using things like liquid nitrogen or special RNA-preserving solutions (like RNAlater) can help protect the mRNA from breaking down.\nLastly, everyone in the lab needs to follow proper procedures and guidelines. This is like having a recipe when you’re cooking – you need to follow the steps precisely to get the best results. Proper guidance and training for lab personnel ensure that experiments are done correctly and reliably."
  },
  {
    "objectID": "chapters/week6.html#de-novo-rna-sequence-assembly",
    "href": "chapters/week6.html#de-novo-rna-sequence-assembly",
    "title": "6  Transcriptomes: Experiment Design and RNA Sequence Data Processing",
    "section": "6.2 De novo RNA-Sequence Assembly",
    "text": "6.2 De novo RNA-Sequence Assembly\n\n\n\n\n\nExample of an RNA Assembly\n\n\n\n\nWhen you’re tasked with something called de novo assembly, it’s a bit like putting together a jigsaw puzzle without the picture on the box to guide you. But in this case, it’s not about puzzles; it’s about assembling genetic information, which is similar to what scientists do when they put together a genome, which is like a biological instruction manual.\nTo help with this task, scientists use tools that are like special equipment for assembling genetic information. These tools are actually variations of the ones used for genome assembly, kind of like how you might use different tools to build different types of things. One important thing to know is that all of these tools use de Bruijn graphs, which are like diagrams that help organize the genetic pieces.\nNow, here’s a tricky part: when you’re doing de novo assembly for RNA, the result can be more fragmented compared to assembling a genome. Think of it like trying to put together a map of a city where the roads are broken into smaller pieces; it can be a bit more challenging. But scientists are continually improving these tools to make the process as accurate as possible.\n\n6.2.1 Some Issues to Consider\nFirstly, just like assembling genomes, RNA de novo assembly is quite computationally demanding. It requires a lot of computer power, including parallel processing and a substantial amount of memory. Think of it like needing a powerful computer to handle complex tasks, like running high-end video games.\nTo tackle this task, scientists use specific software tools designed for RNA assembly, such as Trans-ABySS, Trinity, Oases, and SOAPdenovo-trans. These tools are like specialized software programs created to help with the assembly process, much like using software for video editing or graphic design.\nOne challenging aspect of RNA de novo assembly is that it can be difficult to separate different types of splice variants when you don’t have a complete genome available for reference. Splice variants are like different versions of a gene’s instructions, and without the full genome, it’s harder to tell them apart.\nAfter the assembly, researchers often cluster the results into gene models. This is like organizing different puzzle pieces into groups that belong together, helping make sense of the genetic information.\nLastly, there are versions of RNA de novo assembly that are guided by a known genome. These versions use information from a reference genome to estimate the transcriptome, which is like having a map to help put the genetic puzzle together more accurately. It’s like using a reference book to solve a tricky crossword puzzle."
  },
  {
    "objectID": "chapters/week6.html#rna-sequencing-data-processing",
    "href": "chapters/week6.html#rna-sequencing-data-processing",
    "title": "6  Transcriptomes: Experiment Design and RNA Sequence Data Processing",
    "section": "6.3 RNA Sequencing Data Processing",
    "text": "6.3 RNA Sequencing Data Processing\n\n\n\n\n\nGeneral Workflow for Dealing with RNA Sequencing Data\n\n\n\n\nThe first steps to this are all super similar to the previous weeks’ lecture on quality control and read trimming (i.e., lecture 3).\n\n6.3.1 Read Mapping\nWhen we talk about aligning reads to a genome, it’s like trying to find the right spot for puzzle pieces in a jigsaw puzzle, but with a twist. The problem is that the puzzle pieces (the reads) might have some mistakes or differences, and the puzzle board (the genome) can also have variations.\nThe big challenge here is finding the best, most accurate spot for lots and lots of puzzle pieces (reads) when they might not fit perfectly because of errors or differences in the genetic code. Imagine doing this with thousands of pieces – it can get pretty complicated!\nTo solve this puzzle, scientists use a general solution. They know that the puzzle board (genome) stays the same for every piece they’re trying to place. So, they create something like an index or a map that helps them find the right spots quickly. It’s similar to having a map of a city, and no matter how many people are looking for different places, they can all use the same map because the city doesn’t change.\n\n6.3.1.1 Aligning Reads to the Genome\nhen we talk about aligning reads to a genome, it’s like trying to match pieces from one puzzle (the reads) to another (the genome). There are a few ways to do this, like using a tool called BLAST, which is similar to trying to match puzzle pieces by hand. However, BLAST can be quite slow when dealing with a large number of reads, making it impractical for many scientific purposes.\nTo speed up the process, scientists have come up with a better solution, like using something called the Burrows-Wheeler transform. This is like having a special machine that can quickly and efficiently find the best matches between the puzzle pieces (reads) and the puzzle board (genome).\nScientists have implemented this Burrows-Wheeler transform in software tools such as BWA-mem, Bowtie2, and SOAP. These tools are like computer programs that use the special machine to find matches between the reads and the genome, making the process much faster and more practical for scientific research.\n\n\n\n6.3.2 Splicing Aware Aligners\nThe thing about tools that directly map reads to the genome is that they don’t really work for RNA sequencing (at least not when it comes to RNA sequencing data that hasn’t been processed in some way). So, there are tools like HISAT2, Tophat2, and STAR aligners.\n\n6.3.2.1 Tophat\n\n\n\n\n\nInner Workings of Tophat\n\n\n\n\nTophat is like a detective for your genetic data. Its main idea is to figure out which parts of your genetic material, called “reads,” can be matched to the genome. Think of the genome as a big instruction manual, and the reads are like pieces of text from that manual.\nWhat Tophat does is it finds the reads that perfectly match the genome. These matching reads represent the “exons” of genes, which are like specific paragraphs in the instruction manual. Tophat identifies these exons and helps scientists understand which parts of the genome are being actively used by the cell.\nBut here’s where it gets interesting. Tophat also looks at the reads that don’t match the genome. These unmatched reads are like clues in a mystery novel. They suggest that something special is happening, called splicing. Splicing is like rearranging paragraphs in the instruction manual to create different versions of the gene’s instructions. Tophat helps identify these splicing sites, which are crucial for understanding how genes work.\n\n\n6.3.2.2 STAR\nSTAR works in several ways:\n\nIndexing\nImagine you have a giant book, which is like the genome. STAR’s first step is to create an index for this book. This index is like a quick reference guide that helps you find specific information in the book much faster.\nNow, think of the genetic reads as sentences from the book. STAR’s job is to find the best match for these sentences in the book. It does this by looking for the longest sequence of words in the sentences that match something in the book.\n\n\n\n\n\nUnmapping Parts of STAR\n\n\n\n\nBut here’s where STAR does something clever. It doesn’t just stop at finding the best matches; it also looks at the parts of the sentences that don’t match anything in the book. It’s like finding sentences that are partly in another language or from a different book. These unmatched parts can tell you something important, like hints or clues.\nClustering and Stitching\nImagine you have a bag of puzzle pieces, and each piece is like a small part of a picture. STAR’s job is to group these pieces together in a smart way.\nFirst, STAR looks at the pieces that are close to each other, like putting together pieces that are next to each other in the puzzle. This is called clustering. It’s like collecting all the sky pieces, all the grass pieces, and so on, so you can start building sections of the puzzle.\nBut not all puzzle pieces fit perfectly right away. Some might have a small gap or a piece missing, similar to how genetic reads might have small differences. STAR doesn’t give up; it’s like a puzzle master who can adjust and fill in the gaps, even when there are mistakes or missing parts. This process of adjusting and matching is called stitching.\n\n\n\n\n6.3.3 Quantification for RNA Sequence Analyses\nThink of this process as putting together a jigsaw puzzle with some special helpers.\nFirst, you have your genetic reads, which are like puzzle pieces. Tools like TopHat, HISAT, or STAR are like expert puzzle solvers. Their job is to figure out how these reads match up with the reference genome, similar to how puzzle solvers match pieces to the picture on the puzzle box.\nOnce these tools have aligned the reads, they not only assign the reads to specific parts of genes (exons), but they also figure out how these gene parts connect, kind of like finding the links between pieces in a puzzle.\n\n\n\n\n\nWorkflow in Cufflinks\n\n\n\n\nNow, think of Cufflinks or Stringtie as the experts who take these partially solved puzzles and complete them. They estimate what the complete gene models should look like based on the connections found by the alignment tools. It’s like creating a full picture from the partially assembled puzzle.\nMoreover, Cufflinks or Stringtie also quantify how active each gene is, sort of like measuring how bright different parts of the puzzle are. This helps scientists understand which genes are working hard and which ones are not as active."
  },
  {
    "objectID": "chapters/week6.html#pseudoalignment",
    "href": "chapters/week6.html#pseudoalignment",
    "title": "6  Transcriptomes: Experiment Design and RNA Sequence Data Processing",
    "section": "6.4 Pseudoalignment",
    "text": "6.4 Pseudoalignment\nImagine you have a library of books, and each book represents a gene in the transcriptome. Instead of reading every word in the books (which can be time-consuming), this method is like flipping through the pages quickly to see which books are a good match for a specific sentence (the genetic read).\n\n\n\n\n\nIllustration of What Kallisto Index Does\n\n\n\n\nThe advantage of this approach is speed. It’s much faster because it doesn’t require reading every word in every book (gene) like traditional methods. Instead, it quickly figures out which books (transcripts) the sentence (read) might belong to.\nHowever, there are some downsides to this speedier approach. One drawback is that it may not provide as detailed information as the traditional method. It tells you which books (transcripts) the sentence (read) could belong to, but it doesn’t pinpoint the exact location within each book (transcript). So, you lose some precision.\nTo make this rapid searching possible, these tools employ something called de Bruijn graphs. Think of this as a clever way of organizing information for quick searches, like using an efficient filing system to find documents in a library.\nTwo popular tools for this approach are kallisto and salmon. They take two main steps: First, they create an index, which is like building a fast-reference guide for the library of books. Then, they perform the search, quickly identifying which books (transcripts) match the sentences (reads).\n\n6.4.1 Quantification\nImagine you have a bunch of puzzle pieces (genetic reads) and you’ve already figured out which parts of the puzzle (genes) these pieces belong to using a fast mapping method. However, there’s a bit of complexity here. Sometimes, one puzzle piece (read) can fit into multiple puzzles (gene models).\nThis is where pseudoalignment comes in, and it’s somewhat similar to what tools like Cufflinks do. It’s like trying to figure out how many pieces of each puzzle you have, but with a twist. Instead of directly counting the pieces, you use a sophisticated approach, kind of like a detective trying to solve a mystery.\nIn this approach, you create a probabilistic model, which is like a set of rules that help you estimate how many pieces of each puzzle you have. These rules are based on the idea of maximum likelihood, which means finding the most likely answer given the data you have. It’s a bit like making an educated guess about the number of pieces for each puzzle based on the clues you’ve gathered.\nIn practical terms, you want to find the best estimates for transcript abundances, which is like figuring out how many pieces belong to each puzzle. You do this by making sure that the observed coverage of gene parts (exons) in your reads matches the model’s predictions as closely as possible."
  },
  {
    "objectID": "chapters/week6.html#normalization",
    "href": "chapters/week6.html#normalization",
    "title": "6  Transcriptomes: Experiment Design and RNA Sequence Data Processing",
    "section": "6.5 Normalization",
    "text": "6.5 Normalization\nImagine you have two baskets, and each basket contains different types of fruits. The goal is to compare the number of fruits in each basket, but there are two challenges:\n\nDifferent Library Sizes\nIn one basket, there may be more fruits, and in the other, there could be fewer. This difference in the total number of fruits in each basket makes it tricky to compare them directly. It’s like trying to figure out which basket has more without considering their sizes.\nDifferent Gene Lengths\nNot all fruits are the same size; some are larger, and some are smaller. Similarly, genes in genetic data can have varying lengths. This complicates comparisons because you need to account for these differences in gene sizes when comparing the two baskets.\n\nNormalization in RNA-seq is like a magic tool that helps you address these problems. It’s like resizing the baskets and the fruits so that you can make a fair comparison.\nFor the first issue, normalization adjusts the basket sizes so that they’re the same, making it easier to compare the number of fruits in each. This way, you’re not unfairly favoring one basket over the other just because it started with more or fewer fruits.\nFor the second issue, normalization scales the size of the fruits (gene lengths) so that they are comparable. It’s like resizing the fruits to a common standard, ensuring that you’re not giving more weight to larger fruits when comparing the two baskets.\n\n6.5.1 Transcripts per Million (i.e., TPM)\nImagine you have a collection of different-sized recipe books (genes), and you want to know how popular each recipe book is in terms of the number of recipes inside (reads). However, the recipe books vary in size, making it challenging to compare them directly.\nTPM normalization is like a clever way of standardizing and making these numbers “prettier” for easy comparison.\nHere’s how it works:\n\nDivide Read Counts by Gene Length\nFirst, you count the number of reads that belong to each recipe book (gene). To make things fair, you divide these counts by the length of each recipe book in kilobases (KB). This gives you “reads per kilobase” (RPK), which is like figuring out how many recipes are in each kilobase of the book.\nSum RPK and Scale to a Million\nYou add up all the RPK values for all the recipe books (genes). This sum represents the total number of “recipes per kilobase” (sum RPK). To make the numbers look neater, you divide this sum by a million, creating a scaling factor (S).\nCalculate TPM\nFinally, to find the Transcripts per Million (TPM) for each recipe book (gene), you divide the RPK value for that gene by the scaling factor S. This normalization tells you how abundant each gene is relative to the total number of recipes per million kilobases.\n\nIn simpler terms, TPM is like a way of comparing how popular each recipe book is, taking into account their sizes. It’s the preferred method for RNA-seq normalization nowadays because it helps describe the relative abundance of reads assigned to each gene in a library. So, it’s like getting a clear picture of which recipe books are the most popular in your collection, considering their different sizes."
  },
  {
    "objectID": "chapters/week7.html",
    "href": "chapters/week7.html",
    "title": "7  Transcriptomes: Differential Expression to Enrichment Analyses",
    "section": "",
    "text": "Let’s explore how we go from raw read counts to identifying differential gene expression in a straightforward way.\nImagine you have a collection of different-sized baskets, each containing different numbers of apples (gene counts). Your goal is to figure out which baskets have more or fewer apples, but you need a consistent way to do this.\nNow, in the world of gene expression analysis, different software tools come with their own methods for counting genes, but they typically start with unnormalized count data, just like having apples in baskets without any adjustments.\nIn your analysis, when you use a method like pseudoalignment, it not only provides raw counts (the number of apples in each basket) but also normalized values called TPM (Transcripts per Million), which are like having apples per million baskets.\nHowever, when it comes to finding genes that are differentially expressed (meaning they have significantly more or fewer apples), we usually work with the raw count data, not TPM values. It’s like counting apples directly rather than adjusting for the number of baskets.\nHere’s a standard workflow:\nIn essence, you’re like a fruit analyst, taking the raw count data, adjusting it if needed, and using statistical models to figure out which baskets have significantly different numbers of apples. This is how we identify differential gene expression, helping us understand which genes are playing a more significant role in specific conditions or situations."
  },
  {
    "objectID": "chapters/week7.html#whole-genome-duplications",
    "href": "chapters/week7.html#whole-genome-duplications",
    "title": "7  Annotation",
    "section": "7.1 Whole Genome Duplications",
    "text": "7.1 Whole Genome Duplications\n\n\n\n\n\nIllustration of a Whole Genome Duplication Event\n\n\n\n\nIn the above example, we have a genome from two different organisms - one blue and one orange.\nIf the genomes of two organisms come together, this is called an autopolyploid. Otherwise, this is called an alloalyploid.\nWhen genome evolution happens through whole genome duplications, it’s like making an identical copy of the entire set of genes. Imagine you have a long list of items, and you make an exact duplicate of that list. Here’s what happens:\nRight after the duplication, the genes in both copies are in the same order, which scientists call “synteny.” It’s like having the same items in the same order on both lists.\nBut, as time goes by, some changes occur:\n\nFractionation\nThis is when the genome starts to change. Some genes get lost because they become redundant (like having two identical items on your list), and random mutations can also affect genes.\nGenome rearrangements\nThink of this as reshuffling the items on your list. The order of genes can change.\nSyntenic blocks\nEven after a long time, there are still some parts of the genome where a bunch of genes look very similar to another bunch. It’s like having sections of your list that are almost the same as sections in the duplicate list.\nDiploid form\nEventually, the genome organizes itself back into a diploid form. This means it goes back to having two sets of genes, like you have one original list and one duplicate list, but with some changes.\n\n\n7.1.1 Genome Duplication in Organisms\n\n7.1.1.1 Plants\n\n\n\n\n\nWhole Genome Duplication in Plants\n\n\n\n\nGenome duplications happen quite often in the evolution of plants. It’s like when you have a favorite plant and it suddenly makes extra copies of all its genetic instructions. Here are some examples:\nAncient Hexaploid Event in Eudicots: About 150 million years ago, a group of plants called Eudicots had a special event where their entire set of genes got duplicated not just once, but three times! It's like having three identical instruction manuals for a plant.\n\nGrape\nAfter that ancient event, some plants like grapes didn’t have any more big duplications. So, they just kept the three copies they had and didn’t make any new ones.\nPoplar\nOn the other hand, poplar, which is another type of plant, recently had a whole genome duplication event. This means it made a fresh copy of all its genes, so it now has two sets of instructions.\nArabidopsis\nArabidopsis, yet another type of plant, had two whole genome duplication events in its history. So, it’s like having three instruction manuals originally, then copying them twice more, for a total of six!\n\n\n\n7.1.1.2 Fish\n\n\n\n\n\nWhole Genome Duplication in Fish\n\n\n\n\nIn the world of minnows and carps, which belong to a group called Cyprininae, there are around 1,300 different species. Now, here’s something interesting: approximately 400 of these species are what we call “polyploid.”\nPolyploid is a special term that tells us these species have more than the usual set of genetic instructions. It’s like having extra copies of a recipe book. In the case of these minnows and carps, having extra genetic copies can sometimes be a helpful adaptation. It’s a bit like having extra tools in your toolbox—they can be useful for different situations.\n\n\n\n7.1.2 Comparing Genomes\n\n\n\n\n\nComparing Two Genomes Using a Dot Plot\n\n\n\n\nA dot plot is like a cool tool scientists use to compare different genomes, which are like the instruction manuals for living things. Here’s how it works:\nFirst, imagine you have a bunch of genes from two different organisms, and you want to see how similar they are. The dot plot helps you visualize this by showing dots where genes are similar.\nAt first, it looks a bit messy with lots of dots. But, at a basic level, it helps us find pairs of genes that are kind of like neighbors in the instruction manuals. These are called syntenic gene pairs, and they might be important for understanding how organisms are related.\nIf we want to get even more detailed, we can study these syntenic gene pairs more closely. It’s like zooming in on those neighbor genes to see exactly how they match up.\nOne way to find these syntenic blocks, which are groups of similar genes, is to use a special program called DAGchainer. It’s like a detective that helps us find these important blocks in the instruction manuals.\nDAGchainer uses a smart method called dynamic programming to do this. Think of it like a super organized way to solve puzzles and find important patterns in the genes. So, dot plots and programs like DAGchainer are tools scientists use to uncover the secrets hidden in genomes!\n\n\n7.1.3 Exploring Whole Genome Duplications with Synteny\n\n\n\n\n\nSynteny Plots\n\n\n\n\nImagine you have two sets of genes from different organisms, and you want to figure out when they had a big duplication event. Synteny is like a tool that helps us with this detective work.\nFirst, scientists do something called pairwise alignment, which is like comparing the two sets of genes to see which ones match up. These matching gene pairs are called syntenic gene pairs.\nNow, here’s the cool part: we can look at how many “synonymous substitutions” have happened in these gene pairs. Synonymous substitutions are like small changes in the genes that don’t really affect how the protein is made or its building blocks (amino acids).\nBy counting these mutations, it’s kind of like using a molecular clock. This clock tells us how much time has passed since the big gene duplication happened. Just like a clock helps us keep track of time, counting these mutations helps scientists figure out when the duplication event took place.\n\n\n7.1.4 Functional Biases on Gene Origins\n\n\n\n\n\nProf. Jarkko’s Graph\n\n\n\n\nWhen we talk about how genes work and where they come from, there are two main ways genes can be duplicated: Whole Genome Duplication (WGD) and Tandem Duplication. These two processes can affect how genes function.\nIn the case of Whole Genome Duplication (WGD), there’s something called the dosage-balance hypothesis. This means that genes that are really important and highly connected in the cell are usually kept around when gene duplication happens. Think of it like keeping the most important players on your sports team. These important genes often include things like transcription factors and regulators, which are like the coaches and referees of the cell.\nOn the other hand, with Tandem Duplication, it’s more about recent adaptations. Imagine if you have a group of friends, and you suddenly need to deal with a new challenge, like a surprise test. You might quickly form a study group with the friends who are best at that subject. Similarly, genes that are duplicated in tandem, one after the other, often include genes that help an organism cope with immediate changes in its environment. For example, if there’s a sudden threat from a pathogen, genes related to defense might get duplicated to help the organism adapt quickly.\n\n7.1.4.1 Why is this Important?\nAfter a special kind of gene duplication called Whole Genome Duplications (WGDs), the genes that stay around are usually super important. They’re like the VIPs of the cell—genes that help control things like when to grow, when to stop, and how to respond to signals. We call this idea the “dosage-balance hypothesis.”\nThe cool thing is that similar VIP genes can be found in almost all species, and they work in pretty similar ways. It’s like finding the same important tools in everyone’s toolbox. This makes it easier for scientists to find similar genes in different species, and we call these similar genes “orthologs.”\nNow, when genes are copied one after the other, like in Tandem Duplication, they often help an organism deal with changes in its environment. These are like the genes that help you adapt when something unexpected happens, like a sudden change in weather. These newly copied genes are very similar in their instructions.\nBut here’s where it gets tricky: these copied genes can be a bit different between individuals of the same species, and this can cause problems when scientists are trying to match up short pieces of genetic information (short reads) with the gene instructions.\n\n\n\n7.1.5 Implications for Genome Annotation\nFirst, think about the number of gene copies. It’s like saying that in some species, there might be more copies of a certain gene, while in others, there are fewer. It’s kind of like how some people have more toys than others. This variation in gene copies can make things a bit complicated.\nNow, when it comes to species, those that are more closely related, like siblings, tend to be more similar to each other in terms of their genes. But species that are very different, like distant cousins, might have genes that look quite different. It’s a bit like how you might look more like your brother or sister than a cousin you’ve never met.\nPolyploidy events are like times when a species had too many copies of its entire set of genes. Imagine if you suddenly had two toy chests filled with toys instead of one. It can be a bit tricky to figure out which toys are exactly the same in both chests, and this is kind of like the challenge scientists face when finding orthologs in species that have experienced polyploidy.\nLastly, with tandem duplications, it’s like having the same toy repeated in a row in your toy chest. When this happens with genes, it can be hard to predict because it looks like there’s just one type of toy in your chest, even though there are actually many copies of the same toy. So, for scientists trying to understand gene models, it can be a bit tricky to tell what’s going on."
  },
  {
    "objectID": "chapters/week7.html#quality-control-of-assembly",
    "href": "chapters/week7.html#quality-control-of-assembly",
    "title": "7  Annotation",
    "section": "7.2 Quality Control of Assembly",
    "text": "7.2 Quality Control of Assembly\n\n7.2.1 N50 Value\nWhen scientists are putting together the puzzle pieces of a genome, one of the most important things to check is how well those pieces fit together. This is called contiguity.\nContiguity is like looking at a jigsaw puzzle and making sure all the pieces are lined up nicely. You want to see how many separate pieces, called contigs or scaffolds, make up the genome.\n\n\n\n\n\nIllustration of the N50 Value\n\n\n\n\nNow, to tell how good the assembly is, we use a special number called the N50 value. This number tells us something really important: it shows the length where 50% of the genome assembly is made up of contigs or scaffolds longer than this length.\nThink of it like this: if you have a bunch of puzzle pieces, and you find the N50 value, it means that half of the puzzle pieces are at least that big. So, the higher the N50 value, the better the quality of the genome assembly because it means larger and more complete pieces are used to put together the genome.\n\n\n7.2.2 Conserved Single-Copy Genes\nWhen scientists are working on putting together a genome, one of the most crucial things they want to make sure of is that they capture all the important parts of the genes. Imagine if you were building a car, and you needed to make sure you had all the essential parts like the engine, wheels, and brakes. In genome assembly, those important parts are the genes.\nNow, here’s the tricky part: figuring out if you’ve got all the genes in their proper places can be hard without some extra help. It’s like trying to build a car without an instruction manual.\nBut scientists have a clever trick. They’ve looked at lots of different species and found a special group of genes that are found in nearly all genomes as single copies. This means there’s usually just one of each of these genes in the genome. It’s like finding common tools that are used in many different types of cars.\nThese single-copy genes are super important because they’re often the first ones to disappear if there are extra copies or if genes get shuffled around during evolution.\nTo help scientists check if they’ve got these essential genes in their genome assembly, they use software like CEGMA (which is a bit outdated now) and BUSCO. These programs act like detectives to see if all the crucial genes are present and in good shape, kind of like making sure you have all the essential car parts before you start driving.\n\n\n7.2.3 BUSCO\nImagine you have a big jigsaw puzzle, and you want to make sure you’re putting all the right pieces in the right places. In the world of genomes, those puzzle pieces are genes, and they need to be correctly identified and placed. This is where Benchmarking Universal Single-Copy Orthologs (i.e., BUSCO) comes in.\nBUSCO helps with genome annotation, which is like labeling and understanding the genes in a genome. It does this using a two-step approach:\nFirst, it uses something called tblastn to search for known single-copy protein sequences within the genome. Think of this as looking for specific shapes in your puzzle pieces that you know should be there.\nOnce it finds these matches, it uses them to teach a special tool called “Augustus” how to predict where other genes are in the genome. It’s like showing someone a few pieces of the puzzle so they can guess where the rest of the pieces should go.\nThen, Augustus starts predicting the genes and looks for matches to a set of genes that are known to be present as single copies in most genomes. These genes are kind of like the most important pieces of the puzzle.\nBUSCO gives a report that tells you the number of found full-length genes (genes that are complete and correct), duplicated full-length genes (genes that have extra copies), fractionated genes (genes that are split up), and missing genes (genes that are nowhere to be found).\nThe “Busco score” is a handy percentage that tells you how well your genome assembly is doing. If it’s over 95%, that’s really good and means you have a high-quality assembly. If it’s between 90-95%, that’s still good. But if it’s below 80%, it’s a sign that there might be issues with your genome assembly.\n\n7.2.3.1 Pros and Cons\nUsing BUSCO is like taking a quick test to check how well you’ve done a big job, like building a complex model. It’s become a standard method in genomics because it helps scientists figure out if they did a good job in understanding the genes of an organism quickly.\nNow, there are a few things to keep in mind when using BUSCO:\n\nNot the Whole Picture\nThe results from BUSCO might not tell you everything about the quality of the entire set of genes in the organism. It’s like if you looked at only a few parts of a car and assumed the whole car was in perfect shape.\nPredicting Easy Genes\nBUSCO focuses on finding genes that are considered “easy” to predict because they’re pretty much the same across many species. This means it might miss some of the more unique or hard-to-predict genes.\nReference Species\nBUSCO defines these “easy” genes based on a set of species used as a reference. However, not all organisms have the exact same genes, and in some cases, the genes might be so different that BUSCO can’t spot them.\nDuplicate Genes\nSometimes BUSCO might find extra copies of genes. This can happen because of problems with how the genome was put together (assembly problems) or because the organism naturally has extra copies due to recent genome duplications."
  },
  {
    "objectID": "chapters/week7.html#genome-annotation",
    "href": "chapters/week7.html#genome-annotation",
    "title": "7  Annotation",
    "section": "7.3 Genome Annotation",
    "text": "7.3 Genome Annotation\nLet’s break down three types of annotation tasks:\n\nGenome Annotation\nImagine you have a huge book with lots of words but no titles or chapters. Genome annotation is like giving titles to chapters in this book. Scientists try to figure out where the important genes are hiding in the DNA sequence of an organism’s genome. It’s like identifying the main characters in a story.\nStructural Annotation\nThis is about looking closer at the genes you’ve identified. It’s like studying the characters in a book and finding out if they have any special traits or abilities. In genetics, scientists try to spot specific patterns or “domains” in genes that are similar across different species. These domains are like superpowers for genes, and they help scientists understand what the genes do.\nFunctional Annotation\nOnce you know what the genes look like and what patterns they have, you want to figure out what they actually do. It’s like reading the book to learn about the characters’ roles. Scientists try to guess the function of a gene based on its patterns and shapes. They do this by finding genes in other species that are very similar in structure or sequence (like finding characters in other books that are a lot like the ones you already know). Then, they assume that these similar genes have similar functions. To be really sure, they might do experiments, like RNA sequencing or mutant tests, to check if their guess is right.\n\nSo, in a nutshell, genome annotation1 is like naming characters in a book (identifying genes), structural annotation is like discovering special traits of these characters (identifying patterns in genes), and functional annotation is like figuring out what these characters do in the story (identifying gene functions). It’s all about understanding the genetic story of an organism!\n\n7.3.1 Coding Regions\n\n\n\n\n\nIllustration of Introns and Exons\n\n\n\n\nWhen scientists study a genome, they’re like detectives trying to find important clues. They want to identify the functional parts of the genome, which are like the chapters and important details in a story. These functional parts include things like the promoter (which tells genes when to start), exons (which are the important coding parts of genes), and introns (which are like extra, non-coding sections within genes).\nNow, introns are like tricky puzzles in this genome story. They make the job of predicting or figuring out where genes start and stop a bit harder. It’s like trying to read a book with extra sentences that don’t make much sense. These introns can confuse scientists because they don’t contain the actual instructions for making proteins, so it’s like having extra pages in a recipe book that you don’t need."
  },
  {
    "objectID": "chapters/week7.html#repeat-analysis-and-making",
    "href": "chapters/week7.html#repeat-analysis-and-making",
    "title": "7  Annotation",
    "section": "7.4 Repeat Analysis and Making",
    "text": "7.4 Repeat Analysis and Making\n\n7.4.1 Repetitive and Transposable Elements\nIn genomes, there are parts that repeat themselves, like when a song chorus repeats several times. These repeating patterns in the genome come in different types:\n\nSimple Repeats\nThese are like when you say the same word over and over. They don’t have a lot of information.\nTransposable Elements\nThese are like special sequences that can move around in the genome. They come in two flavors: autonomous (like the main boss) and non-autonomous (like the helpers).\n\nTo find these repeating patterns, scientists use computer programs. These programs are like detectives with a set of tools. They follow a step-by-step process, sort of like how you might go through a recipe to cook something. These tools help them find the repeating patterns in the genome.\nOne of the most commonly used detective programs is called RepeatMasker. It’s like a superstar detective because it combines the powers of different tools to get the job done. Think of it as having multiple gadgets in a detective’s toolkit. RepeatMasker uses two de novo repeat-finding tools (RECON and RepeatScout) to identify new repeats, a tandem repeat finder to spot specific kinds of repeats, and a database called Repbase that holds information about known repeats from other species. It’s like using a reference book to help with the detective work.\nApart from RepeatMasker, there are some other detective software programs like PiRATE and Repet. They work in similar ways but might have their own unique tricks and features to find repeating patterns in the genome.\n\n\n7.4.2 Transcriptomic Evidence\nScientists use a special technique called RNA-sequencing to learn which genes are active in a cell or organism at a particular time. It’s like listening in on a conversation to see who’s talking.\nThis technique helps create something called a transcriptome, which is like a detailed list of all the genes that are “talking” or being used by the cell. It’s the most accurate way to know which genes are active.\nBut here’s the catch: just because you’re listening in on a conversation doesn’t mean you hear everyone talking. Similarly, not all genes will be “talking” or active at the same time, so the transcriptome will be incomplete. Some genes are like shy individuals who only speak up in certain situations.\nRNA-sequencing can also tell us about different types of genes. Some genes are only active in specific tissues (like heart genes in heart tissue), some follow a daily schedule (diurnal genes), and some are only used during certain stages of growth (developmental state-specific genes). It’s like finding out who talks only in the library, who talks only in the morning, and who talks only at a party!\nAnd sometimes, genes can have different versions, like how a story can be told in different ways. These are called splice variants, and they can be specific to certain tissues.\nScientists use RNA-sequencing data in two main ways:\n\nThey can match the “conversations” (reads) they hear to a known “script” (the genome) to figure out which genes are active. It’s like finding out who’s talking by checking a script.\nOr they can piece together a new “script” (de novo assembly) from the conversations and then figure out which genes are active. It’s like creating a script from scratch based on what people are saying.\n\n\n\n7.4.3 Prediction from Sequence (i.e., ab initio)\nTo learn patterns from genes, Bioinformaticians can turn to machine learning to make something called a hidden Markov model (i.e., HHM).\n\n\n\n\n\nLayout of a Hidden Markov Model\n\n\n\n\nA HHM is a kind of computer model that was first created back in the late 1970s. It wasn’t originally meant for genetics or biology, though. Instead, it was designed for something quite different: speech recognition.\nThe idea behind an HMM is to break down a process into two main parts: hidden states and observations. In the case of speech recognition, the hidden states represent different sounds or letters that make up spoken words, while the observations are like the actual sound signals.\nEach hidden state in the model is connected to a specific acoustic signal. So, when there’s a change in the hidden state, it’s like switching to a different letter or sound. When you put all these changes together, you get a sequence, or a path, of hidden states that make up a word.\nIn speech recognition, the goal is to figure out the most likely path of hidden states based on the sounds we hear. So, HMMs help computers understand spoken words by finding the best match between what they “hear” (the observations) and the most likely sequence of sounds (the hidden states). It’s like trying to guess the word someone is saying based on the sounds you hear.\n\n7.4.3.1 In Genetics?\nIn the world of genetics, scientists also use Hidden Markov Models (HMMs), but for a different purpose: understanding genes.\nJust like in speech recognition, HMMs for genes have hidden states and observations. In this case, the hidden states represent different parts of a gene, such as exons (the important coding sections), splice donors/acceptors (which are like gene punctuation marks), introns (non-coding parts within a gene), and intergenic regions (spaces between genes).\nThe observations, in this context, are the sequences of DNA letters (A, T, C, G) that make up the gene. It’s like looking at the genetic “code” to understand how genes are put together.\nA gene, in this model, is like a path that starts at the first exon and goes through all the hidden states, following the rules of the model.\n\n\n7.4.3.2 Why Use ab initio Predictions?\nFirst, it’s important to understand that not all genes are active or “expressed” in the samples being studied. Think of it like a library where not all the books are being read at the same time. RNA sequencing only tells us about the genes that are actively “reading” or expressing themselves.\nAnother reason is that some genes don’t follow the usual rules of gene splicing. It’s like having a few books in the library that are written in a different style, and these books might be missed by the RNA sequencing approach.\nLet’s take the example of birch, a type of tree. When scientists used RNAseq for birch, they found about 20,000 sequences (isotigs). However, to get a more complete picture, they still needed to predict around 10,000 genes computationally. This is because RNA sequencing doesn’t always capture all the genes.\nNow, when it comes to picking which gene prediction tools to use, there are a few top contenders in the field, like Augustus and BRAKER. But there are also many other software options available, making it a bit like choosing between different tools for a job.\nIn practice, scientists often use a combination of these tools. For birch, they tried out about 10 different gene predictors and ended up using the four that worked the best. This is because different predictors have different ways of modeling genes, so some might be better at detecting certain types of genes compared to others.\n\n\n\n7.4.4 Gene Models in Other Species\nOne of the key ways to make sure gene predictions are accurate is by looking at external evidence. Think of it like checking your work with a trusted source.\nOne of the most reliable sources of external evidence is RNA sequencing data. This data is like a gold standard because it tells us exactly which genes are active and how they’re structured. It’s a bit like having the answers to a test. Scientists can compare their gene predictions with RNA sequencing data to see if they match up.\nThere are two ways to use RNA sequencing data. One is called de novo transcriptome assembly, which is like putting together a puzzle without a picture on the box. It’s useful when you don’t have a complete genome to work with. But sometimes, it’s a bit like solving a puzzle with missing pieces, and it can be hard to tell similar genes apart.\nAnother good source of evidence is old collections of Expressed Sequence Tags (ESTs). These are like clues left behind by genes. Scientists can use these clues to confirm their gene predictions.\nIt’s also helpful to look at genes in well-studied or closely related species. It’s like asking someone who’s good at a subject for help. If these genes are similar to the ones you’re predicting, it adds confidence to your predictions.\nHowever, predicting genes in gene families that are right next to each other (tandem repeats) can still be a tricky puzzle. It’s like trying to tell identical twins apart. Sometimes, even with external evidence, it’s hard to be certain.\n\n7.4.4.1 Aligning Protein Evidence to Genome\nImagine you have a jigsaw puzzle, and you want to find where certain pieces fit. When scientists have pieces of evidence like proteins or transcripts and want to see where they match in a genome, it’s a bit like finding the right spot for puzzle pieces.\nOne way to do this is using a tool called tblastn. Think of it as a detective tool. What it does is take the genome and look at it in different ways, kind of like trying to read a book from different angles. It’s searching for matches to a protein or transcript.\nHowever, there’s a limitation with tblastn. It doesn’t take into account something important called splicing, which is like how sentences are rearranged in a book to make sense. So, tblastn might miss some of the edges or boundaries of where the protein or transcript matches the genome. It’s like finding parts of a sentence but not realizing where one sentence ends and another begins.\nTo get the whole picture, scientists use other tools like Exonerate, PASA, and GeMoMa. These are like detective tools with special glasses that help them see the splicing parts better. They pay attention to the spots where the sentences (or genes) are joined together with special signals called donor and acceptor sites.\nThese splicing-aware tools also do something clever. They look for regions in the genome where there’s a high match to the protein or transcript evidence and then join these regions together. It’s like finding pieces of a puzzle that fit really well and realizing they belong together."
  },
  {
    "objectID": "chapters/week7.html#combining-evidence",
    "href": "chapters/week7.html#combining-evidence",
    "title": "7  Annotation",
    "section": "7.5 Combining Evidence",
    "text": "7.5 Combining Evidence\nWhen scientists want to figure out what a gene does, they turn to a tool called Interproscan. It’s like using a detective kit with different tools to uncover the gene’s secrets. Interproscan does two important jobs at once. It’s a bit like looking at both the shape and function of a key to understand what it can unlock. For genes, it checks their structure and function.\nOne thing it checks is something called “conserved protein domains.” Think of these as like the building blocks or patterns that genes are made of. Interproscan compares these patterns to a big database that includes PFAM, protein superfamilies, and PANTHER.\nAnother tool in its kit is BLAST, which is like a gene family tree detective. It helps find genes that are similar to the one being studied. These similar genes can give clues about what the gene does. Interproscan also looks at metabolic pathways (like the chemical processes in a cell) using tools like KEGG and Metacyc. It’s like figuring out how ingredients are used in a recipe. It also predicts Gene Ontology (GO) categories, which are like labels that describe what a gene is involved in.\nTo assign a gene’s function, Interproscan uses something called the guilt-by-association principle. It’s like assuming that if someone is often seen with a group of people doing a particular activity, they’re likely involved in that activity too. In genes, if a gene is similar to others in a certain pathway or category, it’s probably involved in the same kind of job."
  },
  {
    "objectID": "chapters/week7.html#final-verification",
    "href": "chapters/week7.html#final-verification",
    "title": "7  Annotation",
    "section": "7.6 Final Verification",
    "text": "7.6 Final Verification\nProf. Jarkko lists a sample checklist for this bit - just to ensure that the genome in question is a good one:\n\nStart with the Gene Model\nBegin with the predicted gene model as your reference. It’s like having a blueprint for a building.\nCompare with RNAseq and EST Data\nCheck if the RNA sequencing (RNAseq) and EST data match the gene model. It’s similar to making sure the building looks like the blueprint.\nExamine Protein Matches in Other Species\nSee if proteins from related species match your gene. It’s like comparing your building to others in the neighborhood.\nBLAST the Protein Sequence\nUse a tool called BLAST to compare your gene’s protein sequence to a big database of genetic information. This helps you find similar genes in various species and might give hints about gene function.\nGather Good Hit Sequences\nCollect the sequences that closely match your gene, especially from well-studied model species. Think of it as getting advice from experts.\nMultiple Sequence Alignment\nLine up the protein sequences and see if they match well. It’s like checking if puzzle pieces fit together. Look out for mistakes like frame shifts, incorrect splicing, or missing start/stop signals.\nWatch Out for “Ns”\nIf you see “N” in the sequence, it means there’s missing information. It’s like having a blank spot on the blueprint.\nManual Adjustments\nIf needed, make manual adjustments to the gene model to improve accuracy. It’s like fine-tuning the building plans to make everything fit perfectly.\nExperimental Verification\nFinally, validate the gene model with experimental data. This involves using RNAseq and targeted experiments to confirm that the gene functions as predicted. It’s like testing the building to ensure it works as intended."
  },
  {
    "objectID": "chapters/week7.html#fundamental-metrics",
    "href": "chapters/week7.html#fundamental-metrics",
    "title": "7  Transcriptomes: Differential Expression to Enrichment Analyses",
    "section": "7.1 Fundamental Metrics",
    "text": "7.1 Fundamental Metrics\n\n7.1.1 Fold Change\nLet’s discuss the concept of “Fold Change” and how it serves as a measure of differential gene expression.\nImagine you have two baskets of apples: one from a case scenario (like a treatment group) and the other from a control scenario (like a standard group). You want to know if the number of apples in the case basket is different from the control basket. Fold change is a simple way to express this difference.\nHere’s how it works:\n\nCalculating Fold Change\n\\[\\begin{equation}\n  \\log\\left(\\frac{\\text{Case}}{\\text{Control}}\\right) = \\log\\left(\\text{Case}\\right) - \\log\\left(\\text{Control}\\right)\n\\end{equation}\\]\nTo calculate fold change, you take the number of apples (gene counts) in the case basket and divide it by the number of apples in the control basket. This ratio tells you how many times more (or less) apples there are in the case compared to the control.\nLog2 Transformation\nFold changes are often reported in the log2 domain because it has some useful properties. When you take the logarithm of the fold change, it simplifies the interpretation. Also, it’s symmetric, meaning that if a gene is upregulated (more apples in the case), it will have a positive value, and if it’s downregulated (fewer apples in the case), it will have a negative value.\nRelationship to Difference\nThe \\(\\log2\\) of the fold change is related to the difference between the case and control. It’s like expressing the change in the number of apples as a difference in their logarithms.\nGeometric Mean\n\\[\\begin{equation}\n  \\log\\left(\\sqrt{\\text{Case} \\cdot \\text{Control}}\\right) = \\frac{\\log\\left(\\text{Case}\\right) + \\log\\left(\\text{Control}\\right)}{2}\n\\end{equation}\\]\nAnother way to express fold change is to use the geometric mean of the apples in the case and control baskets. This is like finding the average number of apples between the two baskets, but in the \\(\\log2\\) domain.\n\nHowever, fold change has limitations. It works well when you have only one measurement, but it’s not ideal when your data is very noisy or when you’re dealing with complex datasets. Noise can lead to misleading results, which is why more sophisticated statistical methods are often used for differential gene expression analysis in RNA-seq data.\n\n\n7.1.2 MA Plots\n\n\n\n\n\nExamples of MA Plots\n\n\n\n\nImagine you have data points on a graph, where one axis represents the log fold change (how much something has changed on a logarithmic scale, like we discussed earlier), and the other axis represents the mean expression (the average level of something, like the average number of apples in our baskets).\nNow, when you look at this scatterplot of log fold change, it might be a bit hard to interpret because it’s not as intuitive as seeing the actual fold change values directly. The MA plot comes to the rescue.\nHere’s how it works:\n\nTransforming the Axes\nThe MA plot essentially takes the scatterplot and rotates the axes by 45 degrees. This means that the log fold change values are now on one axis (the M-axis), and the mean expression values are on the other axis (the A-axis).\nM-A Axes\nThe M-axis typically represents the difference between the log fold change in the case and control groups, while the A-axis represents the average of the log fold change values.\nInterpretable Plot\nNow, instead of looking at log fold change values, you’re looking at something more intuitive – the actual fold change values (like we discussed earlier) on one axis, and the mean expression levels on the other axis. This makes it easier to see how much things have changed in terms of fold change, relative to the average expression."
  },
  {
    "objectID": "chapters/week7.html#testing-significance",
    "href": "chapters/week7.html#testing-significance",
    "title": "7  Transcriptomes: Differential Expression to Enrichment Analyses",
    "section": "7.2 Testing Significance",
    "text": "7.2 Testing Significance\nImagine you have two groups of apples, like a treatment group and a control group. You want to know if there’s a real difference in the number of apples between these two groups or if any difference could have occurred by chance.\nHere’s how significance testing works:\n\nConstruct a Null Hypothesis\nThis is like making an educated guess based on the idea that there’s no real difference between the two groups. In a two-sample case (like treatment vs. control), the null hypothesis would be something like “the mean number of apples in the treatment group is the same as the mean number of apples in the control group.” In a one-sample case, it might be “the mean number of apples is not different from zero.”\nTest the Hypothesis\nYou perform statistical tests to see if this null hypothesis holds. The tests rely on certain assumptions about the data, such as following a known distribution (e.g., Student’s t-distribution for comparing means) and having a test statistic with a known distribution under the null hypothesis.\nP-value\nAfter running the statistical test, you get a p-value. This p-value tells you the probability of obtaining the data you have (or more extreme data) by chance alone, assuming that the null hypothesis is true. In simple terms, it’s like asking, “How likely is it that the observed difference in apples between the two groups happened purely by random chance?”\nInterpreting the P-value\nIf the p-value is small, typically less than 0.01 or 0.05 (but this threshold can vary), it suggests that the difference you observed is unlikely to have occurred purely by chance. In other words, it’s statistically significant. This means you have evidence to reject the null hypothesis and conclude that there’s likely a real difference between the groups.\n\n\n7.2.1 P Values\n\n\n\n\n\nIllustration of a P-Value\n\n\n\n\nImagine you’re playing a game with a coin. You want to know if the coin is fair or biased, meaning it might not land on heads and tails with equal probability. To test this, you flip the coin a bunch of times and record the outcomes.\nNow, the null hypothesis here is that the coin is fair. In other words, there’s no bias; it’s just like any other regular coin. The alternative hypothesis is that the coin is biased.\nHere’s how p-values come into play:\n\nCalculating the p-value\nWhen you calculate a p-value, you’re essentially figuring out the probability that you would get the results you did (like getting heads or tails a certain number of times) if the null hypothesis were true. In other words, it’s like asking, “What’s the chance that the coin appears biased purely by coincidence?”\nInterpreting the p-value\nIf the p-value is small (typically less than 0.01 or 0.05), it suggests that the results you observed are unlikely to have occurred purely by chance. So, if you find a very small p-value, you might conclude that there’s evidence against the null hypothesis (the coin is fair), and you start to consider the alternative hypothesis (the coin is biased).\n\nIn gene expression analysis, p-values work similarly. They tell you the chance that a gene is categorized as differentially expressed (meaning its expression levels have changed significantly) purely by coincidence or random variation.\nFor example, if you’re comparing gene expression in two groups, a small p-value suggests that the difference you observed in gene expression is unlikely to have happened by random chance alone. It provides evidence that the gene is genuinely differentially expressed between the groups.\n\n\n7.2.2 Differences Between Conditions\nLet’s explore two common statistical tests used to determine if there is a significant difference between two conditions in gene expression analysis, the Student’s t-test and the Wilcoxon Rank Sum (or Wilcoxon Mann-Whitney) test.\nImagine you have data from two groups, like a treatment group and a control group, and you want to know if there’s a real difference between them.\n\nStudent’s t-test:\n\nTesting Means: The Student’s t-test is like a tool for comparing the means (averages) of the two groups. It tells you if the difference in means between the groups is statistically significant.\nTypes of t-tests: There are various versions of the t-test, like the two-sample t-test (comparing two groups), one-sample t-test (comparing one group to a known value), and variations that account for assumptions like equal or unequal variances between the groups.\nAssumption: The t-test assumes that the data is normally distributed, meaning it follows a typical bell-shaped curve around the mean. This assumption is important because the test relies on normal distribution properties.\n\nWilcoxon Rank Sum (or Wilcoxon Mann-Whitney) test:\n\nTesting Medians: The Wilcoxon Rank Sum test, also known as the Mann-Whitney test, is different from the t-test. It compares the location, specifically the medians, of the two groups. It tells you if the medians are significantly different.\nTypes of Wilcoxon tests: Like the t-test, you can use variations of the Wilcoxon test, including two-sample Wilcoxon (comparing two groups) and one-sample Wilcoxon (comparing one group to a known value).\nAssumption: The Wilcoxon test doesn’t rely on the assumption of normal distribution. Instead, it works well when the distributions are symmetric and similar.\nSensitivity: The Wilcoxon test is often considered less powerful (meaning it’s less likely to detect a true difference) compared to the t-test when the data is close to a normal distribution. However, it’s more robust when dealing with data that doesn’t meet the normal distribution assumption.\n\n\nIn summary, both the Student’s t-test and the Wilcoxon Rank Sum test are used to determine if there’s a significant difference between two conditions. The t-test focuses on means and assumes normal distribution, while the Wilcoxon test examines medians and doesn’t rely on normal distribution assumptions. The choice between them depends on your data and whether the assumptions of the t-test are met.\n\n\n7.2.3 Models to Analyze RNA Sequencing Data\n\n\n\n\n\nModel Transformation for RNA Sequencing Data\n\n\n\n\nWhen we’re working with RNA sequencing data, we use something called generalized linear models to transform our data to follow a Gaussian distribution (i.e., a bell curve) - especially error values in transcription to positive values.\nSuch a transformation often takes into account the fact that the data does not follow a Gaussian distribution. Several tools have different ways of doing this:\n\nUsing theory\nTools like EdgeR and DESeq2 (i.e., from BioConductor in R) use the Negative Binomial distribution or the Poisson distribution (e.g., PoissonSeq does this).\n\n\n\n7.2.4 Biological vs. Statistical Significance\nWhen we talk about biological significance and statistical significance, we’re trying to figure out if something we see in our data is not only mathematically significant but also meaningful in the real world of biology. You see, sometimes the numbers we get from our experiments might be statistically significant, which means they’re different enough to catch our attention, but they might be so tiny that they don’t actually matter in the bigger picture of how a living thing functions.\nTo decide if something is genuinely important in biology, we need to consider two things. First, we look at the average change or difference we observe, and second, we check if this change is statistically significant. Think of it like this: we’re not just interested in whether something is different; we also want to know if that difference is big enough to make a real impact.\nTo help us figure this out, scientists often use a rule of thumb: if the change is at least ±1 log2-ratio, it’s usually considered biologically meaningful. But here’s the tricky part: this “meaningful” threshold can vary depending on what kind of cells we’re studying (like brain cells versus liver cells) and what we’re investigating in our experiment (like subtle changes in how a gene works versus genes that react really strongly to something).\n\n\n7.2.5 Multiple Testing\nThe thing about high throughput bioinformatics is that we usually need to have a lot of tests to measure significances between different things. However, the problem with such tests is that genes that are said to be significantly expressed may actually be false positives in disguise.\n\n\n7.2.6 Classical Correction Methods\nImagine you’re in charge of testing a bunch of different things, like trying to figure out which light switches in a giant building work. If you just randomly flip switches and say, “This one works!” you might make mistakes. So, you set a rule: if you’re going to say a switch works, you want to be pretty darn sure it’s working, to avoid false alarms.\nIn the world of statistics, when we do lots of tests (like checking the switches), we want to make sure we’re not saying things are true when they’re not. That’s where these correction methods come in:\n\nSidak Correction and Bonferroni Correction\n\\[\\begin{align}\n  \\text{Sidak } \\alpha_e &= 1 - \\sqrt[m]{1 - \\alpha_e} \\\\\n  \\text{Bonferroni } \\alpha_e &= \\frac{\\alpha_e}{m}\n\\end{align}\\]\nThese are like rules we use to decide how confident we need to be before we say something is true (like a switch works). The idea is to set a threshold for each test. If the chance of being wrong (a false positive) is too high, we won’t call it true.\nFamily-wise Error Rate\nThis is just a fancy way of saying that we’re looking at the overall rate of false positives when we do many tests. We don’t want too many switches to be labeled as working when they’re not.\n\nNow, here’s the catch: these methods are sometimes seen as overly cautious. Why? Well, first, the tests we do aren’t always completely independent, meaning the results of one test might affect the next one. Second, they control for something a bit different: the chance that all the tests are correct, not just one. So, sometimes they might make us miss switches that actually work because we’re being too strict.\n\n\n7.2.7 False Discovery Rate\nImagine you’re a detective trying to solve a mystery. In this mystery, you’re looking for clues in different places, and you want to make sure you catch as many real clues as possible, but you’re okay with the idea that you might pick up a few fake clues along the way.\nNow, in the world of statistics, when we’re doing lots of tests or looking for lots of clues, we have to decide how strict we want to be about avoiding false leads (false positives).\nTraditionally, we used to focus on controlling the family-wise error rate (i.e., FWER), which meant we wanted to make sure we didn’t make any mistakes anywhere in our tests. It’s like saying, “I don’t want a single false clue in my investigation.”\n\\[\\begin{equation}\n  \\text{\\#False Discovery Rate} = \\frac{\\text{\\#False Positives}}{\\text{\\#True Positives + \\#False Positives}}\n\\end{equation}\\]\nBut sometimes, being too strict about this can make us miss out on real discoveries. So, instead of worrying too much about catching every single error, we can control the false discovery rate (i.e., FDR). This approach allows us to be a bit more flexible. It means that we’re okay with accepting a certain proportion of our significant findings as potentially false positives.\n\n7.2.7.1 Benjamini-Hochberg Correction\nImagine you’re a scientist working with a lot of data, like testing many genes in an experiment. You want to make sure that when you say a gene is significant, it’s not just a random chance finding. The Benjamini-Hochberg correction helps you do just that.\nHere’s how it works:\n\nChoose Your Significance Level: First, you decide how strict you want to be. This is like setting a rule that says, “I want to be this confident before I call something significant.” For example, you might choose a 5% significance level, which means you’re willing to accept a 5% chance of making a mistake.\nCompute Many Individual Tests: Next, you run lots of individual tests, one for each gene you’re studying. These tests tell you how likely it is that each gene’s results are just random.\nSort the p-values: Now, you have a bunch of p-values, which are like scores that tell you how likely it is that something is significant. You sort these p-values from smallest to largest, like arranging them in order.\nCompute Gene-Specific Thresholds: Here’s where the Benjamini-Hochberg correction comes in. You calculate a specific threshold for each gene based on its p-value and the significance level you chose in step 1. This threshold helps you decide which genes are significant.\n\nHere’s what’s cool about this method: It’s a bit like being a careful scientist who wants to be sure about their findings. It doesn’t just apply the same rule to all genes; it customizes the rules for each gene based on its own data. So, some genes might have to meet a higher bar to be called significant, while others have a lower bar, depending on their p-values and the chosen significance level.\nIn simple terms, the Benjamini-Hochberg correction helps you avoid calling something significant when it’s not, while also being flexible enough to account for the individual uniqueness of each gene in your study. It’s like being both cautious and fair in your analysis of a big dataset.\n\n\n7.2.7.2 Q Values\nImagine you’re exploring a big forest looking for hidden treasures, but you’re also worried about accidentally finding something that isn’t a treasure.\nNow, when scientists like to discover things in their experiments, they’re also concerned about making mistakes, like saying they found something important when it’s not actually there. That’s where q-values come in.\n\nEstimating True Negatives: Q-values help scientists figure out how many times they might have made mistakes by calling something significant when it’s not. You can think of it as estimating the number of times they thought they found a treasure, but it turned out to be just a rock. This is something that the Benjamini-Hochberg method doesn’t really focus on.\nQ-values vs. P-values: Q-values are a bit like p-values, which you can think of as scores for how likely something is to be important. But q-values go a step further. They take a bunch of these p-values and put them in order. Then, they help scientists understand how many of these “important” things might actually be mistakes."
  },
  {
    "objectID": "chapters/week7.html#differentially-expressed-gene-and-biology",
    "href": "chapters/week7.html#differentially-expressed-gene-and-biology",
    "title": "7  Transcriptomes: Differential Expression to Enrichment Analyses",
    "section": "7.3 Differentially Expressed Gene and Biology",
    "text": "7.3 Differentially Expressed Gene and Biology\n\n7.3.1 Tool Recommendations\n\n7.3.1.1 Sahraeian et. al\nWhen it comes to analyzing RNA-seq data, there are some recommendations from a guy called Sahraeian and his team that can help us understand which tools work well. Imagine you have a toolbox, and you want to know which tools are the best for the job.\n\nQuantification\nOne of the things they looked at was how well different tools work to measure gene expression. They found that tools like HISAT2 and TopHat are better at this job compared to STAR. Think of it like some wrenches being better at turning bolts than others.\nEfficiency Matters\nThey also found that some tools that don’t rely on traditional alignment methods (alignment-free tools) are quite efficient. However, when they combined a tool called StringTie with an efficient aligner like HISAT2, it turned out to be the most efficient approach. So, it’s like saying, sometimes it’s better to use two tools together to get the job done quickly.\nSpeed Differences\nSome tools were much faster than others, but they didn’t provide specific numbers in this study. Think of it as some tools being super-fast like a race car, while others are a bit slower like regular cars, but they didn’t give us the exact speed limits.\nDifferential Expression\nAnother important task in RNA-seq analysis is figuring out which genes are different between conditions. They found that a tool called edgeR did a great job at this and had a lower rate of falsely calling genes as different when they weren’t.\nBest Recommendation\nOverall, their top recommendation for differential expression analysis was a tool called DESeq2. It’s like saying, if you want to pick just one tool from your toolbox, DESeq2 might be the best all-around choice. But remember, the best tool can vary depending on the specific job or data you’re working with.\n\n\n\n7.3.1.2 Corchete et. al\n\n\n\n\n\nCorchete et. al’s Work\n\n\n\n\nIn this study by Corchete and their team, they explored various ways to process and analyze RNA-seq data to find differentially expressed genes. Think of it like they tried out many recipes to cook the same dish and wanted to know which one tasted the best.\n\nExploring Different Combinations\nThe researchers didn’t just stick to one way of analyzing the data; they tried a whopping 192 different methods or combinations of steps to see how they affected the results. Imagine if you were making a sandwich, and you tried using different types of bread, spreads, and fillings to find the most delicious combination.\nSamples and Validation\nTo make sure their findings were reliable, they used 18 samples from two different human cell lines. It’s like cooking the same meal multiple times to make sure you get consistent results. They also validated their findings by using a technique called qRT-PCR on the same samples, which is a bit like double-checking the taste of your dish to be absolutely sure it’s delicious.\nRecommended Preprocessing Steps\nAfter trying all these different recipes (or methods), they found that a combination of preprocessing steps worked well. It included tools like Trimmomatic, RUM, HTSeq Union, and TMM. Think of it as using specific ingredients and techniques in a recipe to make your dish turn out just right.\nThe Limma Trend\nAmong all the methods they tested, one called Limma trend consistently performed well. It’s like finding that a particular way of seasoning your dish always makes it taste great."
  },
  {
    "objectID": "chapters/week7.html#analysis-outcomes",
    "href": "chapters/week7.html#analysis-outcomes",
    "title": "7  Transcriptomes: Differential Expression to Enrichment Analyses",
    "section": "7.4 Analysis Outcomes",
    "text": "7.4 Analysis Outcomes\nThere are usually tens of, if not hundreds or even thousands of differentially expressed genes.\nWhen we’re working with RNA-seq data and trying to figure out which genes are significant, it’s like solving a puzzle. We have lots of genes, and for each one, we use a special test to check if it’s behaving differently under different experimental conditions. We also make sure our results are reliable by correcting the test results. What we end up with is a list of genes that seem to be important.\nBut here’s the thing: just having this list of genes might not give us the whole picture. It’s a bit like having puzzle pieces without knowing what the final picture looks like. And depending on the tool we use to analyze the data, the list of important genes can vary.\nNow, why are we doing all this in the first place? The main goal of RNA-seq is to understand how things change between different experimental conditions. We want to know what’s happening at a functional or molecular level. It’s like trying to figure out what’s going on inside a black box.\n\n7.4.1 Levels of Analysis\nWhen scientists are studying gene behavior, they look at things on different levels, a bit like zooming in and out with a microscope. It’s kind of like exploring a forest where you can start with the smallest details and gradually uncover the bigger picture.\n\nSingle Gene Level: At the smallest level, researchers focus on single genes. They want to know if a particular gene behaves differently when it’s exposed to a treatment or experiment compared to when it’s in its normal state (the control). It’s like looking closely at one tree in the forest to see if it’s growing differently because of something happening around it. This helps us understand how individual genes respond to changes and can be important for things like developing new drugs.\nMultiple Genes Level: Zooming out a bit, scientists ask questions about groups of genes. They wonder if there are sets of genes that work together or are somehow connected. For example, they might investigate if a bunch of genes all change their behavior in response to a treatment compared to a control. It’s like stepping back and observing a whole section of trees in the forest to see if they’re all affected in the same way. This helps us see broader patterns in how genes are working together.\nNetworks of Genes Level: Finally, when researchers zoom out even further, they’re looking at the big picture. They want to identify and understand the complex web of interactions between genes, which we call regulatory and signaling networks. It’s like studying the entire ecosystem of the forest, where you’re not just looking at individual trees but also how they all interact with each other, from the smallest plants to the largest animals. This level helps us grasp the underlying mechanisms that control gene behavior.\n\nSo, in essence, scientists use different “microscopes” to study genes, starting from the tiniest details and gradually uncovering larger and more complex interactions. It’s a bit like exploring a forest, from individual trees to the entire ecosystem, to truly understand how everything works together in the world of genetics.\n\n\n7.4.2 Gene Ontology (i.e., GO)\nWhen scientists want to understand how genes are connected to the jobs they do in our bodies, they use something called “Gene Ontology,” or GO for short. Think of it as a giant encyclopedia for genes that helps us make sense of what they do.\nNow, this GO encyclopedia isn’t just one big book; it’s divided into three separate parts, kind of like having different sections in a library:\n\nBiological Process: In this section, scientists describe what specific jobs or tasks genes are involved in when it comes to biological processes. It’s like reading about the roles of different workers in a big company.\nMolecular Function: Here, scientists explain the specific functions or abilities of genes at the molecular level. It’s like understanding the unique skills or talents of each gene, such as their ability to interact with other molecules.\nCellular Component: This section focuses on where genes are located or where they do their work inside cells. It’s like finding out which departments or rooms in a company each worker belongs to.\n\nBut that’s not all! There are also other ways to group genes based on their functions and properties. For example, we have pathways like KEGG, Metacyc, and Reactome, which are like different roads or routes genes can take in the body. We also have categories like protein families and even information about where genes are located on chromosomes, which is a bit like knowing the street addresses of different businesses in a city.\n\n\n7.4.3 How Does Gene Set Analysis Work?\nImagine you’re trying to figure out if a group of friends are all good at playing a game, but you only have a few chances to watch them play. If you try to judge each friend’s skills one by one, it might be really hard to tell if they’re good or not because your observations could be a bit shaky.\nNow, what if you look at how the group as a whole plays the game? It’s like putting all their skills together and looking at the big picture. This can make it easier to see if they’re doing well or not. That’s the basic idea behind analyzing gene sets.\nWhen scientists have limited data, like not enough measurements for each individual gene, it can be tough to tell if a single gene is acting differently. But when they group genes together based on certain criteria (like genes that are related to a specific process), they can see if that whole group is changing in a noticeable way. It’s like looking at how the entire team of friends is playing the game rather than just focusing on one person. This approach helps scientists spot big changes in gene groups more easily.\nThis way of looking at things can help scientists find out if a bunch of genes are working together or changing together, kind of like friends who play the same game in a similar way. Plus, based on experimental evidence, it seems that these group effects are often consistent across different experiments, while individual gene effects can be a bit trickier to pin down and reproduce."
  },
  {
    "objectID": "chapters/week7.html#enrichment-analysis",
    "href": "chapters/week7.html#enrichment-analysis",
    "title": "7  Transcriptomes: Differential Expression to Enrichment Analyses",
    "section": "7.5 Enrichment Analysis",
    "text": "7.5 Enrichment Analysis\nThis is a way scientists figure out if certain groups of genes are doing something special in a given situation.\n\nSingular Enrichment Analysis: Imagine you have a list of genes that you think are acting differently under certain conditions. In this approach, you assume you already know which genes are different and you have a specific set of rules for what counts as a significant change. You then test each group of genes one by one to see if they’re involved in any special processes or functions. It’s like checking each item on a shopping list to see if it’s available at a store. This approach treats each group of genes independently.\nGene Set Enrichment Analysis: Now, think of a scenario where you don’t have strict rules for what counts as a significant change. Instead, you compare the entire group of genes you’re interested in to lots of random groups to see if there’s something unique about them. It’s like comparing a special recipe to a bunch of random ones to see if it’s really unique and delicious. This approach doesn’t rely on specific thresholds; it’s more about comparing your group of genes to many random ones.\nModular Enrichment Analysis: In this approach, you already have a list of genes that you think are behaving differently, but you’re not sure how they’re related. So, you group these genes together based on how they’re described or annotated, kind of like sorting them into different boxes based on their similarities. Then, you see if each box (or module) of genes is involved in any special processes or functions. It’s like organizing your closet by grouping similar clothes together and then checking each group to see if they have a unique style. This approach focuses on the relationships between genes based on their descriptions.\n\n\n7.5.1 Singular Enrichment Analysis\nImagine you have a group of 30 genes, and out of these, a whopping 24 are labeled as “ribosomal proteins.” It might seem like something important is happening with ribosomal proteins in your experiment, right?\nBut hold on, there’s a twist. What if you knew that this group of 30 genes was originally part of a much larger group of 100 genes, and out of those, 90 were also “ribosomal proteins”? Suddenly, it doesn’t seem as extraordinary, does it?\nThat’s where the idea of considering the background comes into play. You see, just knowing how many genes are related to a particular biological process isn’t enough. You need to compare it to what you’d expect by chance. Are you seeing more of those genes than you’d anticipate? Or is it just a typical result?\nTo figure this out, scientists use a statistical test. It’s like having a way to check if the number of “ribosomal proteins” you found in your group of 30 genes is statistically significant, meaning it’s not just due to chance. It helps ensure that when you say something is happening with a biological process, you’re not making a false alarm based on random fluctuations.\n\n7.5.1.1 Fisher’s Exact Test\n\n\n\n\n\nTruth Matrix as Seen by Fisher\n\n\n\n\nLet’s take a journey back in time to understand Fisher’s Exact Test and its interesting history. Imagine a time when people were fascinated by unusual claims, like Muriel Bristol’s assertion that she could tell whether milk or tea was poured first just by tasting it.\n\\[\\begin{equation}\n  p = \\frac{(a+b)!(c+d)!(a+c)!(b+d)!}{n!a!b!c!d!}\n\\end{equation}\\]\nNow, enter Sir Ronald A. Fisher, a brilliant statistician. He wanted to put Muriel’s claim to the test in a scientific way. So, he came up with what’s known today as Fisher’s Exact Test. The idea behind this test was to determine if there was a significant connection between what Muriel called the “Truth” (the actual order of pouring milk or tea) and what “Lady says” (Muriel’s claims based on her taste).\n\n7.5.1.1.1 In Bioinformatics\nNow, let’s dive into how Fisher’s Exact Test is used in the world of gene sets, specifically in the context of gene expression analysis. Imagine you have two categories that you want to compare:\n\n“Gene belongs to the (GO/KEGG) category”: This category is all about genes that have specific labels or annotations, like being part of a certain biological process or pathway.\n“Gene is in the list of differentially expressed genes”: This category includes genes that you’ve identified as behaving differently under certain experimental conditions.\n\nNow, what Fisher’s Exact Test does here is pretty clever. It examines whether these two categories significantly overlap or not. In simpler terms, it helps you figure out if the genes with specific labels or annotations (the first category) tend to show up more often in the list of genes that behave differently (the second category) than you’d expect by random chance.\nIt’s like being a detective once again. You’re trying to find out if there’s a real connection between certain types of genes (those with specific labels) and genes that change their behavior under certain conditions. Fisher’s Exact Test helps you determine if this connection is more than just a coincidence, providing a statistical way to make sense of the data and draw meaningful conclusions in gene expression analysis.\n\n\n7.5.1.1.2 Pros and Cons\nLet’s explore the benefits and downsides of Fisher’s Exact Test, which is like a versatile tool in the world of statistics, but it comes with its own strengths and limitations.\n\nBenefits of Fisher’s Exact Test:\n\nSimple and Fast: One of its standout features is that it’s quite straightforward to use. You don’t need to be a statistics expert to apply it. Plus, it’s speedy, providing results quickly. It’s like having a simple and efficient tool in your toolkit.\nGood Performance: Fisher’s test already does a pretty good job at what it’s designed for. It’s like having a reliable and well-tuned instrument that usually gives you meaningful results.\nVersatility: You can use it in various situations and contexts. It’s not limited to a specific type of data or research area. Think of it as a Swiss Army knife in statistics that can be applied in different scenarios.\n\nDownsides of Fisher’s Test:\n\nSetting a Threshold: To use Fisher’s test effectively, you need to decide on a threshold for what counts as “differential expression.” This can sometimes be a bit subjective and may vary depending on the specific experiment.\nMultiple P-Values: When you apply Fisher’s test to multiple categories (like different gene sets), you end up with a bunch of p-values, one for each category. This can be overwhelming if you have hundreds of enriched categories with p-values less than 0.05. It’s like having a stack of puzzle pieces to put together.\nCorrection for Multiple Testing: Because you’re dealing with lots of p-values, you often need to correct for multiple testing. This step ensures that you don’t mistakenly think something is significant just by chance. However, when categories overlap, it can be challenging to account for this in the correction process.\nDifferent Gene Set Sizes: Not all gene sets are the same size. Some may have many genes, while others have only a few. Fisher’s test might treat them all equally, which can be a limitation when you’re comparing different sets. It’s like trying to compare the impact of big and small puzzle pieces without adjusting for their size.\n\n\nSo, Fisher’s Exact Test is like a trusty tool that gets the job done quickly and effectively in many situations. But, like any tool, it has its quirks, such as the need for threshold decisions and dealing with multiple p-values, making it important to use it thoughtfully and consider its limitations."
  },
  {
    "objectID": "chapters/week7.html#gene-set-enrichment",
    "href": "chapters/week7.html#gene-set-enrichment",
    "title": "7  Transcriptomes: Differential Expression to Enrichment Analyses",
    "section": "7.6 Gene Set Enrichment",
    "text": "7.6 Gene Set Enrichment\nImagine you’re looking at a big collection of genes, and you suspect that there might be small but coordinated changes happening in their expression levels. But here’s the thing: you don’t want to decide on a specific rule or threshold for what counts as a “big” change because it could vary from one experiment to another.\nThat’s where gene set enrichment tests come into play. The basic idea behind these tests is to do two things:\n\nCompute a Gene Set-Wise Test Statistic: For each category or group of genes (like those related to a specific biological process), you calculate a special number called a “test statistic.” This number summarizes how the genes in that category are behaving, kind of like a score that tells you if something interesting might be going on.\nCompare to Random Samples: Next, you compare the test statistic you calculated for the actual group of genes to what you’d expect if you randomly picked genes. It’s like checking if the score for your group of genes is way different from what you’d get by just picking genes at random.\n\nIf the test statistic for your group of genes stands out from what you’d expect by chance, it suggests there’s something significant happening in that category. It’s like finding a treasure chest among a bunch of empty boxes.\n\n7.6.1 Gene Set Enrichment Analysis\n\n\n\n\n\nGeneral Workflow for GSEA\n\n\n\n\nLet’s simplify the process of Gene Set Enrichment Analysis (GSEA) so that it’s easier to understand:\n\nSorting Genes: First, we line up all the genes based on how differently they’re behaving in an experiment. Some genes might be more active, while others are less active.\nCalculating Enrichment Score (ES): Now, we want to figure out if certain groups of genes, let’s call them “gene sets,” are doing something special. We go through the list of genes and do two things for each gene:\n\nIf the gene belongs to a particular gene set we’re interested in (let’s call it “G”), we add 1 to our score.\nIf it doesn’t belong to that gene set, we subtract 1 from our score. After we’ve done this for all the genes, we find the highest score, which we call the “maximum enrichment score” (MES).\n\nTesting It: Imagine we want to make sure our results aren’t just by chance. So, we create a “null hypothesis” by doing a pretend experiment 1,000 times:\n\nWe mix up the labels on our measurements (like shuffling cards).\nThen, we repeat steps 1 and 2, calculating the MES for this shuffled data.\n\nChecking Significance: Finally, we ask a question: “How many times did our pretend MES (from the shuffled data) turn out to be as extreme as or even more extreme than the real MES we found in our actual data?”\n\nIf the real MES is way more extreme than what we got in our pretend experiments, it suggests that the gene set we’re looking at is probably doing something important.\n\n\nSo, in simple terms, GSEA helps us find out if certain groups of genes are acting differently in our experiment compared to what we’d expect by random chance. If they are, it could mean they’re involved in something significant.\n\n\n7.6.2 Gene Set Analyses\n\n\n\n\n\nArrays and Genes in GSA\n\n\n\n\nLet’s break down Gene Set Analysis (GSA) in a way that’s easy to understand:\n\nGene Test Statistics: First, we look at each gene individually and calculate a special number called a “test statistic.” This number helps us understand how different each gene is behaving in our experiment. Think of it as a score for each gene.\nMax-Mean for Sets: Next, we’re interested in groups of genes, which we call “sets.” Instead of looking at each gene on its own, we want to know if whole sets of genes are doing something special. To figure this out, we find the maximum average score \\(\\max\\{\\bar{s}^{(+)}, \\bar{s}^{(-)}\\}\\) for each set. It’s like looking for the highest average score among a group of friends playing a game.\nStandardizing the Scores: We want to make sure our results are reliable, so we do a bit of math to make everything fair and comparable: \\(\\displaystyle S'_{\\text{max}} = \\frac{(S_\\text{max} - \\mu_S)}{\\sigma_S}\\). This step involves standardizing the scores to put them on the same scale, so we can easily compare different sets of genes.\nShuffling the Data: To be extra sure that our findings are meaningful and not just random, we do some pretend experiments. We mix up the data by shuffling the columns around, kind of like playing cards. Then, we calculate the max-mean scores for these shuffled data sets many times (let’s say B times).\n\nAs an extra tidbit of information, the p-values for the \\(S'_\\text{max}\\) shown above can be calculated using the following formula:\n\\[\\begin{equation}\n  p_s = \\frac{\\#(S'_\\text{max} < S^{'B})}{B}\n\\end{equation}\\]\nSo, in a nutshell, Gene Set Analysis helps us figure out if groups of genes are working together in a special way in our experiment. We start by looking at each gene’s behavior, then we check if sets of genes have higher average scores. We make sure our results are reliable by standardizing them and doing some pretend experiments with shuffled data. It’s like investigating whether certain teams of friends are winning a game together, but we want to be sure it’s not just luck!\n\n7.6.2.1 Problems with Gene Set Analysis\nLet’s simplify the challenges in gene set enrichment tests:\n\nSummarizing Gene Data: In these tests, we need to crunch down all the information about each gene into one simple number. This can be tricky, especially in complex experiments with lots of data points. It’s like trying to describe a whole book with just one word.\nMissing Important Clues: These tests might not catch every important detail about how genes work together. Imagine some pathways in our body are like a security system with gatekeeper genes that control what happens. Just looking at the overall expression of genes might not tell us if these gatekeepers are active or not.\n\nSo, while gene set enrichment tests are helpful, they have their limitations. They sometimes struggle to give us a clear picture of what’s happening in complex experiments, and they might miss some important clues about gene activity. It’s a bit like trying to understand a big puzzle, and we might need some extra pieces to see the whole picture. That’s all for today!"
  },
  {
    "objectID": "chapters/week8.html",
    "href": "chapters/week8.html",
    "title": "8  Transcriptomes: Network Analysis",
    "section": "",
    "text": "Illustration of a Transcription Factor\nIn genetics, there’s a fascinating puzzle to solve: how do our genes get turned on and off? Imagine your DNA as a book, and these genes are like chapters. There are two special areas in the DNA that play a big role in this story: promoters (P) and enhancers/silencers (E). Think of them as the “on” and “off” switches for a gene.\nNow, here’s where it gets interesting. Proteins, like tiny molecular superheroes called transcription factors (TFs), come into play. These TFs can attach themselves to the promoter and enhancer areas. The magic happens when these proteins team up. The combination of TFs determines whether a gene is going to be “read” and turned into a protein.\nIf a protein can attach itself to these promoter and enhancer areas, it means it has the power to control that gene. It’s like having control over a remote that decides whether the TV turns on or not. This protein becomes the boss of that gene.\nNow, things get a bit more abstract. Scientists talk about how changes in the boss’s mood can affect the story of the gene. If the boss protein gets more or less active, it can make the gene go from quiet to loud or vice versa. It’s like having a dimmer switch for the lights in a room – you can adjust how bright or dim they are.\nBut here’s the tricky part: in the lab, scientists still need to verify whether a protein can actually attach itself to these promoter and enhancer areas. It’s like having a bunch of keys, but you’re not entirely sure if they fit the locks. This is a crucial step in understanding how our genes are controlled, and scientists are working hard to solve this genetic puzzle."
  },
  {
    "objectID": "chapters/week8.html#ways-of-detecting-gene-function",
    "href": "chapters/week8.html#ways-of-detecting-gene-function",
    "title": "8  Transcriptomes: Network Analysis",
    "section": "8.1 Ways of Detecting Gene Function",
    "text": "8.1 Ways of Detecting Gene Function\nForward genetics starts from the phenotype and goes back to the genome. Reverse genetics starts from the genome and works its way back to the phenotype."
  },
  {
    "objectID": "chapters/week8.html#co-expression-and-co-expression-networks",
    "href": "chapters/week8.html#co-expression-and-co-expression-networks",
    "title": "8  Transcriptomes: Network Analysis",
    "section": "8.2 Co-Expression and Co-Expression Networks",
    "text": "8.2 Co-Expression and Co-Expression Networks\nIn genetics, there’s another interesting concept called co-expression. When scientists don’t know the exact regulatory network, they have to rely on a different approach. They can’t peek inside the genes’ instruction manual, so they start looking for patterns in how genes are turned on or off together, like a team of friends doing the same dance moves.\nThe idea here is pretty simple. Imagine you’re in a big group of people trying to follow a dance routine. If you and your buddy are close to each other in the group, you’re more likely to do the same dance steps at the same time. In the genetic world, this means that when different experimental conditions are applied, genes that are “close” to each other in the way they’re controlled tend to act in a similar way. We say they “correlate.” These genes also tend to associate with one another during clustering (e.g., K-means clustering).\n\n8.2.1 K-Means Clustering\n\n\n\n\n\nSteps Involved in K-Means Clustering\n\n\n\n\nImagine you have a big box of colorful marbles, and you want to sort them into different groups based on their colors. K-means is like a handy tool for this job. Here are some key things to know:\n\nVery Common: K-means is a popular method that many people use. It’s like a favorite tool in a craftsman’s toolbox because it’s useful in many situations.\nGood for Large Data Sets: If you have a massive collection of marbles (or data), K-means can still do its job efficiently. It’s like having a super-fast sorting machine for your marbles.\nVisualization Not Trivial: Sometimes, showing the results of K-means can be a bit tricky. However, if you have a bunch of marbles in many colors, you can represent them on a graph using something called principal components. It’s like making a cool chart that helps you understand how the marbles are grouped.\nFast and Low Memory Consumption: K-means doesn’t hog your computer’s memory, and it works pretty quickly. It’s like having a sorting machine that doesn’t slow down, even when you have lots of marbles to sort.\n\nLet’s take a closer look at some challenges that come with using K-means, a handy tool for grouping things. Here are the key issues:\n1. Choosing K: K-means requires us to decide how many groups (K) we want to create. This can be a bit tricky. Thankfully, there are some rules of thumb, like heuristics, to help us decide, but it’s not always easy. Think of it like trying to figure out how many boxes you need to sort your toys into – you don’t want too few or too many.\n2. Avoiding Local Minima: When you use K-means, you might end up with different results depending on where you start. It’s like solving a maze where you could get stuck in a corner. To overcome this, you can run the K-means algorithm multiple times and pick the best result. It’s like trying different paths in the maze to find your way out.\n3. Message Passing Algorithms: There’s another cool trick – K-means clustering algorithms that use something called “message passing.” This can help find better solutions. It’s like using a secret code or special clues to navigate the maze more effectively. For instance, there’s a method called “affinity propagation” developed by Frey and his team in 2007, which is like having a map to escape the maze.\n\n\n8.2.2 Hierarchical Clustering\nHierarchical clustering is a way to group things together in a very organized manner, and it works like building a tree from the ground up. Here’s how it goes:\nIn this approach, we always start by putting the two most similar things together into a new cluster. Imagine you have a bunch of marbles, and you want to organize them by color. You’d begin by pairing the two marbles that look the most alike.\nNow, when we’re making these clusters, there are a few ways we can decide which marble represents the whole group:\n\nWe can calculate the average of all the marbles in the cluster, like finding the average color.\nOr, we can pick the marble that looks the most similar to the others in the group.\nAlternatively, we could choose the marble that stands out the most, the one that’s least like the rest.\n\nHere’s where it gets interesting: depending on how we choose this representative and the method we use, we get different results. It’s like if we asked a few people to organize the marbles by color, they might come up with slightly different groups.\n\n8.2.2.1 Pros and Cons with Hierarchical Clustering\n\n\n\n\n\nExample of Hierarchical Clustering Used in Literature\n\n\n\n\nHierarchical clustering is a popular way to organize and show data, like putting things in order. It’s so commonly used because it’s not just about sorting data; it’s also a fundamental tool for creating visual representations. Think of it as arranging your toys neatly on a shelf.\nOne cool thing about hierarchical clustering is that it can be used in a two-way fashion. It doesn’t just sort one type of thing; it can sort both experiments and genes. It’s like having a special shelf where you not only arrange your toys but also group your books together.\nBut here’s the catch: while hierarchical clustering is super handy, it works best when you’re dealing with small amounts of data. If you have a massive collection of things to organize, it might not be the best choice. It’s like trying to fit too many toys on a tiny shelf – it gets a bit crowded. So, when you’ve got a smaller pile to sort, hierarchical clustering is your go-to tool for tidying things up and creating visual displays.\nBut there’s a challenge with this approach: how do we decide when to stop making clusters? We have a tree of clusters, but we need to know where to cut it. It’s like having a tree with many branches and trying to figure out where to make the final cuts to create the groups we want. That’s the tricky part in hierarchical clustering.\n\n\n\n8.2.3 Interpreting Co-Expression Networks\nWhen it comes to co-expression clusters, understanding what they mean can be like solving a puzzle. One way to look at it is through the *Guilt by Association principle. This means that if genes are co-expressed, they might be working together or at least involved in similar processes. It’s a bit like assuming that if you often see two friends together, they might be doing something fun or related, like playing the same video game.\nNow, if genes are co-expressed, it’s like they’re on the same team. They might be working together in the same biological pathway. Think of this as a group of friends joining forces to win a game or solve a big problem together.\n\n\n\n\n\nExample of a Distance Metric Used to Form Clusters\n\n\n\n\nTo understand co-expression clusters (i.e., related genes) better, we can also check for gene ontology enrichments. This is like checking if our group of friends shares common interests. It helps us figure out what kind of activities they enjoy together.\nAnother thing to consider is tissue/condition specificity. It’s like looking at when and where our friends like to hang out the most. In this case, we examine the experiments where the cluster’s gene expression is at its peak, which can tell us where these genes are most active.\nSometimes, we might also discover that co-expression clusters have something in common. This can be because they share enriched promoter motifs, which are like special symbols that tell us they belong to the same club. Additionally, if you find “transcription factors” in the same cluster, it’s like identifying key players or leaders in the group. These are the friends who organize the activities or call the shots in their little club of co-expressed genes.\n\n8.2.3.1 Problems with Networks\nExploring co-expression networks can be a bit like hunting for hidden treasures, but there are some challenges we need to tackle along the way. Here are some of the issues we encounter:\nFirst, the data can be noisy. This means there are lots of false alarms or “false positives.” It’s like searching for secret clues, but sometimes you find things that aren’t really connected to your treasure hunt. To deal with this, scientists use clever tricks like bootstrapping, randomization, or cross-validation. These methods help them estimate how often these false alarms happen and make their results more reliable.\nAnother tricky thing is that co-expression networks can’t always tell us if the connection between genes is a direct one or if it’s indirect. It’s like figuring out if your friend’s friend is your friend too, or if they’re just a friendly acquaintance. This direct or indirect link can make a big difference in understanding how genes work together.\n\n\n\n\n\nProblems with Co-Expression Networks\n\n\n\n\nSo, how do we uncover the secret codes that reveal the regulatory structure, like who’s in charge or who’s following whom? Well, scientists have a couple of strategies. In the “wet lab,” they carry out experiments by making changes, like knocking out a gene, to see how it affects the others. It’s like doing experiments in a chemistry lab to see what happens when you mix different ingredients.\nAlternatively, they use more advanced statistical models to find these hidden connections. Think of it as using a super detective’s magnifying glass to spot the subtle clues in the data. By overcoming these challenges, we can unlock the mysteries of co-expression networks and understand how genes influence each other.\n\n\n8.2.3.2 Extra Information\nWhen it comes to understanding genes and how they work, having some extra pieces of information can be incredibly helpful. Here are a few key things that come in handy:\n1. Identity of Transcription Factors and Their Binding Motifs: Imagine transcription factors as the directors of the genetic orchestra. They control which genes play their music. Knowing which transcription factors are in action and what kind of musical notes (or binding motifs) they follow helps us understand how genes are orchestrated.\n2. Knowledge About Functional and Structural Similarity of Genes: Genes are like puzzle pieces in a big genetic puzzle. If we know which pieces fit together because of their shape or function, it makes solving the puzzle easier. Understanding the similarities between genes – whether they perform similar functions or have similar structures – can reveal a lot about how they interact.\n3. Sequence and Binding Sites in the Promoter: Genes have specific “instructions” in their promoter regions that help control when and how they are activated. Knowing these sequences and binding sites is like having the blueprint for building a particular gadget. It tells us how the gene functions and what can influence it.\n4. Measurements About Protein-DNA Binding: Proteins, like transcription factors, often team up with DNA to make things happen. By measuring these protein-DNA interactions, it’s like capturing the moments when the genetic actors step onto the stage. This helps us understand who’s taking part in the genetic play and how they’re influencing the script.\n\n\n\n8.2.4 Possible Workflow\n\n\n\n\n\nPossible Workflow for Network Analysis by Segal et. al\n\n\n\n\n\n\n8.2.5 Advanced Models for Gene Regulation\nIn the fascinating world of gene regulation, there are some advanced models that scientists use to understand how genes are controlled. These models are like powerful tools in their toolkit. Here are a few of them:\n\nGraphical Gaussian Models: Think of these models as building a web of connections. Scientists create this web by testing how genes are related and whether they depend on each other. It’s like drawing lines between friends who influence each other in some way.\nRegression-Based Models: These models are a bit like fortune-tellers. They try to predict a gene’s behavior by looking at other factors, like the expression of regulators or how often certain binding sites appear. It’s as if they’re reading the signs and signals to predict what the gene will do next.\nBayes Nets, Network Models, Data Fusion Methods: These are like building a grand map of gene interactions. Bayes nets are like road signs that show us the most likely paths genes will take. Network models are like a city’s layout, helping us understand how genes connect. Data fusion methods combine information from different sources, like puzzle pieces, to create a clearer picture.\nMore Advanced (Probabilistic) Models: These models are like master chefs who mix various ingredients to create a unique dish. They combine ideas from the models above and add even more background information. This helps scientists create a comprehensive understanding of gene regulation.\n\nAll these models are tools that help scientists explore the intricate world of gene regulation, unraveling the complex relationships between genes and their controllers. It’s like being a detective, using different methods to solve the mystery of how our genes work and influence our lives."
  }
]